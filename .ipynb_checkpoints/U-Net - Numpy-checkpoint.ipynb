{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "#import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "from matplotlib import image\n",
    "\n",
    "def mnist(path=None):\n",
    "    r\"\"\"Return (train_images, train_labels, test_images, test_labels).\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory containing MNIST. Default is\n",
    "            /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist.\n",
    "            Create if nonexistant. Download any missing files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, train_labels, test_images, test_labels), each\n",
    "            a matrix. Rows are examples. Columns of images are pixel values.\n",
    "            Columns of labels are a onehot encoding of the correct class.\n",
    "\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz',\n",
    "             'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz',\n",
    "             't10k-labels-idx1-ubyte.gz']\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        # Set path to /home/USER/data/mnist or C:\\Users\\USER\\data\\salt\n",
    "        path = os.path.join(os.path.expanduser('~/'), 'data', 'salt')\n",
    "\n",
    "    # Create path if it doesn't exist\n",
    "    #os.makedirs(path, exist_ok=True)\n",
    "    \"\"\"\"\n",
    "    # Download any missing files\n",
    "    for file in files:\n",
    "        if file not in os.listdir(path):\n",
    "            urlretrieve(url + file, os.path.join(path, file))\n",
    "            print(\"Downloaded %s to %s\" % (file, path))\n",
    "    print(\"All files are ready to gzip!\")\n",
    "    \"\"\"\n",
    "    def _images(path):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        folder = path + \"/images/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(32, 32)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,32,32).astype('float32')#/255\n",
    "        return pixels[:1,:,:,:]\n",
    "\n",
    "    def _labels(path):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        \n",
    "        folder = path + \"/labels/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(32, 32)) for f in os.listdir(folder)]\n",
    "        #onlyfiles = [cv2.resize(image.imread(folder+f),(32, 32)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,32,32).astype('float32') #/255\n",
    "        return pixels[:1,:,:,:]\n",
    "    \n",
    "    def _t_images(path):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        folder = path + \"/t_images/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(32, 32)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,32,32).astype('float32')#/255\n",
    "        return pixels[0:2,:,:,:]\n",
    "    def _t_labels(path):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        \n",
    "        folder = path + \"/t_labels/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(32, 32)) for f in os.listdir(folder)]\n",
    "        #onlyfiles = [cv2.resize(image.imread(folder+f),(32, 32)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,32,32).astype('float32') #/255\n",
    "        return pixels[0:2,:,:,:]\n",
    "\n",
    "    print(\"Train Images : Loading . . .\")\n",
    "    train_images = _images(path)\n",
    "    print(\"Train Labels : Loading . . .\")\n",
    "    train_labels = _labels(path)\n",
    "    print(\"Test Images  : Loading . . .\")\n",
    "    test_images = _t_images(path)\n",
    "    print(\"Test Labels  : Loading . . .\")\n",
    "    test_labels = _t_labels(path)\n",
    "    print(\"Done!\")\n",
    "    return train_images, train_labels , test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images : Loading . . .\n",
      "Train Labels : Loading . . .\n",
      "Test Images  : Loading . . .\n",
      "Test Labels  : Loading . . .\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, test_images, test_labels= mnist()  #, test_images, test_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 32, 32)\n",
      "(1, 1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "ch = 1 #Number of channels\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape) # ONE-HOT !!!\n",
    "#print(train_images.T.shape)\n",
    "#print(train_labels.reshape((train_images.shape[0], 10)).shape)\n",
    "\n",
    "##### Getting Ready for the Conv Layers #####\n",
    "#train_images = train_images.reshape(train_images.shape[0], ch, 28, 28).squeeze()\n",
    "#test_images = test_images.reshape(test_images.shape[0], ch, 28, 28).squeeze()\n",
    "\n",
    "#############################################\n",
    "\n",
    "#y = train_images[1].reshape(1,28,28)\n",
    "#print(y.squeeze().shape)\n",
    "'''\n",
    "temp = []\n",
    "#train_labels = train_labels.sum(1)\n",
    "for i in range(int(len(train_labels[:]))):\n",
    "    temp.append(list(train_labels[i][:]).index(1))\n",
    "    \n",
    "    \n",
    "train_labels = np.array(temp.copy())\n",
    "#print(train_labels[0:5])\n",
    "'''\n",
    "#plt.imshow(train_images[0].squeeze(), cmap='Greys_r');\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[0].squeeze(), cmap='Greys_r');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_images[0].squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Filter/Parameter Initializattions  ##################\n",
    "\n",
    "def init_filters(layers, init_n_f ,trim):\n",
    "    filters = []\n",
    "    bias = []\n",
    "    f_dc = []\n",
    "    \n",
    "    trimf = trim\n",
    "    trimb = trim*5\n",
    "    n_f = init_n_f #initial number of filters/kernels\n",
    "    ch_in = 1      #input channels\n",
    "    \"\"\"\n",
    "    for i in range(layers):\n",
    "        if(i != 0):\n",
    "            n_f = n_f*2 #16,32,64,128,256\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trimb\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "    \n",
    "    #Deconvolution filters    \n",
    "    for i in range(1,layers):\n",
    "        n_f = n_f//2 #128,64,32,16\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        fdc = np.random.randn(n_f,ch_in,2,2)*trimf #upsampling filter, its result will be conc with conv4 output so the channels will be doubled again\n",
    "        bdc = np.random.randn(fdc.shape[0],1)* trimb\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trimb\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        dc = [fdc, bdc]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "        f_dc.append(dc)\n",
    "        \n",
    "    \"\"\"\n",
    "    trimbr = trim\n",
    "    locbr = 0\n",
    "    for i in range(layers):\n",
    "        if(i != 0):\n",
    "            n_f = n_f*2 #16,32,64,128,256\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.normal(loc = locbr, scale = trimbr , size = f1) #np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.normal(loc = locbr, scale = trimbr , size = (f1.shape[0],1)) #np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.normal(loc = locbr, scale = trimbr , size = f2)\n",
    "        b2 = np.random.normal(loc = locbr, scale = trimbr , size = (f2.shape[0],1))\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "    \n",
    "    #Deconvolution filters    \n",
    "    for i in range(1,layers):\n",
    "        n_f = n_f//2 #128,64,32,16\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        fdc =  np.random.normal(loc = locbr, scale = trimbr , size = (n_f,ch_in,2,2))#upsampling filter, its result will be conc with conv4 output so the channels will be doubled again\n",
    "        bdc = np.random.normal(loc = locbr, scale = trimbr , size = (fdc.shape[0],1))\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.normal(loc = locbr, scale = trimbr , size = (n_f, ch_in, 3, 3))\n",
    "        b1 = np.random.normal(loc = locbr, scale = trimbr , size = (f1.shape[0],1))\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.normal(loc = locbr, scale = trimbr , size = f2)\n",
    "        b2 = np.random.normal(loc = locbr, scale = trimbr , size = (f2.shape[0],1))\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        dc = [fdc, bdc]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "        f_dc.append(dc)\n",
    "    return filters, bias, f_dc   \n",
    "\n",
    "\n",
    "def init_groupnorm_params(bias, out_b, norm_batch, locbr, trimbr):\n",
    "    [b1,b2,b3,b4,b5,b6,b7,b8,b9]= bias \n",
    "    \n",
    "    \n",
    "    t_1,_ = b1\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma1_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) #MAKE IT FLOAT\n",
    "    beta1_1  = np.random.normal( scale = trimbr , size = gb_size)\n",
    "    gamma1_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta1_2  = np.random.normal( scale = trimbr , size = gb_size)\n",
    "    tempg_1 = [gamma1_1,gamma1_2]\n",
    "    tempb_1 = [beta1_1,beta1_2]\n",
    "    \n",
    "    t_1,_ = b2\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma2_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta2_1  = np.random.normal( scale = trimbr , size = gb_size)\n",
    "    gamma2_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta2_2  = np.random.normal(scale = trimbr , size = gb_size)  \n",
    "    tempg_2 = [gamma2_1,gamma2_2]\n",
    "    tempb_2 = [beta2_1,beta2_2]\n",
    "    \n",
    "    t_1,_ = b3\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma3_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta3_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma3_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta3_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_3 = [gamma3_1,gamma3_2]\n",
    "    tempb_3 = [beta3_1,beta3_2]\n",
    "    \n",
    "    t_1,_ = b4\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma4_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta4_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma4_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta4_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_4 = [gamma4_1,gamma4_2]\n",
    "    tempb_4 = [beta4_1,beta4_2]\n",
    "    \n",
    "    t_1,_ = b5\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma5_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta5_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma5_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta5_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_5 = [gamma5_1,gamma5_2]\n",
    "    tempb_5 = [beta5_1,beta5_2]\n",
    "    \n",
    "    t_1,_ = b6\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma6_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta6_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma6_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta6_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_6 = [gamma6_1,gamma6_2]\n",
    "    tempb_6 = [beta6_1,beta6_2]\n",
    "    \n",
    "    t_1,_ = b7\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma7_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta7_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma7_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta7_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_7 = [gamma7_1,gamma7_2]\n",
    "    tempb_7 = [beta7_1,beta7_2]\n",
    "    \n",
    "    t_1,_ = b8\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma8_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta8_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma8_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta8_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_8 = [gamma8_1,gamma8_2]\n",
    "    tempb_8 = [beta8_1,beta8_2]\n",
    "    \n",
    "    t_1,_ = b9\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma9_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta9_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma9_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta9_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_9 = [gamma9_1,gamma9_2]\n",
    "    tempb_9 = [beta9_1,beta9_2]\n",
    "    \n",
    "    ga =[tempg_1,tempg_2,tempg_3,tempg_4,tempg_5,tempg_6, tempg_7,tempg_8,tempg_9]\n",
    "    be =[tempb_1,tempb_2,tempb_3,tempb_4,tempb_5,tempb_6, tempb_7,tempb_8,tempb_9]\n",
    "    \n",
    "    gamma_out = np.random.normal(loc = locbr, scale = trimbr , size = (out_b.shape[0]//norm_batch,1))\n",
    "    beta_out =   np.random.normal(scale = trimbr , size = (out_b.shape[0]//norm_batch,1))\n",
    "    \n",
    "    return ga, be , gamma_out, beta_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation Functions ###\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UTILITIES #############\n",
    "\n",
    "def conv(image, params, s = 1, pad = 1 ): # s = 1 (conv stride)\n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    [f, b] = params\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((image.shape[0],image.shape[1]+2*pad ,image.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = image\n",
    "        image = tmp    \n",
    "        \n",
    "    f_num = f.shape[0]\n",
    "    f_depth = f.shape[1] #f1 = 1 , f2 = 8\n",
    "    f_size = f.shape[2] \n",
    "        \n",
    "\n",
    "    h_range = int((image.shape[1] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    np_o = np.zeros((f_num, h_range, w_range))\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                np_o[z, _h, _w] = np.sum(np.multiply(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] , f[z, :, :, :])) + b[z]\n",
    "    \n",
    "    return np_o\n",
    "\n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s = 1, pad = 1 ):\n",
    "    #filt =np.rot90(filt, 2)  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!! A T T E N T I O N !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    f_num, n_c, f, _ = filt.shape\n",
    "    \n",
    "    _ ,h , w = dconv_prev.shape\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((conv_in.shape[0],conv_in.shape[1]+2*pad ,conv_in.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = conv_in\n",
    "        conv_in = tmp\n",
    "    \n",
    "    dconv_in = np.zeros(conv_in.shape)\n",
    "    db = np.zeros((f_num,1))\n",
    "\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                #each entry of the dconv_prev will try to affect the idxs from which was made of.\n",
    "                dfilt[z] += dconv_prev[z, _h, _w] * conv_in[:, _h*s : _h*s + f, _w*s : _w*s + f]\n",
    "                dconv_in[:, _h*s : _h*s + f, _w*s : _w*s + f] += dconv_prev[z, _h, _w] * filt[z]  \n",
    "        db[z] = np.sum(dconv_prev[z])  #, axis =1) ## AXIS?\n",
    "    \n",
    "    if(pad >=1 ):\n",
    "        dconv_in = dconv_in[:, pad:-pad, pad:-pad]  # Cropping\n",
    "        \n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "def convTransp1(image, params, s = 2, pad = 1):\n",
    "    [f, b] = params\n",
    "    n_f, n_c, f_s, _ = f.shape\n",
    "    input_s = image.shape[1]\n",
    "    target_dim = input_s*2\n",
    "    res = np.zeros((n_f, target_dim, target_dim))\n",
    "    temp =np.zeros((n_c, target_dim, target_dim))\n",
    "    for z in range(n_f):\n",
    "        for _h in range(input_s):      \n",
    "            for _w in range(input_s):\n",
    "                temp[:, _h*s:_h*s+f_s, _w*s:_w*s+f_s] += image[:, _h, _w].reshape(n_c,1,1)*f[z,:,:,:] #bias will be added at the end\n",
    "        res[z] = np.sum(temp , axis = 0) + b[z]\n",
    "    return res, image\n",
    "\n",
    "def convTranspBackward1(dconv_prev, new_in, filt, s = 2):\n",
    "    n_f, n_c, f_s, _ = filt.shape\n",
    "    _, input_s, _ = new_in.shape\n",
    "    #final_dim = (new_in.shape[1] - 2)//2 + 1 \n",
    "    dc_s=dconv_prev.shape[1]\n",
    "    temp = np.zeros((n_c,dc_s,dc_s))\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dconv_in = np.zeros(new_in.shape)\n",
    "    db = np.zeros((n_f,1))\n",
    "    for z in range(n_f):\n",
    "        for _h in range(input_s):      \n",
    "            for _w in range(input_s): \n",
    "                dfilt[z] += dconv_prev[z, _h*s:_h*s+f_s, _w*s:_w*s+f_s]*new_in[:,_h,_w].reshape(n_c,1,1)\n",
    "                temp[:, _h*s:_h*s+f_s, _w*s:_w*s+f_s] += dconv_prev[z, _h*s:_h*s+f_s, _w*s:_w*s+f_s] * filt[z]\n",
    "                for ch in range(n_c):\n",
    "                    dconv_in[ch, _h, _w] += np.sum(temp[ch, _h*s:_h*s+f_s, _w*s:_w*s+f_s])\n",
    "        db[z] = np.sum(dconv_prev[z])        \n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "    \n",
    "def convTransp(image, params, s = 1, pad = 1):\n",
    "    #s is always 1, upsample kernel = 2\n",
    "    #zero insertions between pixels s_downsampled -1 = 2-1 = 1\n",
    "    #required padding in order to double my dimensions with the given data:\n",
    "    #(i-1)*2 + k -2p = output size, where our padding is k - p -1 = 2-0-1=1(we assume p=0)\n",
    "    [f, b]=params\n",
    "    f = np.rot90(f, 1, (2,3))\n",
    "    params = [f, b]\n",
    "    input_s = image.shape[1]\n",
    "    target_dim = input_s*2 #final dim, after conv\n",
    "    required_pad = 1 #always for filter 2x2 ,stride 1, zero insertion 1 and main target to double dim\n",
    "    #make our new custom input\n",
    "    size = input_s*2 +1\n",
    "    new_in = np.zeros((image.shape[0], size, size))\n",
    "    \n",
    "    ### OR just: np.pad(image[:,:,:],2,'constant') # Important, we must loop with respect to the 1st dim\n",
    "    for i in range(1, target_dim, 2):\n",
    "        for j in range(1, target_dim, 2):\n",
    "                new_in[:, i, j] = image[:, i//2, j//2]\n",
    "    #now we do a normal convolution(pad = 0)\n",
    "    res = conv(new_in, params, 1, 0) #thats the final result with target_dim\n",
    "    return res, new_in # we will need new_in so we wont calc it again for the backprop\n",
    "    \n",
    "def convTranspBackward(dconv_prev, new_in, filt, s = 1):\n",
    "    #First, we do a backward convolution on new_in,d_conv_prev,\n",
    "    #then we will choose form the d_conv_new the entries that match the initial 'smaller' input image\n",
    "    #by selecting the odd matrix cells 1,3,5... because we had applied a standard pad=1,zero inser=1\n",
    "    dconv_in, dfilt, db = convolutionBackward(dconv_prev, new_in, filt,1,0)\n",
    "    #Now its time to choose the right entries to build the gradients of the initial input image\n",
    "    dim = dconv_in.shape[1]\n",
    "    final_dim = (new_in.shape[1] - 2)//2 + 1 #based on dimen of image before final conv that gives the result,..\n",
    "    #e.g. for new_in 7x7 that is going to convoluted with a 2x2 kernel and give a 6x6 upsampled from 3x3 init image\n",
    "    # now from this 7x7 --> apply the formula above,we get the 3x3 dimension number\n",
    "    res = np.zeros((dconv_in.shape[0], final_dim, final_dim))\n",
    "    for i in range(1, dim, 2):\n",
    "        for j in range(1, dim, 2):\n",
    "                res[:, i//2, j//2] = dconv_in[:, i, j]\n",
    "    return res, dfilt, db\n",
    "    \n",
    "    \n",
    "def maxpool(image, f=2 , s=2):\n",
    "    \n",
    "    h_range = int((image.shape[1] - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f) / s) + 1     # (W - F + 2P) / S \n",
    "    out = np.zeros((image.shape[0], h_range, w_range))\n",
    "    \n",
    "    for z in range(image.shape[0]): # Number of channels\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                out[z, _h, _w] = np.max(image[z, _h*s : _h*s + f, _w*s : _w*s + f])\n",
    "    return out\n",
    "\n",
    "def nanargmax(arr):\n",
    "    #print(arr.shape)\n",
    "    try:\n",
    "        idx = np.nanargmax(arr)\n",
    "        #print (idx)\n",
    "    except:\n",
    "        idx = 0\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "def maxpoolBackward(dpool, conv, f=2 , s=2):\n",
    "    num_c, h, w = conv.shape\n",
    "    h = int((h - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w = int((w - f) / s) + 1     # (W - F + 2P) / S \n",
    "    \n",
    "    dout = np.zeros(conv.shape)\n",
    "    #print(conv.shape)\n",
    "    for z in range(num_c): # Number of channels\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                (a, b) = nanargmax(conv[z, _h*s : _h*s + f, _w*s : _w*s + f]) #Getting the indexes from the max value in this area\n",
    "                #put it on the new array\n",
    "                dout[z, _h + a, _w + b] = dpool[z, _h, _w]\n",
    "    \n",
    "    \n",
    "    return dout\n",
    "\n",
    "\n",
    "def reshape(img, reshape_dim):\n",
    "    pad = reshape_dim - img.shape[1]\n",
    "    if(pad == 0):\n",
    "        return img\n",
    "    res = np.zeros((img.shape[0], reshape_dim, reshape_dim))\n",
    "    if(pad > 1):\n",
    "        res[:, pad//2:-(pad//2), pad//2:-(pad//2)] = img\n",
    "    else:\n",
    "        res[:, 0:-(pad), 0:-(pad)] = img\n",
    "    return res\n",
    "\n",
    "\n",
    "def crop(img, crop_dim): #TODO : look at it..future upgrade to downsample..maybe\n",
    "    start = img.shape[1]//2 - crop_dim//2\n",
    "    return img[:,(start):(start +crop_dim),(start):(start +crop_dim)]\n",
    "\n",
    "    \n",
    "    \n",
    "def crop2half(img):\n",
    "    #return gradients for decoder side and gradients for encoder side\n",
    "    n_ch = img.shape[0]//2\n",
    "    return img[n_ch:n_ch*2 ,:,:], img[0:n_ch,:,:]\n",
    "    \n",
    "def concat(img1_true, img2):\n",
    "    n_ch = img1_true.shape[0]\n",
    "    dim = img1_true.shape[1]\n",
    "    if(img2.shape[1] != dim):\n",
    "        img2 = crop(img2, dim)\n",
    "        print(\"Warning: Extra crop needed and handled!(%d --> %d)\" %(dim, img2.shape[1]))\n",
    "    res = np.zeros((n_ch*2, dim, dim))\n",
    "    res[0:n_ch,:,:] = img2\n",
    "    res[n_ch:n_ch*2 ,:,:] = img1_true\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cost Calculations ######\n",
    "def Cross_Entropy(logs, targets):  # Pixel-Wise Cross entropy --> average accuracy\n",
    "    out = np.zeros(logs.shape)\n",
    "    mylen = logs.shape[0]*logs.shape[1]*logs.shape[2]\n",
    "    #print(len(targets))\n",
    "    #logs.shape (dim x dim) like target\n",
    "    for i in range(logs.shape[1]):\n",
    "        for j in range(logs.shape[2]):\n",
    "            if(targets[:,i,j] >= logs[:,i,j]):#Gray and above\n",
    "                out[:,i,j] = logs[:,i,j]/targets[:,i,j] \n",
    "            else:\n",
    "                out[:,i,j] = (1 - logs[:,i,j])/(1 - targets[:,i,j]) # e.g if logs[i,j]= 0.4 and we want(target) 0 --> 1-0.4=0.6 prob. for zero \n",
    "    #or\n",
    "    #out = targets*logs + ((-1*(targets-1)) - (-1*(targets-1))*logs)\n",
    "    res =out.sum()/mylen\n",
    "    return -np.log(res),res\n",
    "\n",
    "\n",
    "def Dice_Coef(logs, targets):\n",
    "    out = np.zeros(logs.shape)\n",
    "    mylen = logs.shape[0]*logs.shape[1]*logs.shape[2]\n",
    "    #Apply Dice coefficient\n",
    "    numerator = (logs*targets)\n",
    "    denominator = logs + targets\n",
    "    loss = 1 - (2*np.sum(numerator))/(np.sum(denominator))\n",
    "    return loss, np.exp(-loss)\n",
    "                \n",
    "    \n",
    "    \n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    \n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost) ### ATTENTION!\n",
    "\n",
    "def normalize(output):\n",
    "    output[output<-4]=-4\n",
    "    output[output>4] = 4\n",
    "    return output\n",
    "\n",
    "###### Accuracy Calculation ######\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupnorm_forward(X, gamma_, beta_, eps=1e-5):\n",
    "    \"\"\"\n",
    "    # extract the dimensions\n",
    "    C, H, W = X.shape\n",
    "    # mini-batch mean\n",
    "    mean = nd.mean(X, axis=(1,2))\n",
    "    # mini-batch variance\n",
    "    variance = nd.mean((X - mean.reshape((C, 1, 1))) ** 2, axis=(1, 2))\n",
    "    # normalize\n",
    "    X_hat = (X - mean.reshape((C, 1, 1))) * 1.0 / nd.sqrt(variance.reshape((C, 1, 1)) + eps)\n",
    "    #if is_training:\n",
    "    # while training, we normalize the data using its mean and variance\n",
    "    X_hat = (X - mean.reshape((C, 1, 1))) * 1.0 / nd.sqrt(variance.reshape((C, 1, 1)) + eps)\n",
    "    #else:\n",
    "    # while testing, we normalize the data using the pre-computed mean and variance\n",
    "    #    X_hat = (X - _BN_MOVING_MEANS[scope_name].reshape((1, C, 1, 1))) * 1.0 \\\n",
    "    #        / nd.sqrt(_BN_MOVING_VARS[scope_name].reshape((1, C, 1, 1)) + eps)\n",
    "    # scale and shift\n",
    "    out = gamma.reshape((C, 1, 1)) * X_hat + beta.reshape((C, 1, 1))\n",
    "    \"\"\"\n",
    "    C_all=X.shape[0]\n",
    "    \n",
    "    if(C_all == 1):\n",
    "        batch = 1\n",
    "    else:\n",
    "        batch =2\n",
    "    C= batch\n",
    "    \n",
    "    mu_= np.zeros(C_all//batch)\n",
    "    var_=np.zeros(C_all//batch)\n",
    "    xmu_=np.zeros((C_all,X.shape[1],X.shape[2]))\n",
    "    sqrtvar_= np.zeros(C_all//batch)\n",
    "    ivar_= np.zeros(C_all//batch)\n",
    "    xhat_= np.zeros((C_all,X.shape[1],X.shape[2]))\n",
    "    #gammax_= np.zeros((C_all,1,1))\n",
    "    out_= np.zeros((C_all,X.shape[1],X.shape[2]))\n",
    "    \n",
    "    \n",
    "    for i in range(0, C_all, batch):\n",
    "        \n",
    "        x = X[i:i+C,:,:]\n",
    "        gamma = gamma_[i//batch]  #there is a gamma,beta for each batch of channels\n",
    "        beta = beta_[i//batch]\n",
    "        ###################################################################\n",
    "        _, H, W = x.shape  #WAS N, D\n",
    "\n",
    "        #step1: calculate mean\n",
    "        mu = np.mean(x) #scalar\n",
    "        #print(mu)\n",
    "\n",
    "        #step2: subtract mean vector of every trainings example\n",
    "        xmu = (x - mu)\n",
    "        #step3: following the lower branch - calculation denominator\n",
    "        #step4: calculate variance\n",
    "        var = np.mean(xmu ** 2)\n",
    "\n",
    "        #step5: add eps for numerical stability, then sqrt\n",
    "        sqrtvar = np.sqrt(var + eps)\n",
    "\n",
    "        #step6: invert sqrtwar\n",
    "        ivar = 1./sqrtvar\n",
    "\n",
    "        #step7: execute normalization\n",
    "        xhat = xmu * ivar\n",
    "\n",
    "        #step8: Nor the two transformation steps\n",
    "        gammax = gamma * xhat\n",
    "        #gamma,beta : scalar\n",
    "        #step9\n",
    "        out = gammax + beta\n",
    "        \n",
    "        xhat_[i:i+C,:,:]   =xhat   #.copy()\n",
    "        #gamma_[i:i+2,:,:]  =gamma\n",
    "        xmu_[i:i+C,:,:]    =xmu\n",
    "        ivar_[i//batch]  =ivar\n",
    "        sqrtvar_[i//batch]=sqrtvar\n",
    "        var_[i//batch]   =var\n",
    "        out_[i:i+C,:,:]   =out\n",
    "    #store intermediate\n",
    "    cache = (xhat_,gamma_,xmu_,ivar_,sqrtvar_,var_,eps)\n",
    "    return out_, cache\n",
    "\n",
    "def groupnorm_backward(dout_, cache):\n",
    "\n",
    "    #unfold the variables stored in cache\n",
    "    xhat_,gamma_,xmu_,ivar_,sqrtvar_,var_,eps = cache\n",
    "\n",
    "    \n",
    "    C_all =dout_.shape[0]\n",
    "    if(C_all == 1):\n",
    "        C = 1\n",
    "    else:\n",
    "        C = 2\n",
    "    \n",
    "    batch = C\n",
    "    dx_    = np.zeros((C_all,dout_.shape[1],dout_.shape[2]))\n",
    "    dgamma_= np.zeros(C_all//batch)\n",
    "    dbeta_ = np.zeros(C_all//batch)\n",
    "    \n",
    "    for i in range(0, C_all, batch): \n",
    "        dout = dout_[i:i+C,:,:]\n",
    "        xhat   =xhat_[i:i+C,:,:]\n",
    "        gamma  = gamma_[i//batch]\n",
    "        xmu    =xmu_[i:i+C,:,:]\n",
    "        ivar   =ivar_[i//batch]\n",
    "        sqrtvar=sqrtvar_[i//batch]\n",
    "        var    =var_[i//batch]\n",
    "        \n",
    "        #get the dimensions of the input/output\n",
    "        _, H, W = dout.shape #N,D = dout.shape\n",
    "\n",
    "        #step9\n",
    "        dbeta = np.sum(dout)\n",
    "        dgammax = dout #not necessary, but more understandable\n",
    "\n",
    "        #step8\n",
    "        dgamma = np.sum(dgammax*xhat)\n",
    "        dxhat = dgammax * gamma\n",
    "\n",
    "        #step7\n",
    "        divar = np.sum(dxhat*xmu)\n",
    "        dxmu1 = dxhat * ivar\n",
    "\n",
    "        #step6\n",
    "        dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "\n",
    "        #step5\n",
    "        dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
    "\n",
    "        #step4\n",
    "        dsq = 1./(batch*H*W) * np.ones((C,H,W)) * dvar  #1./C\n",
    "\n",
    "        #step3\n",
    "        dxmu2 = 2 * xmu * dsq\n",
    "\n",
    "        #step2\n",
    "        dx1 = (dxmu1 + dxmu2)\n",
    "        dmu = -1 * np.sum(dxmu1+dxmu2)\n",
    "\n",
    "        #step1\n",
    "        dx2 =  1./(batch*H*W) *np.ones((C,H,W)) * dmu #1. /C *\n",
    "\n",
    "        #step0\n",
    "        dx = dx1 + dx2\n",
    "        dx_[i:i+C,:,:]    = dx\n",
    "        dgamma_[i//batch]= dgamma\n",
    "        dbeta_[i//batch] = dbeta\n",
    "\n",
    "    return dx_, dgamma_.reshape(C_all//batch,1), dbeta_.reshape(C_all//batch,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validate(X, Y, params, GN):\n",
    "    ### Unpacking ###\n",
    "    [filters, bias, f_dc, out_fb, GN_params] = params\n",
    "    [ga, be, gamma_out, beta_out] = GN_params\n",
    "    [f1,f2,f3,f4,f5,f6,f7,f8,f9] = filters\n",
    "    [b1,b2,b3,b4,b5,b6,b7,b8,b9]= bias \n",
    "    [fb6_dc, fb7_dc, fb8_dc, fb9_dc] = f_dc\n",
    "    [out_f, out_b] = out_fb\n",
    "    #################\n",
    "    \n",
    "    \n",
    "    dropout = 0\n",
    "    print('Calculating Forward step . . .')\n",
    "    \n",
    "    batch = 1\n",
    "    for c in range(0, X.shape[0], batch):\n",
    "        if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "            batch = X.shape[0] - c\n",
    "        X_t = X[c:(c + batch)]\n",
    "        Y_t = Y[c:(c + batch)]\n",
    "        for b in range(batch):\n",
    "            ########### 1st Big Layer ###########    (with zero padding ='same',so with stride =1 we get same dim as the input)\n",
    "            conv1_1, conv1_2, normcache1_1, normcache1_2 = Conv_Block(\"Forward\", f1, b1, X_t[b], dropout, GN, ga[0], be[0])\n",
    "            ##################################### conv1_2: 128x128x16\n",
    "\n",
    "            pl1 = maxpool(conv1_2, 2, 2) #   pl1 : (128-2)/2+1  = 64 \n",
    "\n",
    "            ########### 2nd Big Layer ########### \n",
    "            conv2_1, conv2_2, normcache2_1, normcache2_2 = Conv_Block(\"Forward\", f2, b2, pl1, dropout, GN, ga[1], be[1])          \n",
    "            #####################################  64x64x32\n",
    "\n",
    "            pl2 = maxpool(conv2_2, 2, 2) #pool_f = 2 , pool_s = 2    , (64 -2)/2 +1 = 32\n",
    "\n",
    "            ########### 3rd Big Layer ###########\n",
    "            conv3_1, conv3_2, normcache3_1, normcache3_2 = Conv_Block(\"Forward\", f3, b3, pl2, dropout, GN, ga[2], be[2])          \n",
    "            #####################################  32x32x64\n",
    "\n",
    "            pl3 = maxpool(conv3_2, 2, 2) #pool_f = 2 , pool_s = 2   ,  (32-2)/2 +1 = 16\n",
    "\n",
    "            ########### 4th Big Layer ###########\n",
    "            conv4_1, conv4_2, normcache4_1, normcache4_2 = Conv_Block(\"Forward\", f4, b4, pl3, dropout, GN, ga[3], be[3])             \n",
    "            #####################################     16x16x128\n",
    "\n",
    "            pl4 = maxpool(conv4_2, 2, 2) #pool_f = 2 , pool_s = 2  , (16-2)/2 +1 =8  : 8x8x128\n",
    "\n",
    "            ########### 5th Big Layer ###########   8x8x128-->8x8x256\n",
    "            conv5_1, conv5_2, normcache5_1, normcache5_2 = Conv_Block(\"Forward\", f5, b5, pl4, dropout, GN, ga[4], be[4])       \n",
    "            #####################################  8x8x256\n",
    "\n",
    "            #####################################\n",
    "            #Because of ambigious size after the upsampling the concat func must take care possible crop of the conv#_2 \n",
    "            #####################################\n",
    "            #Deconvolution/Upsampling\n",
    "            # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "            params = [fb6_dc[0], fb6_dc[1]] # deconv filter, deconv bias\n",
    "            dc6, new_in6 = convTransp(conv5_2, params, 1, 0)   #result:   =  16x16x128 , # conv5_2 requires NO crop\n",
    "            #Concat dc6 with conv4_2 so we get 256 channels (16x16x256)\n",
    "            c6 = concat(dc6, conv4_2) # 1st one is the right one size  \n",
    "\n",
    "            ########### 6th Big Layer ###########          16x16x256     \n",
    "            conv6_1, conv6_2, normcache6_1, normcache6_2 = Conv_Block(\"Forward\", f6, b6, c6, dropout, GN, ga[5], be[5])  \n",
    "            #####################################    16x16x128\n",
    "            #(16-1)*2 + 2 =32\n",
    "            params = [fb7_dc[0], fb7_dc[1]] # deconv filter, deconv bias\n",
    "            dc7, new_in7 = convTransp(conv6_2, params, 1, 0)   #result:   =  32x32x64\n",
    "            #Concat dc7 with conv3_2 so we get  channels (32x32x128)\n",
    "            c7 = concat(dc7, conv3_2)   \n",
    "\n",
    "            ########### 7th Big Layer ###########          32x32x128     \n",
    "            conv7_1, conv7_2, normcache7_1, normcache7_2 = Conv_Block(\"Forward\", f7, b7, c7, dropout, GN, ga[6], be[6]) \n",
    "            #####################################    32x32x64\n",
    "            #(24-1)*2 + 2 = 48\n",
    "            params = [fb8_dc[0], fb8_dc[1]] # deconv filter, deconv bias\n",
    "            dc8, new_in8 = convTransp(conv7_2, params, 1, 0)   #result:   =  64x64x32\n",
    "            #Concat dc8 with conv2_2 so we get  channels (64x64x64)\n",
    "            c8 = concat(dc8 ,conv2_2)   \n",
    "\n",
    "            ########### 8th Big Layer ###########          64x64x64    \n",
    "            conv8_1, conv8_2, normcache8_1, normcache8_2 = Conv_Block(\"Forward\", f8, b8, c8, dropout, GN, ga[7], be[7])\n",
    "            #####################################    64x64x32                              \n",
    "            #(64-1)*2 + 2 = 128\n",
    "            params = [fb9_dc[0], fb9_dc[1]] # deconv filter, deconv bias\n",
    "            dc9, new_in9 = convTransp(conv8_2, params, 1, 0)   #result:   =  128x128x16\n",
    "            #Concat dc9 with conv1_2 so we get  channels (128x128x32)\n",
    "            c9 = concat(dc9, conv1_2)                   \n",
    "\n",
    "            ########### 9th Big Layer ###########          128x128x32   \n",
    "            conv9_1, conv9_2, normcache9_1, normcache9_2 = Conv_Block(\"Forward\", f9, b9, c9, dropout, GN, ga[8], be[8])\n",
    "            #####################################    128x128x16\n",
    "\n",
    "            ############################# Last Layer conv(1x1) --> 128x128x1 ##########################\n",
    "            params = [out_f, out_b]\n",
    "            output = conv(conv9_2, params, 1, 0) #output.shape: 128x128x1\n",
    "\n",
    "            #print(output[:,0:10,0:10])\n",
    "            output = normalize(output)\n",
    "            ## Sigmoid ##\n",
    "            Y_hat = sigmoid(output)\n",
    "            \n",
    "            Y_hat[Y_hat>0.65]=1\n",
    "            Y_hat[Y_hat<0.35]=0\n",
    "\n",
    "            plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "            cost_,accuracy_ = Dice_Coef(Y_hat, Y_t[b])#Cross_Entropy(Y_hat, Y_t[b])\n",
    "            cost = cost_\n",
    "            accuracy = accuracy_\n",
    "            print(\"Cost: {:.2f}   -   Accuracy: {:.2f}%\".format(cost/batch, (accuracy*100)/batch))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv_Block(step, f, b, myin, dropout, GN, ga, be):\n",
    "    if(step == \"Forward\"):\n",
    "        bc1 = 0\n",
    "        bc2 = 0\n",
    "        ### DROPOUT ###\n",
    "        if(dropout>0):\n",
    "            d = (np.random.rand(myin.shape[0],myin.shape[1],myin.shape[2])<dropout)\n",
    "            d = d*1 #Bool --> int(0s and 1s)\n",
    "            myin = d*myin\n",
    "        ###############\n",
    "        params = [f[0], b[0]]  \n",
    "        conv1 = conv(myin, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "        ### GROUP NORM ###\n",
    "        if(GN == 1):\n",
    "            conv1, bc1 = groupnorm_forward(conv1, ga[0], be[0]) \n",
    "        ##################\n",
    "        conv1[conv1<=0] = 0 #Relu\n",
    "\n",
    "        params = [f[1], b[1]]\n",
    "        conv2 = conv(conv1, params, 1)\n",
    "        ### GROUP NORM ###\n",
    "        if(GN == 1):\n",
    "            conv2, bc2 = groupnorm_forward(conv2, ga[1], be[1]) \n",
    "        ##################\n",
    "        conv2[conv2<=0] = 0 #Relu\n",
    "        return conv1, conv2, bc1, bc2\n",
    "    else: #Backward\n",
    "        if(isinstance(GN, int)):\n",
    "            dconv_prev = b\n",
    "            conv_prev = myin\n",
    "            conv_prev1 = dropout\n",
    "            conc = ga\n",
    "            dconv_prev[conv_prev<=0] = 0\n",
    "            dconv1, df2, db2 = convolutionBackward(dconv_prev, conv_prev1, f[1], 1) #\n",
    "            #pack data\n",
    "            dconv1[conv_prev1<=0] = 0\n",
    "            conc_dconv1, df1, db1 = convolutionBackward(dconv1, conc, f[0], 1) #\n",
    "            return conc_dconv1, df2, db2, df1, db1\n",
    "        else:\n",
    "            dconv_prev = b\n",
    "            conv_prev = myin\n",
    "            conv_prev1 = dropout\n",
    "            conc = GN\n",
    "            normcache1 = ga\n",
    "            normcache2 = be\n",
    "            dconv_prev[conv_prev<=0] = 0 \n",
    "            dconv_prev, dgamma1_2, dbeta1_2 = groupnorm_backward(dconv_prev, normcache2)\n",
    "            dconv1_1, df1_2, db1_2 = convolutionBackward(dconv_prev, conv_prev1, f[1], 1) #\n",
    "            #pack data\n",
    "            dconv1_1[conv_prev1<=0] = 0\n",
    "            dconv1_1, dgamma1_1, dbeta1_1 = groupnorm_backward(dconv1_1, normcache1)\n",
    "            dga= [dgamma1_1,dgamma1_2]\n",
    "            dbe= [dbeta1_1,dbeta1_2]\n",
    "            conc_dconv1, df1_1, db1_1 = convolutionBackward(dconv1_1, conc, f[0], 1) #C9 is not needed for input,we know how to select the right gradients   \n",
    "            return conc_dconv1, df1_2, db1_2, df1_1, db1_1, dga, dbe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  TRAIN  ######\n",
    "#import torch\n",
    "import time\n",
    "time.time()\n",
    "\n",
    "\n",
    "def train(X, Y, epochs, learning_rate, dropout, GN):\n",
    "    verbose=True\n",
    "    # initiation of neural net parameters\n",
    "    #params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    trim = 0.0001\n",
    "    #f1,f2,f3,f4,f5,fd1,fd2,fd3,fd4\n",
    "    filters,bias, f_dc = init_filters(5, 16, trim) #Double the channel-filter 4 times  (up to 256 and back again)\n",
    "    ##Final 1x1 filter\n",
    "    \n",
    "    out_f = np.random.randn(1,16,1,1)*trim\n",
    "    out_b = np.random.randn(out_f.shape[0],1)*trim  \n",
    "    out_fb = [out_f, out_b]\n",
    "    \n",
    "    ### Initialize group normalization parameters\n",
    "    ga, be, gamma_out, beta_out = init_groupnorm_params(bias, out_b, 2, 0, 0.05)#norm_batch, lockbr, trimbr\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    if(GN>0):\n",
    "        print(\"Group Normalization Enabled!\")\n",
    "    else:\n",
    "        print(\"Group Normalization Disabled!\")\n",
    "    if(dropout>0):\n",
    "        print(\"Dropout Enabled! -  Value: {}\".format(dropout))\n",
    "    else:\n",
    "        print(\"Dropout Disabled!\")\n",
    "    print(\"Learning rate: {}\".format(learning_rate))\n",
    "    print(\"Dataset Size: {}\".format(X.shape[0]))\n",
    "    print(\"Weight scale: {}\".format(trim))\n",
    "    print(\"************************************\")\n",
    "    \n",
    "    #filter shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "\n",
    "    v_adam =  [] #filter1,filter2\n",
    "    s_adam =  [] \n",
    "    bv_adam = [] #bias1,bias2\n",
    "    bs_adam = []\n",
    "    fdc_v_adam=[] #filter,bias\n",
    "    fdc_s_adam=[]\n",
    "    #format: [v1,v2,v3,v4,v5,vd1,vd2,vd3,vd4] ,same for the rest, each of these include a tuple for the 2 conv filter\n",
    "    #[s1,s2,s3,s4,s5,sd1,sd2,vs3,sd4]\n",
    "    # upsampling filters : [v1_dc,v2_dc,v3_dc]  \n",
    "\n",
    "            \n",
    "\n",
    "    # performing calculations for subsequent iterations\n",
    "    \n",
    "    \n",
    "    [f1,f2,f3,f4,f5,f6,f7,f8,f9] = filters\n",
    "    [b1,b2,b3,b4,b5,b6,b7,b8,b9]= bias \n",
    "    [fb6_dc, fb7_dc, fb8_dc, fb9_dc] = f_dc\n",
    "    \n",
    "    last_acc = 0\n",
    "    for e in range(epochs):\n",
    "        # step forward\n",
    "        running_loss = 0\n",
    "        cost = 0\n",
    "        accuracy = 0\n",
    "        batch = 1\n",
    "        print(\"Epoch: {%d}\" %(e+1))\n",
    "        for c in range(0, X.shape[0], batch):\n",
    "            print('Batch: {}'.format(int(c/batch +1)))\n",
    "            #### Reset Gradients (Every batch) ####\n",
    "            beta1= 0.92\n",
    "            beta2= 0.995\n",
    "            lr = learning_rate\n",
    "            batch_size= batch\n",
    "            if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "                batch = X.shape[0] - c\n",
    "            X_t = X[c:(c + batch)]  # shape:(m, ch, h, w)\n",
    "            Y_t = Y[c:(c + batch)]\n",
    "            \n",
    "            \n",
    "            df =  []\n",
    "            db =  []\n",
    "            dfb=  []\n",
    "            for i in filters:\n",
    "                v1 = np.zeros(i[0].shape)\n",
    "                v2 = np.zeros(i[1].shape)\n",
    "                s1 = np.zeros(i[0].shape)\n",
    "                s2 = np.zeros(i[1].shape)\n",
    "                v_a = [v1, v2]\n",
    "                s_a = [s1, s2]\n",
    "                v_adam.append(v_a)\n",
    "                s_adam.append(s_a)\n",
    "                \n",
    "                df1_t = np.zeros(i[0].shape)\n",
    "                df2_t = np.zeros(i[1].shape)\n",
    "                f_temp = [df1_t, df2_t]\n",
    "                df.append(f_temp)\n",
    "                \n",
    "            for i in bias:\n",
    "                bv1 = np.zeros(i[0].shape)\n",
    "                bv2 = np.zeros(i[1].shape)\n",
    "                bs1 = np.zeros(i[0].shape)\n",
    "                bs2 = np.zeros(i[1].shape)    \n",
    "                bv_a = [bv1, bv2]\n",
    "                bs_a = [bs1, bs2]\n",
    "                bv_adam.append(bv_a)\n",
    "                bs_adam.append(bs_a)\n",
    "                \n",
    "                \n",
    "                db1_t = np.zeros(i[0].shape)\n",
    "                db2_t = np.zeros(i[1].shape)\n",
    "                b_temp = [db1_t, db2_t]\n",
    "                db.append(b_temp)\n",
    "            \n",
    "            for i in f_dc:\n",
    "                fdc_v1 = np.zeros(i[0].shape)\n",
    "                bdc_v2 = np.zeros(i[1].shape)\n",
    "                fdc_s1 = np.zeros(i[0].shape)\n",
    "                bdc_s2 = np.zeros(i[1].shape)    \n",
    "                fdc_v_a = [fdc_v1, bdc_v2]\n",
    "                fdc_s_a = [fdc_s1, bdc_s2]\n",
    "                fdc_v_adam.append(fdc_v_a)\n",
    "                fdc_s_adam.append(fdc_s_a)\n",
    "                \n",
    "                \n",
    "                df1_t = np.zeros(i[0].shape)\n",
    "                db1_t = np.zeros(i[1].shape)\n",
    "                fb_temp = [df1_t, db1_t]\n",
    "                dfb.append(fb_temp)\n",
    "            \n",
    "            \n",
    "            #Final layer 1x1 filter setup\n",
    "\n",
    "            v_out_f = np.zeros(out_f.shape)\n",
    "            s_out_f = np.zeros(out_f.shape)\n",
    "            bv_out_b = np.zeros(out_b.shape)\n",
    "            bs_out_b = np.zeros(out_b.shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "            dout_f = np.zeros(out_f.shape)\n",
    "            dout_b = np.zeros(out_b.shape)\n",
    "            \n",
    "            ######################################\n",
    "            \n",
    "            \n",
    "            #timestamp1 = time.time()\n",
    "            \n",
    "            \n",
    "            [df1,df2,df3,df4,df5,df6,df7,df8,df9] = df\n",
    "            [db1,db2,db3,db4,db5,db6,db7,db8,db9] = db \n",
    "            [dfb6_dc,dfb7_dc,dfb8_dc,dfb9_dc]     = dfb\n",
    "            \n",
    "            \n",
    "            for b in range(batch):\n",
    "                \n",
    "                print('Image: {}/{}'.format((b+1),batch))\n",
    "                #################### TODO: BLOCK IMPLEMENTATION - FUTURE UPDATE ######################\n",
    "                    \n",
    "                    \n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ######################## Forward Propagation Convolution Part  ##########################\n",
    "\n",
    "                \n",
    "                ########### 1st Big Layer ###########    (with zero padding ='same',so with stride =1 we get same dim as the input)\n",
    "                conv1_1, conv1_2, normcache1_1, normcache1_2 = Conv_Block(\"Forward\", f1, b1, X_t[b], dropout, GN, ga[0], be[0])\n",
    "                ##################################### conv1_2: 128x128x16\n",
    "                \n",
    "                pl1 = maxpool(conv1_2, 2, 2) #   pl1 : (128-2)/2+1  = 64 \n",
    "                \n",
    "                ########### 2nd Big Layer ########### \n",
    "                conv2_1, conv2_2, normcache2_1, normcache2_2 = Conv_Block(\"Forward\", f2, b2, pl1, dropout, GN, ga[1], be[1])          \n",
    "                #####################################  64x64x32\n",
    "\n",
    "                pl2 = maxpool(conv2_2, 2, 2) #pool_f = 2 , pool_s = 2    , (64 -2)/2 +1 = 32\n",
    "\n",
    "                ########### 3rd Big Layer ###########\n",
    "                conv3_1, conv3_2, normcache3_1, normcache3_2 = Conv_Block(\"Forward\", f3, b3, pl2, dropout, GN, ga[2], be[2])          \n",
    "                #####################################  32x32x64\n",
    "\n",
    "                pl3 = maxpool(conv3_2, 2, 2) #pool_f = 2 , pool_s = 2   ,  (32-2)/2 +1 = 16\n",
    "\n",
    "                ########### 4th Big Layer ###########\n",
    "                conv4_1, conv4_2, normcache4_1, normcache4_2 = Conv_Block(\"Forward\", f4, b4, pl3, dropout, GN, ga[3], be[3])             \n",
    "                #####################################     16x16x128\n",
    "\n",
    "                pl4 = maxpool(conv4_2, 2, 2) #pool_f = 2 , pool_s = 2  , (16-2)/2 +1 =8  : 8x8x128\n",
    "                \n",
    "                ########### 5th Big Layer ###########   8x8x128-->8x8x256\n",
    "                conv5_1, conv5_2, normcache5_1, normcache5_2 = Conv_Block(\"Forward\", f5, b5, pl4, dropout, GN, ga[4], be[4])       \n",
    "                #####################################  8x8x256\n",
    "\n",
    "                #####################################\n",
    "                #Because of ambigious size after the upsampling the concat func must take care possible crop of the conv#_2 \n",
    "                #####################################\n",
    "                #Deconvolution/Upsampling\n",
    "                # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "                params = [fb6_dc[0], fb6_dc[1]] # deconv filter, deconv bias\n",
    "                dc6, new_in6 = convTransp(conv5_2, params, 1, 0)   #result:   =  16x16x128 , # conv5_2 requires NO crop\n",
    "                #Concat dc6 with conv4_2 so we get 256 channels (16x16x256)\n",
    "                c6 = concat(dc6, conv4_2) # 1st one is the right one size  \n",
    "                \n",
    "                ########### 6th Big Layer ###########          16x16x256     \n",
    "                conv6_1, conv6_2, normcache6_1, normcache6_2 = Conv_Block(\"Forward\", f6, b6, c6, dropout, GN, ga[5], be[5])  \n",
    "                #####################################    16x16x128\n",
    "                #(16-1)*2 + 2 =32\n",
    "                params = [fb7_dc[0], fb7_dc[1]] # deconv filter, deconv bias\n",
    "                dc7, new_in7 = convTransp(conv6_2, params, 1, 0)   #result:   =  32x32x64\n",
    "                #Concat dc7 with conv3_2 so we get  channels (32x32x128)\n",
    "                c7 = concat(dc7, conv3_2)   \n",
    "                \n",
    "                ########### 7th Big Layer ###########          32x32x128     \n",
    "                conv7_1, conv7_2, normcache7_1, normcache7_2 = Conv_Block(\"Forward\", f7, b7, c7, dropout, GN, ga[6], be[6]) \n",
    "                #####################################    32x32x64\n",
    "                #(24-1)*2 + 2 = 48\n",
    "                params = [fb8_dc[0], fb8_dc[1]] # deconv filter, deconv bias\n",
    "                dc8, new_in8 = convTransp(conv7_2, params, 1, 0)   #result:   =  64x64x32\n",
    "                #Concat dc8 with conv2_2 so we get  channels (64x64x64)\n",
    "                c8 = concat(dc8 ,conv2_2)   \n",
    "                \n",
    "                ########### 8th Big Layer ###########          64x64x64    \n",
    "                conv8_1, conv8_2, normcache8_1, normcache8_2 = Conv_Block(\"Forward\", f8, b8, c8, dropout, GN, ga[7], be[7])\n",
    "                #####################################    64x64x32                              \n",
    "                #(64-1)*2 + 2 = 128\n",
    "                params = [fb9_dc[0], fb9_dc[1]] # deconv filter, deconv bias\n",
    "                dc9, new_in9 = convTransp(conv8_2, params, 1, 0)   #result:   =  128x128x16\n",
    "                #Concat dc9 with conv1_2 so we get  channels (128x128x32)\n",
    "                c9 = concat(dc9, conv1_2)                   \n",
    "               \n",
    "                ########### 9th Big Layer ###########          128x128x32   \n",
    "                conv9_1, conv9_2, normcache9_1, normcache9_2 = Conv_Block(\"Forward\", f9, b9, c9, dropout, GN, ga[8], be[8])\n",
    "                #####################################    128x128x16\n",
    "                \n",
    "                ############################# Last Layer conv(1x1) --> 128x128x1 ##########################\n",
    "                params = [out_f, out_b]\n",
    "                output = conv(conv9_2, params, 1, 0) #output.shape: 128x128x1\n",
    "                \n",
    "                #print(output[:,0:10,0:10])\n",
    "                if(GN == 0):\n",
    "                    output = normalize(output)\n",
    "                ## Sigmoid ##\n",
    "                Y_hat = sigmoid(output)\n",
    "                \n",
    "                #label crop is needed\n",
    "                #Y_t_b = crop(Y_t[b], Y_hat.shape[1])\n",
    "                #plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "                cost_,accuracy_ = Dice_Coef(Y_hat, Y_t[b])\n",
    "                cost += cost_\n",
    "                accuracy += accuracy_\n",
    "                #print(accuracy_*100)\n",
    "                if((c+1) == X.shape[0]): #assuming that batch is always  1\n",
    "                    if (accuracy/(c+1)>last_acc):\n",
    "                        #plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "                        last_acc = accuracy/(c+1)\n",
    "                        print(\"New parameters Saved!\")\n",
    "                        GN_params = [ga, be, gamma_out, beta_out]\n",
    "                        parameters = [filters, bias, f_dc, out_fb, GN_params]\n",
    "                        path = os.path.join(os.path.expanduser('~/'), 'data', 'salt')\n",
    "                        with open(path+'/weights', 'wb') as fp:\n",
    "                            pickle.dump(parameters, fp)\n",
    "                    if ((accuracy/(c+1))>0.89):\n",
    "                        print(\"Latest Accuracy: {}%\".format(accuracy*100))\n",
    "                        params_values = [filters, bias, f_dc, out_fb]\n",
    "                        return params_values\n",
    "                \n",
    "                \n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ################################# Backward Propagation ##################################\n",
    "                \n",
    "                \n",
    "                #Pixel-wise sub, we we can get the diff(Y includes the 2 classes 0 and 1)\n",
    "                \n",
    "                dA_prev = Y_hat - Y_t[b]\n",
    "                dZ_prev = sigmoid_backward(dA_prev, output)\n",
    "                conv_s =1 \n",
    "                dconv9_2, dout_f_, dout_b_ = convolutionBackward(dZ_prev, conv9_2, out_f, conv_s) #       \n",
    "                \n",
    "                if(GN == 0):\n",
    "                    conc_dconv9, df9_2, db9_2, df9_1, db9_1 = Conv_Block(\"Backward\", f9, dconv9_2, conv9_2, conv9_1, 0, c9, 0)\n",
    "                else:\n",
    "                    conc_dconv9, df9_2, db9_2, df9_1, db9_1, dga9, dbe9 = Conv_Block(\"Backward\", f9, dconv9_2, conv9_2, conv9_1, c9, normcache9_1, normcache9_2)\n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv9, dconv1_2 = crop2half(conc_dconv9)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #conv8_2 is not needed for input,we know how to select the right gradients            \n",
    "                #dconv1_2 = reshape(dconv1_2, conv1_2.shape[1])\n",
    "                \n",
    "                dconv8_2, df9_dc, db9_dc = convTranspBackward(dconv9, new_in9, fb9_dc[0],conv_s)\n",
    "                #pack data\n",
    "\n",
    "                if(GN == 0):\n",
    "                    conc_dconv8, df8_2, db8_2, df8_1, db8_1 = Conv_Block(\"Backward\", f8, dconv8_2, conv8_2, conv8_1, 0, c8, 0)\n",
    "                else:\n",
    "                    conc_dconv8, df8_2, db8_2, df8_1, db8_1, dga8, dbe8 = Conv_Block(\"Backward\", f8, dconv8_2, conv8_2, conv8_1, c8, normcache8_1, normcache8_2)\n",
    "                    \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv8, dconv2_2 = crop2half(conc_dconv8)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #dconv2_2 = reshape(dconv2_2, conv2_2.shape[1])\n",
    "                \n",
    "                dconv7_2, df8_dc, db8_dc = convTranspBackward(dconv8, new_in8, fb8_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    conc_dconv7, df7_2, db7_2, df7_1, db7_1 = Conv_Block(\"Backward\", f7, dconv7_2, conv7_2, conv7_1, 0, c7, 0)\n",
    "                else:\n",
    "                    conc_dconv7, df7_2, db7_2, df7_1, db7_1, dga7, dbe7 = Conv_Block(\"Backward\", f7, dconv7_2, conv7_2, conv7_1, c7, normcache7_1, normcache7_2)\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv7, dconv3_2 = crop2half(conc_dconv7)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #Make sure that dconv3_2 is the same dim with the dconv3_2 that will come from maxpool in decoding side\n",
    "                #dconv3_2 = reshape(dconv3_2, conv3_2.shape[1])\n",
    "                \n",
    "                dconv6_2, df7_dc, db7_dc = convTranspBackward(dconv7, new_in7, fb7_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    conc_dconv6, df6_2, db6_2, df6_1, db6_1 = Conv_Block(\"Backward\", f6, dconv6_2, conv6_2, conv6_1, 0, c6, 0)\n",
    "                else:     \n",
    "                    conc_dconv6, df6_2, db6_2, df6_1, db6_1, dga6, dbe6 = Conv_Block(\"Backward\", f6, dconv6_2, conv6_2, conv6_1, c6, normcache6_1, normcache6_2)\n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv6, dconv4_2 = crop2half(conc_dconv6)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #dconv4_2 = reshape(dconv4_2, conv4_2.shape[1])\n",
    "                \n",
    "                dconv5_2, df6_dc, db6_dc = convTranspBackward(dconv6, new_in6, fb6_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    dpl4, df5_2, db5_2, df5_1, db5_1 = Conv_Block(\"Backward\", f5, dconv5_2, conv5_2, conv5_1, 0, pl4, 0)\n",
    "                else:     \n",
    "                    dpl4, df5_2, db5_2, df5_1, db5_1, dga5, dbe5 = Conv_Block(\"Backward\", f5, dconv5_2, conv5_2, conv5_1, pl4, normcache5_1, normcache5_2)\n",
    "\n",
    "                \n",
    "                dconv4_2 += maxpoolBackward(dpl4, conv4_2, f=2 , s=2) #Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    dpl3, df4_2, db4_2, df4_1, db4_1 = Conv_Block(\"Backward\", f4, dconv4_2, conv4_2, conv4_1, 0, pl3, 0)\n",
    "                else:     \n",
    "                    dpl3, df4_2, db4_2, df4_1, db4_1, dga4, dbe4 = Conv_Block(\"Backward\", f4, dconv4_2, conv4_2, conv4_1, pl3, normcache4_1, normcache4_2)\n",
    "\n",
    "\n",
    "                dconv3_2 += maxpoolBackward(dpl3, conv3_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    dpl2, df3_2, db3_2, df3_1, db3_1 = Conv_Block(\"Backward\", f3, dconv3_2, conv3_2, conv3_1, 0, pl2, 0)\n",
    "                else:     \n",
    "                    dpl2, df3_2, db3_2, df3_1, db3_1, dga3, dbe3 = Conv_Block(\"Backward\", f3, dconv3_2, conv3_2, conv3_1, pl2, normcache3_1, normcache3_2)\n",
    "\n",
    "                \n",
    "                dconv2_2 += maxpoolBackward(dpl2, conv2_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    dpl1, df2_2, db2_2, df2_1, db2_1 = Conv_Block(\"Backward\", f2, dconv2_2, conv2_2, conv2_1, 0, pl1, 0)\n",
    "                else:     \n",
    "                    dpl1, df2_2, db2_2, df2_1, db2_1, dga2, dbe2 = Conv_Block(\"Backward\", f2, dconv2_2, conv2_2, conv2_1, pl1, normcache2_1, normcache2_2)\n",
    "\n",
    "                \n",
    "                dconv1_2 += maxpoolBackward(dpl1, conv1_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    _, df1_2, db1_2, df1_1, db1_1 = Conv_Block(\"Backward\", f1, dconv1_2, conv1_2, conv1_1, 0, X_t[b], 0)\n",
    "                else:     \n",
    "                    _, df1_2, db1_2, df1_1, db1_1, dga1, dbe1 = Conv_Block(\"Backward\", f1, dconv1_2, conv1_2, conv1_1, X_t[b], normcache1_1, normcache1_2)\n",
    "\n",
    "                \n",
    "                \n",
    "                if(GN == 1):\n",
    "                    dgamma = [dga1,dga2,dga3,dga4,dga5,dga6,dga7,dga8,dga9]\n",
    "                    dbeta = [dbe1,dbe2,dbe3,dbe4,dbe5,dbe6,dbe7,dbe8,dbe9]\n",
    "                \n",
    "\n",
    "                [df1,df2,df3,df4,df5,df6,df7,df8,df9] = df\n",
    "                [db1,db2,db3,db4,db5,db6,db7,db8,db9] = db \n",
    "                [dfb6_dc,dfb7_dc,dfb8_dc,dfb9_dc]     = dfb\n",
    "\n",
    "                \n",
    "                df1[0] += df1_1\n",
    "                df1[1] += df1_2\n",
    "                df2[0] += df2_1\n",
    "                df2[1] += df2_2\n",
    "                df3[0] += df3_1\n",
    "                df3[1] += df3_2\n",
    "                df4[0] += df4_1\n",
    "                df4[1] += df4_2\n",
    "                df5[0] += df5_1\n",
    "                df5[1] += df5_2\n",
    "                df6[0] += df6_1\n",
    "                df6[1] += df6_2\n",
    "                df7[0] += df7_1\n",
    "                df7[1] += df7_2\n",
    "                df8[0] += df8_1\n",
    "                df8[1] += df8_2\n",
    "                df9[0] += df9_1\n",
    "                df9[1] += df9_2\n",
    "                \n",
    "                db1[0] += db1_1\n",
    "                db1[1] += db1_2\n",
    "                db2[0] += db2_1\n",
    "                db2[1] += db2_2\n",
    "                db3[0] += db3_1\n",
    "                db3[1] += db3_2\n",
    "                db4[0] += db4_1\n",
    "                db4[1] += db4_2\n",
    "                db5[0] += db5_1\n",
    "                db5[1] += db5_2\n",
    "                db6[0] += db6_1\n",
    "                db6[1] += db6_2\n",
    "                db7[0] += db7_1\n",
    "                db7[1] += db7_2\n",
    "                db8[0] += db8_1\n",
    "                db8[1] += db8_2\n",
    "                db9[0] += db9_1\n",
    "                db9[1] += db9_2\n",
    "\n",
    "                dfb6_dc[0] += df6_dc\n",
    "                dfb6_dc[1] += db6_dc\n",
    "                dfb7_dc[0] += df7_dc\n",
    "                dfb7_dc[1] += db7_dc\n",
    "                dfb8_dc[0] += df8_dc\n",
    "                dfb8_dc[1] += db8_dc\n",
    "                dfb9_dc[0] += df9_dc\n",
    "                dfb9_dc[1] += db9_dc\n",
    "\n",
    "                dout_f += dout_f_\n",
    "                dout_b += dout_b_\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                ############## Adam Optimization ################\n",
    "                #changing the main structures(which are also updated)\n",
    "                #TODO: Future update - remove uneccessary memory loads/stores, v,s dont need to be saved \n",
    "                for i in range(len(filters)):\n",
    "                    v_adam[i][0] = beta1*v_adam[i][0] + (1-beta1)*df[i][0]/batch_size #f1\n",
    "                    s_adam[i][0] = beta2*s_adam[i][0] + (1-beta2)*(df[i][0]/batch_size)**2 #f1\n",
    "                    filters[i][0] -= lr*v_adam[i][0]/np.sqrt(s_adam[i][0] + 1e-7)\n",
    "\n",
    "                    v_adam[i][1] = beta1*v_adam[i][1] + (1-beta1)*df[i][1]/batch_size #f2\n",
    "                    s_adam[i][1] = beta2*s_adam[i][1] + (1-beta2)*(df[i][1]/batch_size)**2  #f2\n",
    "                    filters[i][1] -= lr*v_adam[i][1]/np.sqrt(s_adam[i][1] + 1e-7)\n",
    "\n",
    "                for i in range(len(bias)):\n",
    "                    bv_adam[i][0] = beta1*bv_adam[i][0] + (1-beta1)*db[i][0]/batch_size #b1\n",
    "                    bs_adam[i][0] = beta2*bs_adam[i][0] + (1-beta2)*(db[i][0]/batch_size)**2  #b1\n",
    "                    bias[i][0] -= lr*bv_adam[i][0]/np.sqrt(bs_adam[i][0] + 1e-7)\n",
    "\n",
    "                    bv_adam[i][1] = beta1*bv_adam[i][1] + (1-beta1)*db[i][1]/batch_size #b2\n",
    "                    bs_adam[i][1] = beta2*bs_adam[i][1] + (1-beta2)*(db[i][1]/batch_size)**2  #b2\n",
    "                    bias[i][1] -= lr*bv_adam[i][1]/np.sqrt(bs_adam[i][1] + 1e-7)\n",
    "\n",
    "                for i in range(len(f_dc)):\n",
    "                    fdc_v_adam[i][0] = beta1*fdc_v_adam[i][0] + (1-beta1)*dfb[i][0]/batch_size #f1\n",
    "                    fdc_s_adam[i][0] = beta2*fdc_s_adam[i][0] + (1-beta2)*(dfb[i][0]/batch_size)**2  #f1\n",
    "                    f_dc[i][0] -= lr*fdc_v_adam[i][0]/np.sqrt(fdc_s_adam[i][0] + 1e-7)\n",
    "\n",
    "                    fdc_v_adam[i][1] = beta1*fdc_v_adam[i][1] + (1-beta1)*dfb[i][1]/batch_size #b2\n",
    "                    fdc_s_adam[i][1] = beta2*fdc_s_adam[i][1] + (1-beta2)*(dfb[i][1]/batch_size)**2  #b2\n",
    "                    f_dc[i][1] -= lr*fdc_v_adam[i][1]/np.sqrt(fdc_s_adam[i][1] + 1e-7)    \n",
    "\n",
    "                v_out_f = beta1*v_out_f + (1 - beta1)*dout_f/batch_size #f\n",
    "                s_out_f = beta2*s_out_f + (1 - beta2)*(dout_f/batch_size)**2  #f\n",
    "                out_fb[0] -= lr*v_out_f/np.sqrt(s_out_f + 1e-7)\n",
    "\n",
    "                bv_out_b = beta1*bv_out_b + (1 - beta1)*dout_b/batch_size #b\n",
    "                bs_out_b = beta2*bs_out_b + (1 - beta2)*(dout_b/batch_size)**2  #b\n",
    "                out_fb[1] -= lr*bv_out_b/np.sqrt(bs_out_b + 1e-7)\n",
    "\n",
    "                if(GN == 1):\n",
    "                    mytrim = 20\n",
    "                    for i in range(len(ga)):\n",
    "                        ga[i][0] -= lr*mytrim*dgamma[i][0]\n",
    "                        ga[i][1] -= lr*mytrim*dgamma[i][1]\n",
    "\n",
    "                        be[i][0] -= lr*mytrim*dbeta[i][0]\n",
    "                        be[i][1] -= lr*mytrim*dbeta[i][1]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            for i in range(len(filters)):\n",
    "                filters[i][0] -= lr*df[i][0]\n",
    "                bias[i][0] -= lr*db[i][0]\n",
    "            \n",
    "            for i in range(len(f_dc)):\n",
    "                f_dc[i][0] -= lr*dfb[i][0]\n",
    "            \n",
    "            out_fb[0] -= lr*dout_f\n",
    "            out_fb[1] -= lr*dout_b\n",
    "            \n",
    "            #Updating Conv Part\n",
    "            \n",
    "\n",
    "            #print(Yh)\n",
    "            #print(t)\n",
    "            #print(\"Batch:{}\".format(c+12))\n",
    "            \n",
    "           \n",
    "            f1 -= (learning_rate) * (df1/batch)\n",
    "            f2 -= (learning_rate) * (df2/batch)\n",
    "            b1 -= (learning_rate) * (db1/batch)\n",
    "            b2 -= (learning_rate) * (db2/batch)\n",
    "            filter_params = [f1, f2, b1, b2]\n",
    "                \n",
    "            # updating FC Part\n",
    "            #params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "            grads_values['dW1'] = dW1#/(batch)\n",
    "            grads_values['dW2'] = dW2#/(batch)\n",
    "            grads_values['db1'] = dB1#/(batch)\n",
    "            grads_values['db2'] = dB2#/(batch)\n",
    "                   \n",
    "            for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "                \n",
    "                params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "                params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "            \n",
    "            '''\n",
    "            \n",
    "        #END OF LOOP - EPOCH\n",
    "        #timestamp2 = time.time()\n",
    "        #print (\"This took %.2f seconds\" %(timestamp2 - timestamp1))\n",
    "        if(batch == 1):\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/(c+1), (accuracy*100)/(c+1)))\n",
    "        else:\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/batch, (accuracy*100)/batch))\n",
    "        #if(callback is not None):\n",
    "        #    callback(i, params_values)\n",
    "    #pack filters\n",
    "    #params_values = [filters, bias, f_dc, out_fb]\n",
    "    fp.close()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************\n",
      "Group Normalization Enabled!\n",
      "Dropout Disabled!\n",
      "Learning rate: 0.008\n",
      "Dataset Size: 1\n",
      "Weight scale: 0.0001\n",
      "************************************\n",
      "Epoch: {1}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:     1   -   cost: 0.75   -   Accuracy: 47.06%\n",
      "Epoch: {2}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     2   -   cost: 0.75   -   Accuracy: 47.04%\n",
      "Epoch: {3}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     3   -   cost: 0.76   -   Accuracy: 46.99%\n",
      "Epoch: {4}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     4   -   cost: 0.76   -   Accuracy: 46.86%\n",
      "Epoch: {5}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     5   -   cost: 0.76   -   Accuracy: 46.64%\n",
      "Epoch: {6}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     6   -   cost: 0.77   -   Accuracy: 46.13%\n",
      "Epoch: {7}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     7   -   cost: 0.79   -   Accuracy: 45.57%\n",
      "Epoch: {8}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     8   -   cost: 0.80   -   Accuracy: 45.10%\n",
      "Epoch: {9}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     9   -   cost: 0.79   -   Accuracy: 45.21%\n",
      "Epoch: {10}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    10   -   cost: 0.79   -   Accuracy: 45.59%\n",
      "Epoch: {11}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    11   -   cost: 0.79   -   Accuracy: 45.48%\n",
      "Epoch: {12}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    12   -   cost: 0.76   -   Accuracy: 46.68%\n",
      "Epoch: {13}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    13   -   cost: 0.73   -   Accuracy: 48.08%\n",
      "Epoch: {14}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    14   -   cost: 0.76   -   Accuracy: 46.93%\n",
      "Epoch: {15}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    15   -   cost: 0.76   -   Accuracy: 46.80%\n",
      "Epoch: {16}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    16   -   cost: 0.80   -   Accuracy: 44.88%\n",
      "Epoch: {17}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    17   -   cost: 0.79   -   Accuracy: 45.30%\n",
      "Epoch: {18}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    18   -   cost: 0.80   -   Accuracy: 44.96%\n",
      "Epoch: {19}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    19   -   cost: 0.78   -   Accuracy: 45.70%\n",
      "Epoch: {20}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    20   -   cost: 0.76   -   Accuracy: 46.86%\n",
      "Epoch: {21}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    21   -   cost: 0.75   -   Accuracy: 47.41%\n",
      "Epoch: {22}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    22   -   cost: 0.75   -   Accuracy: 47.42%\n",
      "Epoch: {23}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    23   -   cost: 0.74   -   Accuracy: 47.53%\n",
      "Epoch: {24}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    24   -   cost: 0.73   -   Accuracy: 48.07%\n",
      "Epoch: {25}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    25   -   cost: 0.73   -   Accuracy: 48.25%\n",
      "Epoch: {26}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    26   -   cost: 0.77   -   Accuracy: 46.43%\n",
      "Epoch: {27}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    27   -   cost: 0.75   -   Accuracy: 47.42%\n",
      "Epoch: {28}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    28   -   cost: 0.73   -   Accuracy: 48.35%\n",
      "Epoch: {29}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    29   -   cost: 0.71   -   Accuracy: 49.33%\n",
      "Epoch: {30}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    30   -   cost: 0.75   -   Accuracy: 47.41%\n",
      "Epoch: {31}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    31   -   cost: 0.71   -   Accuracy: 49.32%\n",
      "Epoch: {32}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    32   -   cost: 0.58   -   Accuracy: 55.85%\n",
      "Epoch: {33}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    33   -   cost: 0.56   -   Accuracy: 57.33%\n",
      "Epoch: {34}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    34   -   cost: 0.52   -   Accuracy: 59.19%\n",
      "Epoch: {35}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    35   -   cost: 0.50   -   Accuracy: 60.84%\n",
      "Epoch: {36}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    36   -   cost: 0.49   -   Accuracy: 61.30%\n",
      "Epoch: {37}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    37   -   cost: 0.46   -   Accuracy: 63.16%\n",
      "Epoch: {38}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    38   -   cost: 0.44   -   Accuracy: 64.23%\n",
      "Epoch: {39}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    39   -   cost: 0.42   -   Accuracy: 65.70%\n",
      "Epoch: {40}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    40   -   cost: 0.40   -   Accuracy: 67.15%\n",
      "Epoch: {41}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    41   -   cost: 0.38   -   Accuracy: 68.06%\n",
      "Epoch: {42}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    42   -   cost: 0.36   -   Accuracy: 70.07%\n",
      "Epoch: {43}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    43   -   cost: 0.35   -   Accuracy: 70.14%\n",
      "Epoch: {44}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    44   -   cost: 0.38   -   Accuracy: 68.10%\n",
      "Epoch: {45}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    45   -   cost: 0.37   -   Accuracy: 69.31%\n",
      "Epoch: {46}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    46   -   cost: 0.32   -   Accuracy: 72.46%\n",
      "Epoch: {47}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    47   -   cost: 0.47   -   Accuracy: 62.19%\n",
      "Epoch: {48}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    48   -   cost: 0.34   -   Accuracy: 71.21%\n",
      "Epoch: {49}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    49   -   cost: 0.36   -   Accuracy: 69.85%\n",
      "Epoch: {50}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    50   -   cost: 0.38   -   Accuracy: 68.61%\n",
      "Epoch: {51}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    51   -   cost: 0.37   -   Accuracy: 69.22%\n",
      "Epoch: {52}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    52   -   cost: 0.44   -   Accuracy: 64.10%\n",
      "Epoch: {53}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    53   -   cost: 0.48   -   Accuracy: 62.16%\n",
      "Epoch: {54}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    54   -   cost: 0.38   -   Accuracy: 68.26%\n",
      "Epoch: {55}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    55   -   cost: 0.43   -   Accuracy: 65.26%\n",
      "Epoch: {56}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    56   -   cost: 0.40   -   Accuracy: 67.26%\n",
      "Epoch: {57}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    57   -   cost: 0.41   -   Accuracy: 66.25%\n",
      "Epoch: {58}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    58   -   cost: 0.38   -   Accuracy: 68.57%\n",
      "Epoch: {59}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    59   -   cost: 0.36   -   Accuracy: 69.47%\n",
      "Epoch: {60}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    60   -   cost: 0.35   -   Accuracy: 70.23%\n",
      "Epoch: {61}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    61   -   cost: 0.35   -   Accuracy: 70.68%\n",
      "Epoch: {62}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    62   -   cost: 0.36   -   Accuracy: 69.97%\n",
      "Epoch: {63}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    63   -   cost: 0.35   -   Accuracy: 70.78%\n",
      "Epoch: {64}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    64   -   cost: 0.36   -   Accuracy: 69.73%\n",
      "Epoch: {65}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    65   -   cost: 0.33   -   Accuracy: 71.53%\n",
      "Epoch: {66}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    66   -   cost: 0.37   -   Accuracy: 68.97%\n",
      "Epoch: {67}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    67   -   cost: 0.30   -   Accuracy: 73.80%\n",
      "Epoch: {68}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    68   -   cost: 0.49   -   Accuracy: 61.42%\n",
      "Epoch: {69}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    69   -   cost: 0.31   -   Accuracy: 73.40%\n",
      "Epoch: {70}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    70   -   cost: 0.35   -   Accuracy: 70.55%\n",
      "Epoch: {71}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    71   -   cost: 0.32   -   Accuracy: 72.74%\n",
      "Epoch: {72}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    72   -   cost: 0.34   -   Accuracy: 71.49%\n",
      "Epoch: {73}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    73   -   cost: 0.32   -   Accuracy: 72.85%\n",
      "Epoch: {74}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    74   -   cost: 0.32   -   Accuracy: 72.30%\n",
      "Epoch: {75}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    75   -   cost: 0.31   -   Accuracy: 73.11%\n",
      "Epoch: {76}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    76   -   cost: 0.32   -   Accuracy: 72.62%\n",
      "Epoch: {77}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    77   -   cost: 0.31   -   Accuracy: 73.68%\n",
      "Epoch: {78}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    78   -   cost: 0.27   -   Accuracy: 75.99%\n",
      "Epoch: {79}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    79   -   cost: 0.29   -   Accuracy: 74.93%\n",
      "Epoch: {80}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    80   -   cost: 0.25   -   Accuracy: 78.26%\n",
      "Epoch: {81}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    81   -   cost: 0.29   -   Accuracy: 74.93%\n",
      "Epoch: {82}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    82   -   cost: 0.24   -   Accuracy: 78.99%\n",
      "Epoch: {83}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    83   -   cost: 0.36   -   Accuracy: 69.99%\n",
      "Epoch: {84}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    84   -   cost: 0.32   -   Accuracy: 72.26%\n",
      "Epoch: {85}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    85   -   cost: 0.53   -   Accuracy: 58.64%\n",
      "Epoch: {86}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    86   -   cost: 0.24   -   Accuracy: 78.36%\n",
      "Epoch: {87}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    87   -   cost: 0.32   -   Accuracy: 72.73%\n",
      "Epoch: {88}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    88   -   cost: 0.27   -   Accuracy: 76.57%\n",
      "Epoch: {89}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    89   -   cost: 0.31   -   Accuracy: 73.69%\n",
      "Epoch: {90}\n",
      "Batch: 1\n",
      "Image: 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    90   -   cost: 0.32   -   Accuracy: 72.59%\n"
     ]
    }
   ],
   "source": [
    "###### Training ######\n",
    "#train_images, train_labels, test_images, test_labels\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "params_values = train(train_images, train_labels, 90, 0.008, 0, 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Forward step . . .\n",
      "Cost: 0.22   -   Accuracy: 80.53%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = os.path.join(os.path.expanduser('~/'), 'data', 'salt')\n",
    "with open (path+'/weights', 'rb') as fp:\n",
    "    params = pickle.load(fp)  \n",
    "fp.close()\n",
    "Validate(train_images[0:1,:,:,:], train_labels[0:1,:,:,:], params, 1) #GN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Forward step . . .\n",
      "Cost: 0.13   -   Accuracy: 88.16%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Validate(test_images[1:2,:,:,:], test_labels[1:2,:,:,:], params_values);\n",
    "Validate(test_images[0:1,:,:,:], test_labels[0:1,:,:,:], params, 1) #GN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
