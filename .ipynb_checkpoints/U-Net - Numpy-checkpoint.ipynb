{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "#import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "from matplotlib import image\n",
    "\n",
    "def mnist(path=None):\n",
    "    r\"\"\"Return (train_images, train_labels, test_images, test_labels).\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory containing MNIST. Default is\n",
    "            /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist.\n",
    "            Create if nonexistant. Download any missing files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, train_labels, test_images, test_labels), each\n",
    "            a matrix. Rows are examples. Columns of images are pixel values.\n",
    "            Columns of labels are a onehot encoding of the correct class.\n",
    "\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz',\n",
    "             'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz',\n",
    "             't10k-labels-idx1-ubyte.gz']\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        # Set path to /home/USER/data/mnist or C:\\Users\\USER\\data\\salt\n",
    "        path = os.path.join(os.path.expanduser('~/'), 'data', 'salt2')\n",
    "\n",
    "    # Create path if it doesn't exist\n",
    "    #os.makedirs(path, exist_ok=True)\n",
    "    \"\"\"\"\n",
    "    # Download any missing files\n",
    "    for file in files:\n",
    "        if file not in os.listdir(path):\n",
    "            urlretrieve(url + file, os.path.join(path, file))\n",
    "            print(\"Downloaded %s to %s\" % (file, path))\n",
    "    print(\"All files are ready to gzip!\")\n",
    "    \"\"\"\n",
    "    dim=64\n",
    "    dataset=4\n",
    "    def _images(path,dim):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        folder = path + \"/images/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(dim, dim)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,dim,dim).astype('float32')#/255\n",
    "        return pixels[:dataset,:,:,:]\n",
    "\n",
    "    def _labels(path,dim):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        \n",
    "        folder = path + \"/labels/\"\n",
    "        #onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(dim, dim)) for f in os.listdir(folder)]\n",
    "        onlyfiles = [cv2.resize(image.imread(folder+f),(dim, dim)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,dim,dim).astype('float32') #/255\n",
    "        return pixels[:dataset,:,:,:]\n",
    "    \n",
    "    def _t_images(path,dim):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        folder = path + \"/t_images/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(dim, dim)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,dim,dim).astype('float32')#/255\n",
    "        return pixels[0:2,:,:,:]\n",
    "    def _t_labels(path,dim):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        \n",
    "        folder = path + \"/t_labels/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(dim, dim)) for f in os.listdir(folder)]\n",
    "        #onlyfiles = [cv2.resize(image.imread(folder+f),(32, 32)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,dim,dim).astype('float32') #/255\n",
    "        return pixels[0:2,:,:,:]\n",
    "\n",
    "    print(\"Train Images : Loading . . .\")\n",
    "    train_images = _images(path,dim)\n",
    "    print(\"Train Labels : Loading . . .\")\n",
    "    train_labels = _labels(path,dim)\n",
    "    print(\"Test Images  : Loading . . .\")\n",
    "    #test_images = _t_images(path,dim)\n",
    "    print(\"Test Labels  : Loading . . .\")\n",
    "    #test_labels = _t_labels(path,dim)\n",
    "    print(\"Done!\")\n",
    "    return train_images, train_labels# , test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images : Loading . . .\n",
      "Train Labels : Loading . . .\n",
      "Test Images  : Loading . . .\n",
      "Test Labels  : Loading . . .\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels= mnist()  #, test_images, test_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 64, 64)\n",
      "(4, 1, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "ch = 1 #Number of channels\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape) # ONE-HOT !!!\n",
    "#print(train_images.T.shape)\n",
    "#print(train_labels.reshape((train_images.shape[0], 10)).shape)\n",
    "\n",
    "##### Getting Ready for the Conv Layers #####\n",
    "#train_images = train_images.reshape(train_images.shape[0], ch, 28, 28).squeeze()\n",
    "#test_images = test_images.reshape(test_images.shape[0], ch, 28, 28).squeeze()\n",
    "\n",
    "#############################################\n",
    "\n",
    "#y = train_images[1].reshape(1,28,28)\n",
    "#print(y.squeeze().shape)\n",
    "'''\n",
    "temp = []\n",
    "#train_labels = train_labels.sum(1)\n",
    "for i in range(int(len(train_labels[:]))):\n",
    "    temp.append(list(train_labels[i][:]).index(1))\n",
    "    \n",
    "    \n",
    "train_labels = np.array(temp.copy())\n",
    "#print(train_labels[0:5])\n",
    "'''\n",
    "#plt.imshow(train_images[0].squeeze(), cmap='Greys_r');\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_labels[0].squeeze(), cmap='Greys_r');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-ef14d097a34c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Greys_r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_images' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(test_images[0].squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Filter/Parameter Initializattions  ##################\n",
    "\n",
    "def init_filters(layers, init_n_f ,trim):\n",
    "    filters = []\n",
    "    bias = []\n",
    "    f_dc = []\n",
    "    \n",
    "    trimf = trim\n",
    "    trimb = trim*5\n",
    "    n_f = init_n_f #initial number of filters/kernels\n",
    "    ch_in = 1      #input channels\n",
    "    \"\"\"\n",
    "    for i in range(layers):\n",
    "        if(i != 0):\n",
    "            n_f = n_f*2 #16,32,64,128,256\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trimb\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "    \n",
    "    #Deconvolution filters    \n",
    "    for i in range(1,layers):\n",
    "        n_f = n_f//2 #128,64,32,16\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        fdc = np.random.randn(n_f,ch_in,2,2)*trimf #upsampling filter, its result will be conc with conv4 output so the channels will be doubled again\n",
    "        bdc = np.random.randn(fdc.shape[0],1)* trimb\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trimb\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        dc = [fdc, bdc]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "        f_dc.append(dc)\n",
    "        \n",
    "    \"\"\"\n",
    "    trimbr = trim\n",
    "    locbr = 0\n",
    "    scb=1\n",
    "    for i in range(layers):\n",
    "        if(i != 0):\n",
    "            n_f = n_f*2 #16,32,64,128,256\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.normal(loc = locbr, scale = trimbr , size = f1) #np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.normal(loc = locbr, scale = trimbr*scb , size = (f1.shape[0],1)) #np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.normal(loc = locbr, scale = trimbr , size = f2)\n",
    "        b2 = np.random.normal(loc = locbr, scale = trimbr*scb, size = (f2.shape[0],1))\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "    \n",
    "    #Deconvolution filters    \n",
    "    for i in range(1,layers):\n",
    "        n_f = n_f//2 #128,64,32,16\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        fdc =  np.random.normal(loc = locbr, scale = trimbr , size = (n_f,ch_in,2,2))#upsampling filter, its result will be conc with conv4 output so the channels will be doubled again\n",
    "        bdc = np.random.normal(loc = locbr, scale = trimbr*scb , size = (fdc.shape[0],1))\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.normal(loc = locbr, scale = trimbr , size = (n_f, ch_in, 3, 3))\n",
    "        b1 = np.random.normal(loc = locbr, scale = trimbr*scb , size = (f1.shape[0],1))\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.normal(loc = locbr, scale = trimbr , size = f2)\n",
    "        b2 = np.random.normal(loc = locbr, scale = trimbr*scb , size = (f2.shape[0],1))\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        dc = [fdc, bdc]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "        f_dc.append(dc)\n",
    "    return filters, bias, f_dc   \n",
    "\n",
    "\n",
    "def init_groupnorm_params(bias, out_b, norm_batch, locbr, trimbr):\n",
    "    [b1,b2,b3,b4,b5,b6,b7,b8,b9]= bias \n",
    "    \n",
    "    \n",
    "    t_1,_ = b1\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma1_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) #MAKE IT FLOAT\n",
    "    beta1_1  = np.random.normal( scale = trimbr , size = gb_size)\n",
    "    gamma1_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta1_2  = np.random.normal( scale = trimbr , size = gb_size)\n",
    "    tempg_1 = [gamma1_1,gamma1_2]\n",
    "    tempb_1 = [beta1_1,beta1_2]\n",
    "    \n",
    "    t_1,_ = b2\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma2_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta2_1  = np.random.normal( scale = trimbr , size = gb_size)\n",
    "    gamma2_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta2_2  = np.random.normal(scale = trimbr , size = gb_size)  \n",
    "    tempg_2 = [gamma2_1,gamma2_2]\n",
    "    tempb_2 = [beta2_1,beta2_2]\n",
    "    \n",
    "    t_1,_ = b3\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma3_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta3_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma3_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta3_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_3 = [gamma3_1,gamma3_2]\n",
    "    tempb_3 = [beta3_1,beta3_2]\n",
    "    \n",
    "    t_1,_ = b4\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma4_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta4_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma4_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta4_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_4 = [gamma4_1,gamma4_2]\n",
    "    tempb_4 = [beta4_1,beta4_2]\n",
    "    \n",
    "    t_1,_ = b5\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma5_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta5_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma5_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta5_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_5 = [gamma5_1,gamma5_2]\n",
    "    tempb_5 = [beta5_1,beta5_2]\n",
    "    \n",
    "    t_1,_ = b6\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma6_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta6_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma6_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta6_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_6 = [gamma6_1,gamma6_2]\n",
    "    tempb_6 = [beta6_1,beta6_2]\n",
    "    \n",
    "    t_1,_ = b7\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma7_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta7_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma7_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta7_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_7 = [gamma7_1,gamma7_2]\n",
    "    tempb_7 = [beta7_1,beta7_2]\n",
    "    \n",
    "    t_1,_ = b8\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma8_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta8_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma8_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta8_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_8 = [gamma8_1,gamma8_2]\n",
    "    tempb_8 = [beta8_1,beta8_2]\n",
    "    \n",
    "    t_1,_ = b9\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma9_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta9_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma9_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta9_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_9 = [gamma9_1,gamma9_2]\n",
    "    tempb_9 = [beta9_1,beta9_2]\n",
    "    \n",
    "    ga =[tempg_1,tempg_2,tempg_3,tempg_4,tempg_5,tempg_6, tempg_7,tempg_8,tempg_9]\n",
    "    be =[tempb_1,tempb_2,tempb_3,tempb_4,tempb_5,tempb_6, tempb_7,tempb_8,tempb_9]\n",
    "    \n",
    "    gamma_out = np.random.normal(loc = locbr, scale = trimbr , size = (out_b.shape[0]//norm_batch,1))\n",
    "    beta_out =   np.random.normal(scale = trimbr , size = (out_b.shape[0]//norm_batch,1))\n",
    "    \n",
    "    return ga, be , gamma_out, beta_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation Functions ###\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UTILITIES #############\n",
    "\n",
    "def conv(image, params, s = 1, pad = 1 ): # s = 1 (conv stride)\n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    [f, b] = params\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((image.shape[0],image.shape[1]+2*pad ,image.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = image\n",
    "        image = tmp    \n",
    "        \n",
    "    f_num = f.shape[0]\n",
    "    f_depth = f.shape[1] #f1 = 1 , f2 = 8\n",
    "    f_size = f.shape[2] \n",
    "        \n",
    "\n",
    "    h_range = int((image.shape[1] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    np_o = np.zeros((f_num, h_range, w_range))\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                np_o[z, _h, _w] = np.sum(np.multiply(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] , f[z, :, :, :])) + b[z]\n",
    "    \n",
    "    return np_o\n",
    "\n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s = 1, pad = 1 ):\n",
    "    #filt =np.rot90(filt, 2)  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!! A T T E N T I O N !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    f_num, n_c, f, _ = filt.shape\n",
    "    \n",
    "    _ ,h , w = dconv_prev.shape\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((conv_in.shape[0],conv_in.shape[1]+2*pad ,conv_in.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = conv_in\n",
    "        conv_in = tmp\n",
    "    \n",
    "    dconv_in = np.zeros(conv_in.shape)\n",
    "    db = np.zeros((f_num,1))\n",
    "\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                #each entry of the dconv_prev will try to affect the idxs from which was made of.\n",
    "                dfilt[z] += dconv_prev[z, _h, _w] * conv_in[:, _h*s : _h*s + f, _w*s : _w*s + f]\n",
    "                dconv_in[:, _h*s : _h*s + f, _w*s : _w*s + f] += dconv_prev[z, _h, _w] * filt[z]  \n",
    "        db[z] = np.sum(dconv_prev[z])  #, axis =1) ## AXIS?\n",
    "    \n",
    "    if(pad >=1 ):\n",
    "        dconv_in = dconv_in[:, pad:-pad, pad:-pad]  # Cropping\n",
    "        \n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "def convTransp1(image, params, s = 2, pad = 1):\n",
    "    [f, b] = params\n",
    "    n_f, n_c, f_s, _ = f.shape\n",
    "    input_s = image.shape[1]\n",
    "    target_dim = input_s*2\n",
    "    res = np.zeros((n_f, target_dim, target_dim))\n",
    "    temp =np.zeros((n_c, target_dim, target_dim))\n",
    "    for z in range(n_f):\n",
    "        for _h in range(input_s):      \n",
    "            for _w in range(input_s):\n",
    "                temp[:, _h*s:_h*s+f_s, _w*s:_w*s+f_s] += image[:, _h, _w].reshape(n_c,1,1)*f[z,:,:,:] #bias will be added at the end\n",
    "        res[z] = np.sum(temp , axis = 0) + b[z]\n",
    "    return res, image\n",
    "\n",
    "def convTranspBackward1(dconv_prev, new_in, filt, s = 2):\n",
    "    n_f, n_c, f_s, _ = filt.shape\n",
    "    _, input_s, _ = new_in.shape\n",
    "    #final_dim = (new_in.shape[1] - 2)//2 + 1 \n",
    "    dc_s=dconv_prev.shape[1]\n",
    "    temp = np.zeros((n_c,dc_s,dc_s))\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dconv_in = np.zeros(new_in.shape)\n",
    "    db = np.zeros((n_f,1))\n",
    "    for z in range(n_f):\n",
    "        for _h in range(input_s):      \n",
    "            for _w in range(input_s): \n",
    "                dfilt[z] += dconv_prev[z, _h*s:_h*s+f_s, _w*s:_w*s+f_s]*new_in[:,_h,_w].reshape(n_c,1,1)\n",
    "                temp[:, _h*s:_h*s+f_s, _w*s:_w*s+f_s] += dconv_prev[z, _h*s:_h*s+f_s, _w*s:_w*s+f_s] * filt[z]\n",
    "                for ch in range(n_c):\n",
    "                    dconv_in[ch, _h, _w] += np.sum(temp[ch, _h*s:_h*s+f_s, _w*s:_w*s+f_s])\n",
    "        db[z] = np.sum(dconv_prev[z])        \n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "    \n",
    "def convTransp(image, params, s = 1, pad = 1):\n",
    "    #s is always 1, upsample kernel = 2\n",
    "    #zero insertions between pixels s_downsampled -1 = 2-1 = 1\n",
    "    #required padding in order to double my dimensions with the given data:\n",
    "    #(i-1)*2 + k -2p = output size, where our padding is k - p -1 = 2-0-1=1(we assume p=0)\n",
    "    [f, b]=params\n",
    "    f = np.rot90(f, 2, (2,3))\n",
    "    params = [f, b]\n",
    "    input_s = image.shape[1]\n",
    "    target_dim = input_s*2 #final dim, after conv\n",
    "    required_pad = 1 #always for filter 2x2 ,stride 1, zero insertion 1 and main target to double dim\n",
    "    #make our new custom input\n",
    "    size = input_s*2 +1\n",
    "    new_in = np.zeros((image.shape[0], size, size))\n",
    "    \n",
    "    ### OR just: np.pad(image[:,:,:],2,'constant') # Important, we must loop with respect to the 1st dim\n",
    "    for i in range(1, target_dim, 2):\n",
    "        for j in range(1, target_dim, 2):\n",
    "                new_in[:, i, j] = image[:, i//2, j//2]\n",
    "    #now we do a normal convolution(pad = 0)\n",
    "    res = conv(new_in, params, 1, 0) #thats the final result with target_dim\n",
    "    return res, new_in # we will need new_in so we wont calc it again for the backprop\n",
    "    \n",
    "def convTranspBackward(dconv_prev, new_in, filt, s = 1):\n",
    "    #First, we do a backward convolution on new_in,d_conv_prev,\n",
    "    #then we will choose form the d_conv_new the entries that match the initial 'smaller' input image\n",
    "    #by selecting the odd matrix cells 1,3,5... because we had applied a standard pad=1,zero inser=1\n",
    "    #filt = np.rot90(filt, 2, (2,3))\n",
    "    dconv_in, dfilt, db = convolutionBackward(dconv_prev, new_in, filt,1,0)\n",
    "    #Now its time to choose the right entries to build the gradients of the initial input image\n",
    "    dim = dconv_in.shape[1]\n",
    "    final_dim = (new_in.shape[1] - 2)//2 + 1 #based on dimen of image before final conv that gives the result,..\n",
    "    #e.g. for new_in 7x7 that is going to convoluted with a 2x2 kernel and give a 6x6 upsampled from 3x3 init image\n",
    "    # now from this 7x7 --> apply the formula above,we get the 3x3 dimension number\n",
    "    res = np.zeros((dconv_in.shape[0], final_dim, final_dim))\n",
    "    for i in range(1, dim, 2):\n",
    "        for j in range(1, dim, 2):\n",
    "                res[:, i//2, j//2] = dconv_in[:, i, j]\n",
    "    return res, dfilt, db\n",
    "    \n",
    "    \n",
    "def maxpool(image, f=2 , s=2):\n",
    "    \n",
    "    h_range = int((image.shape[1] - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f) / s) + 1     # (W - F + 2P) / S \n",
    "    out = np.zeros((image.shape[0], h_range, w_range))\n",
    "    \n",
    "    for z in range(image.shape[0]): # Number of channels\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                out[z, _h, _w] = np.max(image[z, _h*s : _h*s + f, _w*s : _w*s + f])\n",
    "    return out\n",
    "\n",
    "def nanargmax(arr):\n",
    "\n",
    "    idx = np.nanargmax(arr)\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "def maxpoolBackward(dpool, conv, f=2 , s=2):\n",
    "    num_c, h, w = conv.shape\n",
    "    h = int((h - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w = int((w - f) / s) + 1     # (W - F + 2P) / S \n",
    "    \n",
    "    dout = np.zeros(conv.shape)\n",
    "    #print(conv.shape)\n",
    "    for z in range(num_c): # Number of channels\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                (a, b) = nanargmax(conv[z, _h*s : _h*s + f, _w*s : _w*s + f]) #Getting the indexes from the max value in this area\n",
    "                #put it on the new array\n",
    "                dout[z, _h*s + a, _w*s + b] = dpool[z, _h, _w]\n",
    "    \n",
    "    \n",
    "    return dout\n",
    "\n",
    "\n",
    "def reshape(img, reshape_dim):\n",
    "    pad = reshape_dim - img.shape[1]\n",
    "    if(pad == 0):\n",
    "        return img\n",
    "    res = np.zeros((img.shape[0], reshape_dim, reshape_dim))\n",
    "    if(pad > 1):\n",
    "        res[:, pad//2:-(pad//2), pad//2:-(pad//2)] = img\n",
    "    else:\n",
    "        res[:, 0:-(pad), 0:-(pad)] = img\n",
    "    return res\n",
    "\n",
    "\n",
    "def crop(img, crop_dim): #TODO : look at it..future upgrade to downsample..maybe\n",
    "    start = img.shape[1]//2 - crop_dim//2\n",
    "    return img[:,(start):(start +crop_dim),(start):(start +crop_dim)]\n",
    "\n",
    "    \n",
    "    \n",
    "def crop2half(img):\n",
    "    #return gradients for decoder side and gradients for encoder side\n",
    "    n_ch = img.shape[0]//2\n",
    "    return img[n_ch:n_ch*2 ,:,:], img[0:n_ch,:,:]\n",
    "    \n",
    "def concat(img1_true, img2):\n",
    "    n_ch = img1_true.shape[0]\n",
    "    dim = img1_true.shape[1]\n",
    "    if(img2.shape[1] != dim):\n",
    "        img2 = crop(img2, dim)\n",
    "        print(\"Warning: Extra crop needed and handled!(%d --> %d)\" %(dim, img2.shape[1]))\n",
    "    res = np.zeros((n_ch*2, dim, dim))\n",
    "    res[0:n_ch,:,:] = img2\n",
    "    res[n_ch:n_ch*2 ,:,:] = img1_true\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cost Calculations ######\n",
    "def Cross_Entropy(logs, targets):  # Pixel-Wise Cross entropy --> average accuracy\n",
    "    out = np.zeros(logs.shape)\n",
    "    mylen = logs.shape[0]*logs.shape[1]*logs.shape[2]\n",
    "    #print(len(targets))\n",
    "    #logs.shape (dim x dim) like target\n",
    "    for i in range(logs.shape[1]):\n",
    "        for j in range(logs.shape[2]):\n",
    "            if(targets[:,i,j] >= logs[:,i,j]):#Gray and above\n",
    "                out[:,i,j] = logs[:,i,j]/targets[:,i,j] \n",
    "            else:\n",
    "                out[:,i,j] = (1 - logs[:,i,j])/(1 - targets[:,i,j]) # e.g if logs[i,j]= 0.4 and we want(target) 0 --> 1-0.4=0.6 prob. for zero \n",
    "    #or\n",
    "    #out = targets*logs + ((-1*(targets-1)) - (-1*(targets-1))*logs)\n",
    "    res =out.sum()/mylen\n",
    "    return -np.log(res),res\n",
    "\n",
    "\n",
    "def Dice_Coef(logs, targets):\n",
    "    out = np.zeros(logs.shape)\n",
    "    mylen = logs.shape[0]*logs.shape[1]*logs.shape[2]\n",
    "    #Apply Dice coefficient\n",
    "    numerator = (logs*targets)\n",
    "    denominator = logs + targets\n",
    "    loss = 1 - (2*np.sum(numerator))/(np.sum(denominator))\n",
    "    return loss, np.exp(-loss)\n",
    "                \n",
    "    \n",
    "    \n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    \n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost) ### ATTENTION!\n",
    "\n",
    "def normalize(output):\n",
    "    output[output<-4]=-4\n",
    "    output[output>4] = 4\n",
    "    return output\n",
    "\n",
    "###### Accuracy Calculation ######\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupnorm_forward(X, gamma_, beta_, eps=1e-5):\n",
    "    \"\"\"\n",
    "    # extract the dimensions\n",
    "    C, H, W = X.shape\n",
    "    # mini-batch mean\n",
    "    mean = nd.mean(X, axis=(1,2))\n",
    "    # mini-batch variance\n",
    "    variance = nd.mean((X - mean.reshape((C, 1, 1))) ** 2, axis=(1, 2))\n",
    "    # normalize\n",
    "    X_hat = (X - mean.reshape((C, 1, 1))) * 1.0 / nd.sqrt(variance.reshape((C, 1, 1)) + eps)\n",
    "    #if is_training:\n",
    "    # while training, we normalize the data using its mean and variance\n",
    "    X_hat = (X - mean.reshape((C, 1, 1))) * 1.0 / nd.sqrt(variance.reshape((C, 1, 1)) + eps)\n",
    "    #else:\n",
    "    # while testing, we normalize the data using the pre-computed mean and variance\n",
    "    #    X_hat = (X - _BN_MOVING_MEANS[scope_name].reshape((1, C, 1, 1))) * 1.0 \\\n",
    "    #        / nd.sqrt(_BN_MOVING_VARS[scope_name].reshape((1, C, 1, 1)) + eps)\n",
    "    # scale and shift\n",
    "    out = gamma.reshape((C, 1, 1)) * X_hat + beta.reshape((C, 1, 1))\n",
    "    \"\"\"\n",
    "    C_all=X.shape[0]\n",
    "    \n",
    "    if(C_all == 1):\n",
    "        batch = 1\n",
    "    else:\n",
    "        batch =2\n",
    "    C= batch\n",
    "    \n",
    "    mu_= np.zeros(C_all//batch)\n",
    "    var_=np.zeros(C_all//batch)\n",
    "    xmu_=np.zeros((C_all,X.shape[1],X.shape[2]))\n",
    "    sqrtvar_= np.zeros(C_all//batch)\n",
    "    ivar_= np.zeros(C_all//batch)\n",
    "    xhat_= np.zeros((C_all,X.shape[1],X.shape[2]))\n",
    "    #gammax_= np.zeros((C_all,1,1))\n",
    "    out_= np.zeros((C_all,X.shape[1],X.shape[2]))\n",
    "    \n",
    "    \n",
    "    for i in range(0, C_all, batch):\n",
    "        \n",
    "        x = X[i:i+C,:,:]\n",
    "        gamma = gamma_[i//batch]  #there is a gamma,beta for each batch of channels\n",
    "        beta = beta_[i//batch]\n",
    "        ###################################################################\n",
    "        _, H, W = x.shape  #WAS N, D\n",
    "\n",
    "        #step1: calculate mean\n",
    "        mu = np.mean(x) #scalar\n",
    "        #print(mu)\n",
    "\n",
    "        #step2: subtract mean vector of every trainings example\n",
    "        xmu = (x - mu)\n",
    "        #step3: following the lower branch - calculation denominator\n",
    "        #step4: calculate variance\n",
    "        var = np.mean(xmu ** 2)\n",
    "\n",
    "        #step5: add eps for numerical stability, then sqrt\n",
    "        sqrtvar = np.sqrt(var + eps)\n",
    "\n",
    "        #step6: invert sqrtwar\n",
    "        ivar = 1./sqrtvar\n",
    "\n",
    "        #step7: execute normalization\n",
    "        xhat = xmu * ivar\n",
    "\n",
    "        #step8: Nor the two transformation steps\n",
    "        gammax = gamma * xhat\n",
    "        #gamma,beta : scalar\n",
    "        #step9\n",
    "        out = gammax + beta\n",
    "        \n",
    "        xhat_[i:i+C,:,:]   =xhat   #.copy()\n",
    "        #gamma_[i:i+2,:,:]  =gamma\n",
    "        xmu_[i:i+C,:,:]    =xmu\n",
    "        ivar_[i//batch]  =ivar\n",
    "        sqrtvar_[i//batch]=sqrtvar\n",
    "        var_[i//batch]   =var\n",
    "        out_[i:i+C,:,:]   =out\n",
    "    #store intermediate\n",
    "    cache = (xhat_,gamma_,xmu_,ivar_,sqrtvar_,var_,eps)\n",
    "    return out_, cache\n",
    "\n",
    "def groupnorm_backward(dout_, cache):\n",
    "\n",
    "    #unfold the variables stored in cache\n",
    "    xhat_,gamma_,xmu_,ivar_,sqrtvar_,var_,eps = cache\n",
    "\n",
    "    \n",
    "    C_all =dout_.shape[0]\n",
    "    if(C_all == 1):\n",
    "        C = 1\n",
    "    else:\n",
    "        C = 2\n",
    "    \n",
    "    batch = C\n",
    "    dx_    = np.zeros((C_all,dout_.shape[1],dout_.shape[2]))\n",
    "    dgamma_= np.zeros(C_all//batch)\n",
    "    dbeta_ = np.zeros(C_all//batch)\n",
    "    \n",
    "    for i in range(0, C_all, batch): \n",
    "        dout = dout_[i:i+C,:,:]\n",
    "        xhat   =xhat_[i:i+C,:,:]\n",
    "        gamma  = gamma_[i//batch]\n",
    "        xmu    =xmu_[i:i+C,:,:]\n",
    "        ivar   =ivar_[i//batch]\n",
    "        sqrtvar=sqrtvar_[i//batch]\n",
    "        var    =var_[i//batch]\n",
    "        \n",
    "        #get the dimensions of the input/output\n",
    "        _, H, W = dout.shape #N,D = dout.shape\n",
    "\n",
    "        #step9\n",
    "        dbeta = np.sum(dout)\n",
    "        dgammax = dout #not necessary, but more understandable\n",
    "\n",
    "        #step8\n",
    "        dgamma = np.sum(dgammax*xhat)\n",
    "        dxhat = dgammax * gamma\n",
    "\n",
    "        #step7\n",
    "        divar = np.sum(dxhat*xmu)\n",
    "        dxmu1 = dxhat * ivar\n",
    "\n",
    "        #step6\n",
    "        dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "\n",
    "        #step5\n",
    "        dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
    "\n",
    "        #step4\n",
    "        dsq = 1./(batch*H*W) * np.ones((C,H,W)) * dvar  #1./C\n",
    "\n",
    "        #step3\n",
    "        dxmu2 = 2 * xmu * dsq\n",
    "\n",
    "        #step2\n",
    "        dx1 = (dxmu1 + dxmu2)\n",
    "        dmu = -1 * np.sum(dxmu1+dxmu2)\n",
    "\n",
    "        #step1\n",
    "        dx2 =  1./(batch*H*W) *np.ones((C,H,W)) * dmu #1. /C *\n",
    "\n",
    "        #step0\n",
    "        dx = dx1 + dx2\n",
    "        dx_[i:i+C,:,:]    = dx\n",
    "        dgamma_[i//batch]= dgamma\n",
    "        dbeta_[i//batch] = dbeta\n",
    "\n",
    "    return dx_, dgamma_.reshape(C_all//batch,1), dbeta_.reshape(C_all//batch,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validate(X, Y, params, GN):\n",
    "    ### Unpacking ###\n",
    "    [filters, bias, f_dc, out_fb, GN_params] = params\n",
    "    [ga, be, gamma_out, beta_out] = GN_params\n",
    "    [f1,f2,f3,f4,f5,f6,f7,f8,f9] = filters\n",
    "    [b1,b2,b3,b4,b5,b6,b7,b8,b9]= bias \n",
    "    [fb6_dc, fb7_dc, fb8_dc, fb9_dc] = f_dc\n",
    "    [out_f, out_b] = out_fb\n",
    "    #################\n",
    "    \n",
    "    \n",
    "    dropout = 0\n",
    "    print('Calculating Forward step . . .')\n",
    "    \n",
    "    batch = 1\n",
    "    for c in range(0, X.shape[0], batch):\n",
    "        if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "            batch = X.shape[0] - c\n",
    "        X_t = X[c:(c + batch)]\n",
    "        Y_t = Y[c:(c + batch)]\n",
    "        for b in range(batch):\n",
    "            ########### 1st Big Layer ###########    (with zero padding ='same',so with stride =1 we get same dim as the input)\n",
    "            conv1_1, conv1_2, normcache1_1, normcache1_2 = Conv_Block(\"Forward\", f1, b1, X_t[b], dropout, GN, ga[0], be[0])\n",
    "            ##################################### conv1_2: 128x128x16\n",
    "\n",
    "            pl1 = maxpool(conv1_2, 2, 2) #   pl1 : (128-2)/2+1  = 64 \n",
    "\n",
    "            ########### 2nd Big Layer ########### \n",
    "            conv2_1, conv2_2, normcache2_1, normcache2_2 = Conv_Block(\"Forward\", f2, b2, pl1, dropout, GN, ga[1], be[1])          \n",
    "            #####################################  64x64x32\n",
    "\n",
    "            pl2 = maxpool(conv2_2, 2, 2) #pool_f = 2 , pool_s = 2    , (64 -2)/2 +1 = 32\n",
    "\n",
    "            ########### 3rd Big Layer ###########\n",
    "            conv3_1, conv3_2, normcache3_1, normcache3_2 = Conv_Block(\"Forward\", f3, b3, pl2, dropout, GN, ga[2], be[2])          \n",
    "            #####################################  32x32x64\n",
    "\n",
    "            pl3 = maxpool(conv3_2, 2, 2) #pool_f = 2 , pool_s = 2   ,  (32-2)/2 +1 = 16\n",
    "\n",
    "            ########### 4th Big Layer ###########\n",
    "            conv4_1, conv4_2, normcache4_1, normcache4_2 = Conv_Block(\"Forward\", f4, b4, pl3, dropout, GN, ga[3], be[3])             \n",
    "            #####################################     16x16x128\n",
    "\n",
    "            pl4 = maxpool(conv4_2, 2, 2) #pool_f = 2 , pool_s = 2  , (16-2)/2 +1 =8  : 8x8x128\n",
    "\n",
    "            ########### 5th Big Layer ###########   8x8x128-->8x8x256\n",
    "            conv5_1, conv5_2, normcache5_1, normcache5_2 = Conv_Block(\"Forward\", f5, b5, pl4, dropout, GN, ga[4], be[4])       \n",
    "            #####################################  8x8x256\n",
    "\n",
    "            #####################################\n",
    "            #Because of ambigious size after the upsampling the concat func must take care possible crop of the conv#_2 \n",
    "            #####################################\n",
    "            #Deconvolution/Upsampling\n",
    "            # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "            params = [fb6_dc[0], fb6_dc[1]] # deconv filter, deconv bias\n",
    "            dc6, new_in6 = convTransp(conv5_2, params, 1, 0)   #result:   =  16x16x128 , # conv5_2 requires NO crop\n",
    "            #Concat dc6 with conv4_2 so we get 256 channels (16x16x256)\n",
    "            c6 = concat(dc6, conv4_2) # 1st one is the right one size  \n",
    "\n",
    "            ########### 6th Big Layer ###########          16x16x256     \n",
    "            conv6_1, conv6_2, normcache6_1, normcache6_2 = Conv_Block(\"Forward\", f6, b6, c6, dropout, GN, ga[5], be[5])  \n",
    "            #####################################    16x16x128\n",
    "            #(16-1)*2 + 2 =32\n",
    "            params = [fb7_dc[0], fb7_dc[1]] # deconv filter, deconv bias\n",
    "            dc7, new_in7 = convTransp(conv6_2, params, 1, 0)   #result:   =  32x32x64\n",
    "            #Concat dc7 with conv3_2 so we get  channels (32x32x128)\n",
    "            c7 = concat(dc7, conv3_2)   \n",
    "\n",
    "            ########### 7th Big Layer ###########          32x32x128     \n",
    "            conv7_1, conv7_2, normcache7_1, normcache7_2 = Conv_Block(\"Forward\", f7, b7, c7, dropout, GN, ga[6], be[6]) \n",
    "            #####################################    32x32x64\n",
    "            #(24-1)*2 + 2 = 48\n",
    "            params = [fb8_dc[0], fb8_dc[1]] # deconv filter, deconv bias\n",
    "            dc8, new_in8 = convTransp(conv7_2, params, 1, 0)   #result:   =  64x64x32\n",
    "            #Concat dc8 with conv2_2 so we get  channels (64x64x64)\n",
    "            c8 = concat(dc8 ,conv2_2)   \n",
    "\n",
    "            ########### 8th Big Layer ###########          64x64x64    \n",
    "            conv8_1, conv8_2, normcache8_1, normcache8_2 = Conv_Block(\"Forward\", f8, b8, c8, dropout, GN, ga[7], be[7])\n",
    "            #####################################    64x64x32                              \n",
    "            #(64-1)*2 + 2 = 128\n",
    "            params = [fb9_dc[0], fb9_dc[1]] # deconv filter, deconv bias\n",
    "            dc9, new_in9 = convTransp(conv8_2, params, 1, 0)   #result:   =  128x128x16\n",
    "            #Concat dc9 with conv1_2 so we get  channels (128x128x32)\n",
    "            c9 = concat(dc9, conv1_2)                   \n",
    "\n",
    "            ########### 9th Big Layer ###########          128x128x32   \n",
    "            conv9_1, conv9_2, normcache9_1, normcache9_2 = Conv_Block(\"Forward\", f9, b9, c9, dropout, GN, ga[8], be[8])\n",
    "            #####################################    128x128x16\n",
    "\n",
    "            ############################# Last Layer conv(1x1) --> 128x128x1 ##########################\n",
    "            params = [out_f, out_b]\n",
    "            output = conv(conv9_2, params, 1, 0) #output.shape: 128x128x1\n",
    "\n",
    "            #print(output[:,0:10,0:10])\n",
    "            output = normalize(output)\n",
    "            ## Sigmoid ##\n",
    "            Y_hat = sigmoid(output)\n",
    "            \n",
    "            Y_hat[Y_hat>0.65]=1\n",
    "            Y_hat[Y_hat<0.35]=0\n",
    "\n",
    "            plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "            cost_,accuracy_ = Dice_Coef(Y_hat, Y_t[b])#Cross_Entropy(Y_hat, Y_t[b])\n",
    "            cost = cost_\n",
    "            accuracy = accuracy_\n",
    "            print(\"Cost: {:.2f}   -   Accuracy: {:.2f}%\".format(cost/batch, (accuracy*100)/batch))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv_Block(step, f, b, myin, dropout, GN, ga, be):\n",
    "    if(step == \"Forward\"):\n",
    "        bc1 = 0\n",
    "        bc2 = 0\n",
    "        ### DROPOUT ###\n",
    "        if(dropout>0):\n",
    "            d = (np.random.rand(myin.shape[0],myin.shape[1],myin.shape[2])<dropout)\n",
    "            d = d*1 #Bool --> int(0s and 1s)\n",
    "            myin = d*myin\n",
    "        ###############\n",
    "        params = [f[0], b[0]]  \n",
    "        conv1 = conv(myin, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "        ### GROUP NORM ###\n",
    "        if(GN == 1):\n",
    "            conv1, bc1 = groupnorm_forward(conv1, ga[0], be[0]) \n",
    "        ##################\n",
    "        conv1[conv1<=0] = 0 #Relu\n",
    "\n",
    "        params = [f[1], b[1]]\n",
    "        conv2 = conv(conv1, params, 1)\n",
    "        ### GROUP NORM ###\n",
    "        if(GN == 1):\n",
    "            conv2, bc2 = groupnorm_forward(conv2, ga[1], be[1]) \n",
    "        ##################\n",
    "        conv2[conv2<=0] = 0 #Relu\n",
    "        return conv1, conv2, bc1, bc2\n",
    "    else: #Backward\n",
    "        if(isinstance(GN, int)):\n",
    "            dconv_prev = b\n",
    "            conv_prev = myin\n",
    "            conv_prev1 = dropout\n",
    "            conc = ga\n",
    "            dconv_prev[conv_prev<=0] = 0\n",
    "            dconv1, df2, db2 = convolutionBackward(dconv_prev, conv_prev1, f[1], 1) #\n",
    "            #pack data\n",
    "            dconv1[conv_prev1<=0] = 0\n",
    "            conc_dconv1, df1, db1 = convolutionBackward(dconv1, conc, f[0], 1) #\n",
    "            return conc_dconv1, df2, db2, df1, db1\n",
    "        else:\n",
    "            dconv_prev = b\n",
    "            conv_prev = myin\n",
    "            conv_prev1 = dropout\n",
    "            conc = GN\n",
    "            normcache1 = ga\n",
    "            normcache2 = be\n",
    "            dconv_prev[conv_prev<=0] = 0 \n",
    "            dconv_prev, dgamma1_2, dbeta1_2 = groupnorm_backward(dconv_prev, normcache2)\n",
    "            dconv1_1, df1_2, db1_2 = convolutionBackward(dconv_prev, conv_prev1, f[1], 1) #\n",
    "            #pack data\n",
    "            dconv1_1[conv_prev1<=0] = 0\n",
    "            dconv1_1, dgamma1_1, dbeta1_1 = groupnorm_backward(dconv1_1, normcache1)\n",
    "            dga= [dgamma1_1,dgamma1_2]\n",
    "            dbe= [dbeta1_1,dbeta1_2]\n",
    "            conc_dconv1, df1_1, db1_1 = convolutionBackward(dconv1_1, conc, f[0], 1) #C9 is not needed for input,we know how to select the right gradients   \n",
    "            return conc_dconv1, df1_2, db1_2, df1_1, db1_1, dga, dbe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  TRAIN  ######\n",
    "#import torch\n",
    "import time\n",
    "time.time()\n",
    "\n",
    "\n",
    "def train(X, Y, epochs, learning_rate, dropout, GN):\n",
    "    verbose=True\n",
    "    # initiation of neural net parameters\n",
    "    #params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    trim = 0.1\n",
    "    #f1,f2,f3,f4,f5,fd1,fd2,fd3,fd4\n",
    "    filters,bias, f_dc = init_filters(5, 16, trim) #Double the channel-filter 4 times  (up to 256 and back again)\n",
    "    ##Final 1x1 filter\n",
    "    \n",
    "    \n",
    "    #out_f = np.random.randn(1,16,1,1)*trim\n",
    "    #out_b = np.random.randn(out_f.shape[0],1)*trim \n",
    "    out_f = (1,16,1,1)\n",
    "    out_f = np.random.normal(loc = 0, scale = trim , size = out_f) #np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "    out_b = (out_f.shape[0],1)\n",
    "    out_b = np.random.normal(loc = 0, scale = trim , size = out_b) #np.random.randn(f1.shape[0],1)* trimb\n",
    "    out_fb = [out_f, out_b]\n",
    "    \n",
    "    ### Initialize group normalization parameters\n",
    "    ga, be, gamma_out, beta_out = init_groupnorm_params(bias, out_b, 2, 0, 0.05)#norm_batch, lockbr, trimbr\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    if(GN>0):\n",
    "        print(\"Group Normalization Enabled!\")\n",
    "    else:\n",
    "        print(\"Group Normalization Disabled!\")\n",
    "    if(dropout>0):\n",
    "        print(\"Dropout Enabled! -  Value: {}\".format(dropout))\n",
    "    else:\n",
    "        print(\"Dropout Disabled!\")\n",
    "    print(\"Learning rate: {}\".format(learning_rate))\n",
    "    print(\"Dataset Size: {}\".format(X.shape[0]))\n",
    "    print(\"Weight scale: {}\".format(trim))\n",
    "    print(\"************************************\")\n",
    "    \n",
    "    #filter shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "\n",
    "    v_adam =  [] #filter1,filter2\n",
    "    s_adam =  [] \n",
    "    bv_adam = [] #bias1,bias2\n",
    "    bs_adam = []\n",
    "    fdc_v_adam=[] #filter,bias\n",
    "    fdc_s_adam=[]\n",
    "    #format: [v1,v2,v3,v4,v5,vd1,vd2,vd3,vd4] ,same for the rest, each of these include a tuple for the 2 conv filter\n",
    "    #[s1,s2,s3,s4,s5,sd1,sd2,vs3,sd4]\n",
    "    # upsampling filters : [v1_dc,v2_dc,v3_dc]  \n",
    "\n",
    "            \n",
    "\n",
    "    # performing calculations for subsequent iterations\n",
    "    \n",
    "    \n",
    "    [f1,f2,f3,f4,f5,f6,f7,f8,f9] = filters\n",
    "    [b1,b2,b3,b4,b5,b6,b7,b8,b9]= bias \n",
    "    [fb6_dc, fb7_dc, fb8_dc, fb9_dc] = f_dc\n",
    "    \n",
    "    last_acc = 0\n",
    "    for e in range(epochs):\n",
    "        # step forward\n",
    "        running_loss = 0\n",
    "        cost = 0\n",
    "        accuracy = 0\n",
    "        batch = 2\n",
    "        print(\"Epoch: {%d}\" %(e+1))\n",
    "        for c in range(0, X.shape[0], batch):\n",
    "            print('Batch: {}'.format(int(c/batch +1)))\n",
    "            #### Reset Gradients (Every batch) ####\n",
    "            beta1= 0.92\n",
    "            beta2= 0.995\n",
    "            lr = learning_rate\n",
    "            batch_size= batch\n",
    "            if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "                batch = X.shape[0] - c\n",
    "            X_t = X[c:(c + batch)]  # shape:(m, ch, h, w)\n",
    "            Y_t = Y[c:(c + batch)]\n",
    "            \n",
    "            \n",
    "            df =  []\n",
    "            db =  []\n",
    "            dfb=  []\n",
    "            for i in filters:\n",
    "                v1 = np.zeros(i[0].shape)\n",
    "                v2 = np.zeros(i[1].shape)\n",
    "                s1 = np.zeros(i[0].shape)\n",
    "                s2 = np.zeros(i[1].shape)\n",
    "                v_a = [v1, v2]\n",
    "                s_a = [s1, s2]\n",
    "                v_adam.append(v_a)\n",
    "                s_adam.append(s_a)\n",
    "                \n",
    "                df1_t = np.zeros(i[0].shape)\n",
    "                df2_t = np.zeros(i[1].shape)\n",
    "                f_temp = [df1_t, df2_t]\n",
    "                df.append(f_temp)\n",
    "                \n",
    "            for i in bias:\n",
    "                bv1 = np.zeros(i[0].shape)\n",
    "                bv2 = np.zeros(i[1].shape)\n",
    "                bs1 = np.zeros(i[0].shape)\n",
    "                bs2 = np.zeros(i[1].shape)    \n",
    "                bv_a = [bv1, bv2]\n",
    "                bs_a = [bs1, bs2]\n",
    "                bv_adam.append(bv_a)\n",
    "                bs_adam.append(bs_a)\n",
    "                \n",
    "                \n",
    "                db1_t = np.zeros(i[0].shape)\n",
    "                db2_t = np.zeros(i[1].shape)\n",
    "                b_temp = [db1_t, db2_t]\n",
    "                db.append(b_temp)\n",
    "            \n",
    "            for i in f_dc:\n",
    "                fdc_v1 = np.zeros(i[0].shape)\n",
    "                bdc_v2 = np.zeros(i[1].shape)\n",
    "                fdc_s1 = np.zeros(i[0].shape)\n",
    "                bdc_s2 = np.zeros(i[1].shape)    \n",
    "                fdc_v_a = [fdc_v1, bdc_v2]\n",
    "                fdc_s_a = [fdc_s1, bdc_s2]\n",
    "                fdc_v_adam.append(fdc_v_a)\n",
    "                fdc_s_adam.append(fdc_s_a)\n",
    "                \n",
    "                \n",
    "                df1_t = np.zeros(i[0].shape)\n",
    "                db1_t = np.zeros(i[1].shape)\n",
    "                fb_temp = [df1_t, db1_t]\n",
    "                dfb.append(fb_temp)\n",
    "            \n",
    "            \n",
    "            #Final layer 1x1 filter setup\n",
    "\n",
    "            v_out_f = np.zeros(out_f.shape)\n",
    "            s_out_f = np.zeros(out_f.shape)\n",
    "            bv_out_b = np.zeros(out_b.shape)\n",
    "            bs_out_b = np.zeros(out_b.shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "            dout_f = np.zeros(out_f.shape)\n",
    "            dout_b = np.zeros(out_b.shape)\n",
    "            \n",
    "            ######################################\n",
    "            \n",
    "            \n",
    "            #timestamp1 = time.time()\n",
    "            \n",
    "            \n",
    "            [df1,df2,df3,df4,df5,df6,df7,df8,df9] = df\n",
    "            [db1,db2,db3,db4,db5,db6,db7,db8,db9] = db \n",
    "            [dfb6_dc,dfb7_dc,dfb8_dc,dfb9_dc]     = dfb\n",
    "            \n",
    "            \n",
    "            for b in range(batch):\n",
    "                \n",
    "                print('Image: {}/{}'.format((b+1),batch))\n",
    "                #################### TODO: BLOCK IMPLEMENTATION - FUTURE UPDATE ######################\n",
    "                    \n",
    "                    \n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ######################## Forward Propagation Convolution Part  ##########################\n",
    "\n",
    "                \n",
    "                ########### 1st Big Layer ###########    (with zero padding ='same',so with stride =1 we get same dim as the input)\n",
    "                conv1_1, conv1_2, normcache1_1, normcache1_2 = Conv_Block(\"Forward\", f1, b1, X_t[b], dropout, GN, ga[0], be[0])\n",
    "                ##################################### conv1_2: 128x128x16\n",
    "                \n",
    "                pl1 = maxpool(conv1_2, 2, 2) #   pl1 : (128-2)/2+1  = 64 \n",
    "                \n",
    "                ########### 2nd Big Layer ########### \n",
    "                conv2_1, conv2_2, normcache2_1, normcache2_2 = Conv_Block(\"Forward\", f2, b2, pl1, dropout, GN, ga[1], be[1])          \n",
    "                #####################################  64x64x32\n",
    "\n",
    "                pl2 = maxpool(conv2_2, 2, 2) #pool_f = 2 , pool_s = 2    , (64 -2)/2 +1 = 32\n",
    "\n",
    "                ########### 3rd Big Layer ###########\n",
    "                conv3_1, conv3_2, normcache3_1, normcache3_2 = Conv_Block(\"Forward\", f3, b3, pl2, dropout, GN, ga[2], be[2])          \n",
    "                #####################################  32x32x64\n",
    "\n",
    "                pl3 = maxpool(conv3_2, 2, 2) #pool_f = 2 , pool_s = 2   ,  (32-2)/2 +1 = 16\n",
    "\n",
    "                ########### 4th Big Layer ###########\n",
    "                conv4_1, conv4_2, normcache4_1, normcache4_2 = Conv_Block(\"Forward\", f4, b4, pl3, dropout, GN, ga[3], be[3])             \n",
    "                #####################################     16x16x128\n",
    "\n",
    "                pl4 = maxpool(conv4_2, 2, 2) #pool_f = 2 , pool_s = 2  , (16-2)/2 +1 =8  : 8x8x128\n",
    "                \n",
    "                ########### 5th Big Layer ###########   8x8x128-->8x8x256\n",
    "                conv5_1, conv5_2, normcache5_1, normcache5_2 = Conv_Block(\"Forward\", f5, b5, pl4, dropout, GN, ga[4], be[4])       \n",
    "                #####################################  8x8x256\n",
    "\n",
    "                #####################################\n",
    "                #Because of ambigious size after the upsampling the concat func must take care possible crop of the conv#_2 \n",
    "                #####################################\n",
    "                #Deconvolution/Upsampling\n",
    "                # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "                params = [fb6_dc[0], fb6_dc[1]] # deconv filter, deconv bias\n",
    "                dc6, new_in6 = convTransp(conv5_2, params, 1, 0)   #result:   =  16x16x128 , # conv5_2 requires NO crop\n",
    "                #Concat dc6 with conv4_2 so we get 256 channels (16x16x256)\n",
    "                c6 = concat(dc6, conv4_2) # 1st one is the right one size  \n",
    "                \n",
    "                ########### 6th Big Layer ###########          16x16x256     \n",
    "                conv6_1, conv6_2, normcache6_1, normcache6_2 = Conv_Block(\"Forward\", f6, b6, c6, dropout, GN, ga[5], be[5])  \n",
    "                #####################################    16x16x128\n",
    "                #(16-1)*2 + 2 =32\n",
    "                params = [fb7_dc[0], fb7_dc[1]] # deconv filter, deconv bias\n",
    "                dc7, new_in7 = convTransp(conv6_2, params, 1, 0)   #result:   =  32x32x64\n",
    "                #Concat dc7 with conv3_2 so we get  channels (32x32x128)\n",
    "                c7 = concat(dc7, conv3_2)   \n",
    "                \n",
    "                ########### 7th Big Layer ###########          32x32x128     \n",
    "                conv7_1, conv7_2, normcache7_1, normcache7_2 = Conv_Block(\"Forward\", f7, b7, c7, dropout, GN, ga[6], be[6]) \n",
    "                #####################################    32x32x64\n",
    "                #(24-1)*2 + 2 = 48\n",
    "                params = [fb8_dc[0], fb8_dc[1]] # deconv filter, deconv bias\n",
    "                dc8, new_in8 = convTransp(conv7_2, params, 1, 0)   #result:   =  64x64x32\n",
    "                #Concat dc8 with conv2_2 so we get  channels (64x64x64)\n",
    "                c8 = concat(dc8 ,conv2_2)   \n",
    "                \n",
    "                ########### 8th Big Layer ###########          64x64x64    \n",
    "                conv8_1, conv8_2, normcache8_1, normcache8_2 = Conv_Block(\"Forward\", f8, b8, c8, dropout, GN, ga[7], be[7])\n",
    "                #####################################    64x64x32                              \n",
    "                #(64-1)*2 + 2 = 128\n",
    "                params = [fb9_dc[0], fb9_dc[1]] # deconv filter, deconv bias\n",
    "                dc9, new_in9 = convTransp(conv8_2, params, 1, 0)   #result:   =  128x128x16\n",
    "                #Concat dc9 with conv1_2 so we get  channels (128x128x32)\n",
    "                c9 = concat(dc9, conv1_2)                   \n",
    "               \n",
    "                ########### 9th Big Layer ###########          128x128x32   \n",
    "                conv9_1, conv9_2, normcache9_1, normcache9_2 = Conv_Block(\"Forward\", f9, b9, c9, dropout, GN, ga[8], be[8])\n",
    "                #####################################    128x128x16\n",
    "                \n",
    "                ############################# Last Layer conv(1x1) --> 128x128x1 ##########################\n",
    "                params = [out_f, out_b]\n",
    "                output = conv(conv9_2, params, 1, 0) #output.shape: 128x128x1\n",
    "                \n",
    "                #print(output[:,0:10,0:10])\n",
    "                #if(GN == 0):\n",
    "                output = normalize(output)\n",
    "                ## Sigmoid ##\n",
    "                Y_hat = sigmoid(output)\n",
    "                \n",
    "                #label crop is needed\n",
    "                #Y_t_b = crop(Y_t[b], Y_hat.shape[1])\n",
    "                #plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "                cost_,accuracy_ = Dice_Coef(Y_hat, Y_t[b])\n",
    "                cost += cost_\n",
    "                accuracy += accuracy_\n",
    "                print(accuracy_*100)\n",
    "                if((c+1) == X.shape[0]): #assuming that batch is always  1\n",
    "                    if (accuracy/(c+1)>last_acc):\n",
    "                        #plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "                        last_acc = accuracy/(c+1)\n",
    "                        print(\"New parameters Saved!\")\n",
    "                        GN_params = [ga, be, gamma_out, beta_out]\n",
    "                        parameters = [filters, bias, f_dc, out_fb, GN_params]\n",
    "                        path = os.path.join(os.path.expanduser('~/'), 'data', 'salt')\n",
    "                        with open(path+'/weights', 'wb') as fp:\n",
    "                            pickle.dump(parameters, fp)\n",
    "                    if ((accuracy/(c+1))>0.99):\n",
    "                        print(\"Latest Accuracy: {}%\".format(accuracy*100))\n",
    "                        params_values = [filters, bias, f_dc, out_fb]\n",
    "                        return params_values\n",
    "                if(batch>1):\n",
    "                    if((b+1) == batch):\n",
    "                        if((accuracy/batch)>last_acc):\n",
    "                            print(\"New parameters Saved!\")\n",
    "                            last_acc = accuracy/batch\n",
    "                            #Saving\n",
    "                            GN_params = [ga, be, gamma_out, beta_out]\n",
    "                            parameters = [filters, bias, f_dc, out_fb, GN_params]\n",
    "                            path = os.path.join(os.path.expanduser('~/'), 'data', 'salt')\n",
    "                            with open(path+'/weights', 'wb') as fp:\n",
    "                                pickle.dump(parameters, fp)\n",
    "                    \n",
    "                \n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ################################# Backward Propagation ##################################\n",
    "                \n",
    "                \n",
    "                #Pixel-wise sub, we we can get the diff(Y includes the 2 classes 0 and 1)\n",
    "                \n",
    "                dA_prev = Y_hat - Y_t[b]\n",
    "                dZ_prev = sigmoid_backward(dA_prev, output)\n",
    "                conv_s =1 \n",
    "                dconv9_2, dout_f_, dout_b_ = convolutionBackward(dZ_prev, conv9_2, out_f, conv_s) #       \n",
    "                \n",
    "                if(GN == 0):\n",
    "                    conc_dconv9, df9_2, db9_2, df9_1, db9_1 = Conv_Block(\"Backward\", f9, dconv9_2, conv9_2, conv9_1, 0, c9, 0)\n",
    "                else:\n",
    "                    conc_dconv9, df9_2, db9_2, df9_1, db9_1, dga9, dbe9 = Conv_Block(\"Backward\", f9, dconv9_2, conv9_2, conv9_1, c9, normcache9_1, normcache9_2)\n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv9, dconv1_2 = crop2half(conc_dconv9)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #conv8_2 is not needed for input,we know how to select the right gradients            \n",
    "                #dconv1_2 = reshape(dconv1_2, conv1_2.shape[1])\n",
    "                \n",
    "                dconv8_2, df9_dc, db9_dc = convTranspBackward(dconv9, new_in9, fb9_dc[0],conv_s)\n",
    "                #pack data\n",
    "\n",
    "                if(GN == 0):\n",
    "                    conc_dconv8, df8_2, db8_2, df8_1, db8_1 = Conv_Block(\"Backward\", f8, dconv8_2, conv8_2, conv8_1, 0, c8, 0)\n",
    "                else:\n",
    "                    conc_dconv8, df8_2, db8_2, df8_1, db8_1, dga8, dbe8 = Conv_Block(\"Backward\", f8, dconv8_2, conv8_2, conv8_1, c8, normcache8_1, normcache8_2)\n",
    "                    \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv8, dconv2_2 = crop2half(conc_dconv8)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #dconv2_2 = reshape(dconv2_2, conv2_2.shape[1])\n",
    "                \n",
    "                dconv7_2, df8_dc, db8_dc = convTranspBackward(dconv8, new_in8, fb8_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    conc_dconv7, df7_2, db7_2, df7_1, db7_1 = Conv_Block(\"Backward\", f7, dconv7_2, conv7_2, conv7_1, 0, c7, 0)\n",
    "                else:\n",
    "                    conc_dconv7, df7_2, db7_2, df7_1, db7_1, dga7, dbe7 = Conv_Block(\"Backward\", f7, dconv7_2, conv7_2, conv7_1, c7, normcache7_1, normcache7_2)\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv7, dconv3_2 = crop2half(conc_dconv7)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #Make sure that dconv3_2 is the same dim with the dconv3_2 that will come from maxpool in decoding side\n",
    "                #dconv3_2 = reshape(dconv3_2, conv3_2.shape[1])\n",
    "                \n",
    "                dconv6_2, df7_dc, db7_dc = convTranspBackward(dconv7, new_in7, fb7_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    conc_dconv6, df6_2, db6_2, df6_1, db6_1 = Conv_Block(\"Backward\", f6, dconv6_2, conv6_2, conv6_1, 0, c6, 0)\n",
    "                else:     \n",
    "                    conc_dconv6, df6_2, db6_2, df6_1, db6_1, dga6, dbe6 = Conv_Block(\"Backward\", f6, dconv6_2, conv6_2, conv6_1, c6, normcache6_1, normcache6_2)\n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv6, dconv4_2 = crop2half(conc_dconv6)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #dconv4_2 = reshape(dconv4_2, conv4_2.shape[1])\n",
    "                \n",
    "                dconv5_2, df6_dc, db6_dc = convTranspBackward(dconv6, new_in6, fb6_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    dpl4, df5_2, db5_2, df5_1, db5_1 = Conv_Block(\"Backward\", f5, dconv5_2, conv5_2, conv5_1, 0, pl4, 0)\n",
    "                else:     \n",
    "                    dpl4, df5_2, db5_2, df5_1, db5_1, dga5, dbe5 = Conv_Block(\"Backward\", f5, dconv5_2, conv5_2, conv5_1, pl4, normcache5_1, normcache5_2)\n",
    "\n",
    "                \n",
    "                dconv4_2 += maxpoolBackward(dpl4, conv4_2, f=2 , s=2) #Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    dpl3, df4_2, db4_2, df4_1, db4_1 = Conv_Block(\"Backward\", f4, dconv4_2, conv4_2, conv4_1, 0, pl3, 0)\n",
    "                else:     \n",
    "                    dpl3, df4_2, db4_2, df4_1, db4_1, dga4, dbe4 = Conv_Block(\"Backward\", f4, dconv4_2, conv4_2, conv4_1, pl3, normcache4_1, normcache4_2)\n",
    "\n",
    "\n",
    "                dconv3_2 += maxpoolBackward(dpl3, conv3_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    dpl2, df3_2, db3_2, df3_1, db3_1 = Conv_Block(\"Backward\", f3, dconv3_2, conv3_2, conv3_1, 0, pl2, 0)\n",
    "                else:     \n",
    "                    dpl2, df3_2, db3_2, df3_1, db3_1, dga3, dbe3 = Conv_Block(\"Backward\", f3, dconv3_2, conv3_2, conv3_1, pl2, normcache3_1, normcache3_2)\n",
    "\n",
    "                \n",
    "                dconv2_2 += maxpoolBackward(dpl2, conv2_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    dpl1, df2_2, db2_2, df2_1, db2_1 = Conv_Block(\"Backward\", f2, dconv2_2, conv2_2, conv2_1, 0, pl1, 0)\n",
    "                else:     \n",
    "                    dpl1, df2_2, db2_2, df2_1, db2_1, dga2, dbe2 = Conv_Block(\"Backward\", f2, dconv2_2, conv2_2, conv2_1, pl1, normcache2_1, normcache2_2)\n",
    "\n",
    "                \n",
    "                dconv1_2 += maxpoolBackward(dpl1, conv1_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    _, df1_2, db1_2, df1_1, db1_1 = Conv_Block(\"Backward\", f1, dconv1_2, conv1_2, conv1_1, 0, X_t[b], 0)\n",
    "                else:     \n",
    "                    _, df1_2, db1_2, df1_1, db1_1, dga1, dbe1 = Conv_Block(\"Backward\", f1, dconv1_2, conv1_2, conv1_1, X_t[b], normcache1_1, normcache1_2)\n",
    "\n",
    "                \n",
    "                \n",
    "                if(GN == 1):\n",
    "                    dgamma = [dga1,dga2,dga3,dga4,dga5,dga6,dga7,dga8,dga9]\n",
    "                    dbeta = [dbe1,dbe2,dbe3,dbe4,dbe5,dbe6,dbe7,dbe8,dbe9]\n",
    "                \n",
    "\n",
    "                [df1,df2,df3,df4,df5,df6,df7,df8,df9] = df\n",
    "                [db1,db2,db3,db4,db5,db6,db7,db8,db9] = db \n",
    "                [dfb6_dc,dfb7_dc,dfb8_dc,dfb9_dc]     = dfb\n",
    "\n",
    "                \n",
    "                df1[0] += df1_1\n",
    "                df1[1] += df1_2\n",
    "                df2[0] += df2_1\n",
    "                df2[1] += df2_2\n",
    "                df3[0] += df3_1\n",
    "                df3[1] += df3_2\n",
    "                df4[0] += df4_1\n",
    "                df4[1] += df4_2\n",
    "                df5[0] += df5_1\n",
    "                df5[1] += df5_2\n",
    "                df6[0] += df6_1\n",
    "                df6[1] += df6_2\n",
    "                df7[0] += df7_1\n",
    "                df7[1] += df7_2\n",
    "                df8[0] += df8_1\n",
    "                df8[1] += df8_2\n",
    "                df9[0] += df9_1\n",
    "                df9[1] += df9_2\n",
    "                \n",
    "                db1[0] += db1_1\n",
    "                db1[1] += db1_2\n",
    "                db2[0] += db2_1\n",
    "                db2[1] += db2_2\n",
    "                db3[0] += db3_1\n",
    "                db3[1] += db3_2\n",
    "                db4[0] += db4_1\n",
    "                db4[1] += db4_2\n",
    "                db5[0] += db5_1\n",
    "                db5[1] += db5_2\n",
    "                db6[0] += db6_1\n",
    "                db6[1] += db6_2\n",
    "                db7[0] += db7_1\n",
    "                db7[1] += db7_2\n",
    "                db8[0] += db8_1\n",
    "                db8[1] += db8_2\n",
    "                db9[0] += db9_1\n",
    "                db9[1] += db9_2\n",
    "\n",
    "                dfb6_dc[0] += df6_dc\n",
    "                dfb6_dc[1] += db6_dc\n",
    "                dfb7_dc[0] += df7_dc\n",
    "                dfb7_dc[1] += db7_dc\n",
    "                dfb8_dc[0] += df8_dc\n",
    "                dfb8_dc[1] += db8_dc\n",
    "                dfb9_dc[0] += df9_dc\n",
    "                dfb9_dc[1] += db9_dc\n",
    "\n",
    "                dout_f += dout_f_\n",
    "                dout_b += dout_b_\n",
    "                if(GN == 1):\n",
    "                    mytrim = 20\n",
    "                    for i in range(len(ga)):\n",
    "                        ga[i][0] -= lr*mytrim*dgamma[i][0]\n",
    "                        ga[i][1] -= lr*mytrim*dgamma[i][1]\n",
    "\n",
    "                        be[i][0] -= lr*mytrim*dbeta[i][0]\n",
    "                        be[i][1] -= lr*mytrim*dbeta[i][1]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ############## Adam Optimization ################\n",
    "            #changing the main structures(which are also updated)\n",
    "            #TODO: Future update - remove uneccessary memory loads/stores, v,s dont need to be saved \n",
    "            for i in range(len(filters)):\n",
    "                v_adam[i][0] = beta1*v_adam[i][0] + (1-beta1)*df[i][0]/batch_size #f1\n",
    "                s_adam[i][0] = beta2*s_adam[i][0] + (1-beta2)*(df[i][0]/batch_size)**2 #f1\n",
    "                filters[i][0] -= lr*v_adam[i][0]/np.sqrt(s_adam[i][0] + 1e-7)\n",
    "\n",
    "                v_adam[i][1] = beta1*v_adam[i][1] + (1-beta1)*df[i][1]/batch_size #f2\n",
    "                s_adam[i][1] = beta2*s_adam[i][1] + (1-beta2)*(df[i][1]/batch_size)**2  #f2\n",
    "                filters[i][1] -= lr*v_adam[i][1]/np.sqrt(s_adam[i][1] + 1e-7)\n",
    "\n",
    "            for i in range(len(bias)):\n",
    "                bv_adam[i][0] = beta1*bv_adam[i][0] + (1-beta1)*db[i][0]/batch_size #b1\n",
    "                bs_adam[i][0] = beta2*bs_adam[i][0] + (1-beta2)*(db[i][0]/batch_size)**2  #b1\n",
    "                bias[i][0] -= lr*bv_adam[i][0]/np.sqrt(bs_adam[i][0] + 1e-7)\n",
    "\n",
    "                bv_adam[i][1] = beta1*bv_adam[i][1] + (1-beta1)*db[i][1]/batch_size #b2\n",
    "                bs_adam[i][1] = beta2*bs_adam[i][1] + (1-beta2)*(db[i][1]/batch_size)**2  #b2\n",
    "                bias[i][1] -= lr*bv_adam[i][1]/np.sqrt(bs_adam[i][1] + 1e-7)\n",
    "\n",
    "            for i in range(len(f_dc)):\n",
    "                fdc_v_adam[i][0] = beta1*fdc_v_adam[i][0] + (1-beta1)*dfb[i][0]/batch_size #f1\n",
    "                fdc_s_adam[i][0] = beta2*fdc_s_adam[i][0] + (1-beta2)*(dfb[i][0]/batch_size)**2  #f1\n",
    "                f_dc[i][0] -= lr*fdc_v_adam[i][0]/np.sqrt(fdc_s_adam[i][0] + 1e-7)\n",
    "\n",
    "                fdc_v_adam[i][1] = beta1*fdc_v_adam[i][1] + (1-beta1)*dfb[i][1]/batch_size #b2\n",
    "                fdc_s_adam[i][1] = beta2*fdc_s_adam[i][1] + (1-beta2)*(dfb[i][1]/batch_size)**2  #b2\n",
    "                f_dc[i][1] -= lr*fdc_v_adam[i][1]/np.sqrt(fdc_s_adam[i][1] + 1e-7)    \n",
    "\n",
    "            v_out_f = beta1*v_out_f + (1 - beta1)*dout_f/batch_size #f\n",
    "            s_out_f = beta2*s_out_f + (1 - beta2)*(dout_f/batch_size)**2  #f\n",
    "            out_fb[0] -= lr*v_out_f/np.sqrt(s_out_f + 1e-7)\n",
    "\n",
    "            bv_out_b = beta1*bv_out_b + (1 - beta1)*dout_b/batch_size #b\n",
    "            bs_out_b = beta2*bs_out_b + (1 - beta2)*(dout_b/batch_size)**2  #b\n",
    "            out_fb[1] -= lr*bv_out_b/np.sqrt(bs_out_b + 1e-7)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            for i in range(len(filters)):\n",
    "                filters[i][0] -= lr*df[i][0]\n",
    "                bias[i][0] -= lr*db[i][0]\n",
    "            \n",
    "            for i in range(len(f_dc)):\n",
    "                f_dc[i][0] -= lr*dfb[i][0]\n",
    "            \n",
    "            out_fb[0] -= lr*dout_f\n",
    "            out_fb[1] -= lr*dout_b\n",
    "            \n",
    "            #Updating Conv Part\n",
    "            \n",
    "\n",
    "            #print(Yh)\n",
    "            #print(t)\n",
    "            #print(\"Batch:{}\".format(c+12))\n",
    "            \n",
    "           \n",
    "            f1 -= (learning_rate) * (df1/batch)\n",
    "            f2 -= (learning_rate) * (df2/batch)\n",
    "            b1 -= (learning_rate) * (db1/batch)\n",
    "            b2 -= (learning_rate) * (db2/batch)\n",
    "            filter_params = [f1, f2, b1, b2]\n",
    "                \n",
    "            # updating FC Part\n",
    "            #params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "            grads_values['dW1'] = dW1#/(batch)\n",
    "            grads_values['dW2'] = dW2#/(batch)\n",
    "            grads_values['db1'] = dB1#/(batch)\n",
    "            grads_values['db2'] = dB2#/(batch)\n",
    "                   \n",
    "            for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "                \n",
    "                params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "                params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "            \n",
    "            '''\n",
    "            \n",
    "        #END OF LOOP - EPOCH\n",
    "        #timestamp2 = time.time()\n",
    "        #print (\"This took %.2f seconds\" %(timestamp2 - timestamp1))\n",
    "        if(batch == 1):\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/(c+1), (accuracy*100)/(X.shape[0])))\n",
    "        else:\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/batch, (accuracy*100)/(X.shape[0])))\n",
    "        #if(callback is not None):\n",
    "        #    callback(i, params_values)\n",
    "    #pack filters\n",
    "    #params_values = [filters, bias, f_dc, out_fb]\n",
    "    fp.close()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************\n",
      "Group Normalization Enabled!\n",
      "Dropout Disabled!\n",
      "Learning rate: 0.008\n",
      "Dataset Size: 4\n",
      "Weight scale: 0.1\n",
      "************************************\n",
      "Epoch: {1}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "62.826732122172245\n",
      "Image: 2/2\n",
      "70.33743381273531\n",
      "New parameters Saved!\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "59.614518562870366\n",
      "Image: 2/2\n",
      "56.96200702228398\n",
      "New parameters Saved!\n",
      "Epoch:     1   -   cost: 0.95   -   Accuracy: 124.87%\n",
      "Epoch: {2}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "63.85030588762245\n",
      "Image: 2/2\n",
      "66.82705955298476\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "60.50702376035081\n",
      "Image: 2/2\n",
      "59.87945477545891\n",
      "New parameters Saved!\n",
      "Epoch:     2   -   cost: 0.93   -   Accuracy: 125.53%\n",
      "Epoch: {3}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "62.03719649062119\n",
      "Image: 2/2\n",
      "65.7209224644268\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "60.26870299836293\n",
      "Image: 2/2\n",
      "59.88242073040859\n",
      "Epoch:     3   -   cost: 0.96   -   Accuracy: 123.95%\n",
      "Epoch: {4}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "60.61483525718325\n",
      "Image: 2/2\n",
      "66.85713709185812\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "62.10702539108822\n",
      "Image: 2/2\n",
      "60.00582390285505\n",
      "Epoch:     4   -   cost: 0.95   -   Accuracy: 124.79%\n",
      "Epoch: {5}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "64.59536624870334\n",
      "Image: 2/2\n",
      "70.68912635020948\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "64.50960485838834\n",
      "Image: 2/2\n",
      "67.79912568488325\n",
      "New parameters Saved!\n",
      "Epoch:     5   -   cost: 0.81   -   Accuracy: 133.80%\n",
      "Epoch: {6}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "64.28714340108525\n",
      "Image: 2/2\n",
      "72.38447395772363\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "62.22305388770537\n",
      "Image: 2/2\n",
      "69.99725633756067\n",
      "New parameters Saved!\n",
      "Epoch:     6   -   cost: 0.80   -   Accuracy: 134.45%\n",
      "Epoch: {7}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "67.04372488760568\n",
      "Image: 2/2\n",
      "73.5529161243221\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "65.0643491153088\n",
      "Image: 2/2\n",
      "70.14327918990485\n",
      "New parameters Saved!\n",
      "Epoch:     7   -   cost: 0.75   -   Accuracy: 137.90%\n",
      "Epoch: {8}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "67.79040839564733\n",
      "Image: 2/2\n",
      "73.66070313608827\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "65.85306550650314\n",
      "Image: 2/2\n",
      "70.0116781526616\n",
      "New parameters Saved!\n",
      "Epoch:     8   -   cost: 0.73   -   Accuracy: 138.66%\n",
      "Epoch: {9}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "68.1080521728697\n",
      "Image: 2/2\n",
      "73.70580389701264\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "65.9907452426096\n",
      "Image: 2/2\n",
      "70.15318631025704\n",
      "New parameters Saved!\n",
      "Epoch:     9   -   cost: 0.73   -   Accuracy: 138.98%\n",
      "Epoch: {10}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "68.07024768417787\n",
      "Image: 2/2\n",
      "73.93618016725127\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "65.97909857605707\n",
      "Image: 2/2\n",
      "70.78394646264266\n",
      "New parameters Saved!\n",
      "Epoch:    10   -   cost: 0.72   -   Accuracy: 139.38%\n",
      "Epoch: {11}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "68.20024426527813\n",
      "Image: 2/2\n",
      "74.16613160742934\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "66.09649779450532\n",
      "Image: 2/2\n",
      "70.94347466754853\n",
      "New parameters Saved!\n",
      "Epoch:    11   -   cost: 0.72   -   Accuracy: 139.70%\n",
      "Epoch: {12}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "68.17420903900219\n",
      "Image: 2/2\n",
      "74.42187157534936\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "66.62947719614706\n",
      "Image: 2/2\n",
      "71.13979523368307\n",
      "New parameters Saved!\n",
      "Epoch:    12   -   cost: 0.71   -   Accuracy: 140.18%\n",
      "Epoch: {13}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "68.33001465251103\n",
      "Image: 2/2\n",
      "74.55707875401586\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "67.06486153377473\n",
      "Image: 2/2\n",
      "70.96940624801786\n",
      "New parameters Saved!\n",
      "Epoch:    13   -   cost: 0.71   -   Accuracy: 140.46%\n",
      "Epoch: {14}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "67.98394014763706\n",
      "Image: 2/2\n",
      "74.46977516496868\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "66.92299417397417\n",
      "Image: 2/2\n",
      "71.27124747312232\n",
      "Epoch:    14   -   cost: 0.71   -   Accuracy: 140.32%\n",
      "Epoch: {15}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "68.78069211889671\n",
      "Image: 2/2\n",
      "74.33015064214676\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "67.72336626709948\n",
      "Image: 2/2\n",
      "71.8816016653359\n",
      "New parameters Saved!\n",
      "Epoch:    15   -   cost: 0.70   -   Accuracy: 141.36%\n",
      "Epoch: {16}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "68.61208275750282\n",
      "Image: 2/2\n",
      "74.56783146957784\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "67.30058192122861\n",
      "Image: 2/2\n",
      "72.33232919290661\n",
      "New parameters Saved!\n",
      "Epoch:    16   -   cost: 0.70   -   Accuracy: 141.41%\n",
      "Epoch: {17}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "68.28464905300379\n",
      "Image: 2/2\n",
      "75.79603337058512\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "67.97884608331559\n",
      "Image: 2/2\n",
      "73.28707181429354\n",
      "New parameters Saved!\n",
      "Epoch:    17   -   cost: 0.68   -   Accuracy: 142.67%\n",
      "Epoch: {18}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "68.81267482561358\n",
      "Image: 2/2\n",
      "74.45187119782226\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "68.80538146622848\n",
      "Image: 2/2\n",
      "73.58375854552625\n",
      "New parameters Saved!\n",
      "Epoch:    18   -   cost: 0.67   -   Accuracy: 142.83%\n",
      "Epoch: {19}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "69.83481358580192\n",
      "Image: 2/2\n",
      "76.35217019107299\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "69.10463259954747\n",
      "Image: 2/2\n",
      "71.30968886096736\n",
      "New parameters Saved!\n",
      "Epoch:    19   -   cost: 0.67   -   Accuracy: 143.30%\n",
      "Epoch: {20}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "68.60987229321013\n",
      "Image: 2/2\n",
      "75.0016922648135\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "68.85003476535616\n",
      "Image: 2/2\n",
      "72.06499079317771\n",
      "Epoch:    20   -   cost: 0.68   -   Accuracy: 142.26%\n",
      "Epoch: {21}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "68.27330530863772\n",
      "Image: 2/2\n",
      "75.09461501119465\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "69.62131877733232\n",
      "Image: 2/2\n",
      "74.87562822960875\n",
      "New parameters Saved!\n",
      "Epoch:    21   -   cost: 0.66   -   Accuracy: 143.93%\n",
      "Epoch: {22}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "70.46117325005865\n",
      "Image: 2/2\n",
      "75.76442171535943\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "69.8598253616611\n",
      "Image: 2/2\n",
      "74.31611914751666\n",
      "New parameters Saved!\n",
      "Epoch:    22   -   cost: 0.64   -   Accuracy: 145.20%\n",
      "Epoch: {23}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "72.1918618681048\n",
      "Image: 2/2\n",
      "77.12866315395728\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "72.27952689242875\n",
      "Image: 2/2\n",
      "76.48330248131671\n",
      "New parameters Saved!\n",
      "Epoch:    23   -   cost: 0.59   -   Accuracy: 149.04%\n",
      "Epoch: {24}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "72.2427336347206\n",
      "Image: 2/2\n",
      "77.687362880078\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "68.70356660372725\n",
      "Image: 2/2\n",
      "76.5541720058167\n",
      "Epoch:    24   -   cost: 0.61   -   Accuracy: 147.59%\n",
      "Epoch: {25}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "74.18924008406464\n",
      "Image: 2/2\n",
      "79.42301056175425\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "72.76129688054866\n",
      "Image: 2/2\n",
      "77.5346860461665\n",
      "New parameters Saved!\n",
      "Epoch:    25   -   cost: 0.55   -   Accuracy: 151.95%\n",
      "Epoch: {26}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "76.22343845686453\n",
      "Image: 2/2\n",
      "81.03201855185992\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "72.79702722960202\n",
      "Image: 2/2\n",
      "77.38645986014696\n",
      "New parameters Saved!\n",
      "Epoch:    26   -   cost: 0.53   -   Accuracy: 153.72%\n",
      "Epoch: {27}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "77.70822950530999\n",
      "Image: 2/2\n",
      "82.24944638371672\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "74.91244537305668\n",
      "Image: 2/2\n",
      "79.43361086569875\n",
      "New parameters Saved!\n",
      "Epoch:    27   -   cost: 0.48   -   Accuracy: 157.15%\n",
      "Epoch: {28}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "77.2609875843514\n",
      "Image: 2/2\n",
      "80.63931427197957\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "73.14454561605443\n",
      "Image: 2/2\n",
      "78.13952861788252\n",
      "Epoch:    28   -   cost: 0.52   -   Accuracy: 154.59%\n",
      "Epoch: {29}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "79.37118743314599\n",
      "Image: 2/2\n",
      "82.58767188416655\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "75.77623430908363\n",
      "Image: 2/2\n",
      "80.22395203868304\n",
      "New parameters Saved!\n",
      "Epoch:    29   -   cost: 0.46   -   Accuracy: 158.98%\n",
      "Epoch: {30}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "83.00986247705076\n",
      "Image: 2/2\n",
      "85.15100114637836\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "77.02662362968059\n",
      "Image: 2/2\n",
      "82.2062060036391\n",
      "New parameters Saved!\n",
      "Epoch:    30   -   cost: 0.40   -   Accuracy: 163.70%\n",
      "Epoch: {31}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "84.22086260485753\n",
      "Image: 2/2\n",
      "87.48647173689858\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "80.53252846151975\n",
      "Image: 2/2\n",
      "83.10678451280859\n",
      "New parameters Saved!\n",
      "Epoch:    31   -   cost: 0.35   -   Accuracy: 167.67%\n",
      "Epoch: {32}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "85.17459016562728\n",
      "Image: 2/2\n",
      "88.4862705088512\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "81.50301585723255\n",
      "Image: 2/2\n",
      "82.33900816564541\n",
      "New parameters Saved!\n",
      "Epoch:    32   -   cost: 0.34   -   Accuracy: 168.75%\n",
      "Epoch: {33}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "84.25838884373638\n",
      "Image: 2/2\n",
      "90.07199571733841\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "84.06572362897064\n",
      "Image: 2/2\n",
      "85.10270439572308\n",
      "New parameters Saved!\n",
      "Epoch:    33   -   cost: 0.31   -   Accuracy: 171.75%\n",
      "Epoch: {34}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "86.74767807709374\n",
      "Image: 2/2\n",
      "90.71920860955537\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "83.40691616433405\n",
      "Image: 2/2\n",
      "85.89695918650605\n",
      "New parameters Saved!\n",
      "Epoch:    34   -   cost: 0.29   -   Accuracy: 173.39%\n",
      "Epoch: {35}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "84.95074827225231\n",
      "Image: 2/2\n",
      "90.18449783009494\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "84.90180476232624\n",
      "Image: 2/2\n",
      "85.57353788125486\n",
      "Epoch:    35   -   cost: 0.29   -   Accuracy: 172.81%\n",
      "Epoch: {36}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "88.5789349030017\n",
      "Image: 2/2\n",
      "91.75276616128923\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "85.07721469242317\n",
      "Image: 2/2\n",
      "86.7594302431753\n",
      "New parameters Saved!\n",
      "Epoch:    36   -   cost: 0.26   -   Accuracy: 176.08%\n",
      "Epoch: {37}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "88.70731013300339\n",
      "Image: 2/2\n",
      "92.24240158110618\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "85.78955247234109\n",
      "Image: 2/2\n",
      "86.94384911181629\n",
      "New parameters Saved!\n",
      "Epoch:    37   -   cost: 0.25   -   Accuracy: 176.84%\n",
      "Epoch: {38}\n",
      "Batch: 1\n",
      "Image: 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.67742641692196\n",
      "Image: 2/2\n",
      "93.30905286061858\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "87.39460956186213\n",
      "Image: 2/2\n",
      "87.34050750743144\n",
      "New parameters Saved!\n",
      "Epoch:    38   -   cost: 0.22   -   Accuracy: 178.86%\n",
      "Epoch: {39}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "90.49569863512197\n",
      "Image: 2/2\n",
      "93.52030520078388\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "87.60409654947601\n",
      "Image: 2/2\n",
      "88.31804210512617\n",
      "New parameters Saved!\n",
      "Epoch:    39   -   cost: 0.21   -   Accuracy: 179.97%\n",
      "Epoch: {40}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "92.33169374462348\n",
      "Image: 2/2\n",
      "94.93380655453949\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "88.06050415769425\n",
      "Image: 2/2\n",
      "89.93065736422815\n",
      "New parameters Saved!\n",
      "Epoch:    40   -   cost: 0.18   -   Accuracy: 182.63%\n",
      "Epoch: {41}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "93.5948562396817\n",
      "Image: 2/2\n",
      "95.1267790081304\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "89.03943928637311\n",
      "Image: 2/2\n",
      "89.56479357902123\n",
      "New parameters Saved!\n",
      "Epoch:    41   -   cost: 0.17   -   Accuracy: 183.66%\n",
      "Epoch: {42}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "89.31444608789413\n",
      "Image: 2/2\n",
      "92.45649350035792\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "82.54594691018063\n",
      "Image: 2/2\n",
      "87.26147875768791\n",
      "Epoch:    42   -   cost: 0.26   -   Accuracy: 175.79%\n",
      "Epoch: {43}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "91.62363897231712\n",
      "Image: 2/2\n",
      "94.96836368985083\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "87.36634773513074\n",
      "Image: 2/2\n",
      "89.10588923704118\n",
      "Epoch:    43   -   cost: 0.19   -   Accuracy: 181.53%\n",
      "Epoch: {44}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "92.84696818395581\n",
      "Image: 2/2\n",
      "95.44106255998834\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "89.26519380410922\n",
      "Image: 2/2\n",
      "88.95909702092688\n",
      "Epoch:    44   -   cost: 0.18   -   Accuracy: 183.26%\n",
      "Epoch: {45}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "93.22421288570537\n",
      "Image: 2/2\n",
      "95.69199501141541\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "89.07992178903584\n",
      "Image: 2/2\n",
      "89.69645191355666\n",
      "New parameters Saved!\n",
      "Epoch:    45   -   cost: 0.17   -   Accuracy: 183.85%\n",
      "Epoch: {46}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "93.36564095316844\n",
      "Image: 2/2\n",
      "95.80140163891147\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "88.14318459553007\n",
      "Image: 2/2\n",
      "87.86237904945202\n",
      "Epoch:    46   -   cost: 0.18   -   Accuracy: 182.59%\n",
      "Epoch: {47}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "92.04551006490499\n",
      "Image: 2/2\n",
      "95.74329494164493\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "86.51269109908701\n",
      "Image: 2/2\n",
      "89.05991163442587\n",
      "Epoch:    47   -   cost: 0.19   -   Accuracy: 181.68%\n",
      "Epoch: {48}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "90.61401878509207\n",
      "Image: 2/2\n",
      "94.60870040410305\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "89.45205491409018\n",
      "Image: 2/2\n",
      "89.22364927362959\n",
      "Epoch:    48   -   cost: 0.19   -   Accuracy: 181.95%\n",
      "Epoch: {49}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "92.51206481887499\n",
      "Image: 2/2\n",
      "94.26792607411903\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "88.0061091337715\n",
      "Image: 2/2\n",
      "85.15818531595005\n",
      "Epoch:    49   -   cost: 0.21   -   Accuracy: 179.97%\n",
      "Epoch: {50}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "91.99361815439671\n",
      "Image: 2/2\n",
      "93.68120562065576\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "89.87679355974515\n",
      "Image: 2/2\n",
      "89.24181676421406\n",
      "Epoch:    50   -   cost: 0.18   -   Accuracy: 182.40%\n",
      "Epoch: {51}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "92.52953121153075\n",
      "Image: 2/2\n",
      "94.15903860873983\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "90.01860268741105\n",
      "Image: 2/2\n",
      "90.48828772614075\n",
      "Epoch:    51   -   cost: 0.17   -   Accuracy: 183.60%\n",
      "Epoch: {52}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "91.26991593920259\n",
      "Image: 2/2\n",
      "93.76928558229832\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "89.01373794328796\n",
      "Image: 2/2\n",
      "89.90497779119859\n",
      "Epoch:    52   -   cost: 0.19   -   Accuracy: 181.98%\n",
      "Epoch: {53}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "92.49820794253353\n",
      "Image: 2/2\n",
      "85.64405426965168\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "87.7923543854766\n",
      "Image: 2/2\n",
      "89.29516918670953\n",
      "Epoch:    53   -   cost: 0.24   -   Accuracy: 177.61%\n",
      "Epoch: {54}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "87.87892070402062\n",
      "Image: 2/2\n",
      "88.05491908172478\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "88.13090949457067\n",
      "Image: 2/2\n",
      "90.52926390583809\n",
      "Epoch:    54   -   cost: 0.24   -   Accuracy: 177.30%\n",
      "Epoch: {55}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "90.76345179405895\n",
      "Image: 2/2\n",
      "91.08105491232344\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "87.20175734276319\n",
      "Image: 2/2\n",
      "90.86199877867026\n",
      "Epoch:    55   -   cost: 0.21   -   Accuracy: 179.95%\n",
      "Epoch: {56}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "90.39202604654237\n",
      "Image: 2/2\n",
      "92.1607815372476\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "87.06790958550795\n",
      "Image: 2/2\n",
      "89.48853016926235\n",
      "Epoch:    56   -   cost: 0.22   -   Accuracy: 179.55%\n",
      "Epoch: {57}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "89.90477772653857\n",
      "Image: 2/2\n",
      "86.96709170344475\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "83.84580764704488\n",
      "Image: 2/2\n",
      "76.88581006808994\n",
      "Epoch:    57   -   cost: 0.34   -   Accuracy: 168.80%\n",
      "Epoch: {58}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "83.40737451582113\n",
      "Image: 2/2\n",
      "78.52931365055717\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "77.00206152001991\n",
      "Image: 2/2\n",
      "67.20683453465219\n",
      "Epoch:    58   -   cost: 0.54   -   Accuracy: 153.07%\n",
      "Epoch: {59}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "84.50595722832482\n",
      "Image: 2/2\n",
      "84.65497529141682\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "71.253740338542\n",
      "Image: 2/2\n",
      "70.99267471736547\n",
      "Epoch:    59   -   cost: 0.51   -   Accuracy: 155.70%\n",
      "Epoch: {60}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "80.72724879135615\n",
      "Image: 2/2\n",
      "83.12759615859771\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "78.0042203047893\n",
      "Image: 2/2\n",
      "79.6276395258921\n",
      "Epoch:    60   -   cost: 0.44   -   Accuracy: 160.74%\n",
      "Epoch: {61}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "80.48520233959302\n",
      "Image: 2/2\n",
      "80.88859314692569\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "77.75259645241333\n",
      "Image: 2/2\n",
      "81.51269075645718\n",
      "Epoch:    61   -   cost: 0.44   -   Accuracy: 160.32%\n",
      "Epoch: {62}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "80.03623112510162\n",
      "Image: 2/2\n",
      "82.91294852211657\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "75.0819829434592\n",
      "Image: 2/2\n",
      "80.33242066977056\n",
      "Epoch:    62   -   cost: 0.46   -   Accuracy: 159.18%\n",
      "Epoch: {63}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "84.06645648184526\n",
      "Image: 2/2\n",
      "89.27827275295796\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "77.2083767802275\n",
      "Image: 2/2\n",
      "72.98578707425938\n",
      "Epoch:    63   -   cost: 0.43   -   Accuracy: 161.77%\n",
      "Epoch: {64}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "75.60186085854326\n",
      "Image: 2/2\n",
      "81.32650160974856\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "58.10180254508713\n",
      "Image: 2/2\n",
      "63.698997676977285\n",
      "Epoch:    64   -   cost: 0.74   -   Accuracy: 139.36%\n",
      "Epoch: {65}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "64.23182713309384\n",
      "Image: 2/2\n",
      "69.30025361771077\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "63.46767251395078\n",
      "Image: 2/2\n",
      "70.12982516986412\n",
      "Epoch:    65   -   cost: 0.81   -   Accuracy: 133.56%\n",
      "Epoch: {66}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "70.5552611449138\n",
      "Image: 2/2\n",
      "78.14787819098093\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "68.04093354191063\n",
      "Image: 2/2\n",
      "67.79751951823984\n",
      "Epoch:    66   -   cost: 0.68   -   Accuracy: 142.27%\n",
      "Epoch: {67}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "63.902448266108024\n",
      "Image: 2/2\n",
      "75.08739616816419\n",
      "Batch: 2\n",
      "Image: 1/2\n",
      "60.58597058204954\n",
      "Image: 2/2\n",
      "60.68126030838411\n",
      "Epoch:    67   -   cost: 0.87   -   Accuracy: 130.13%\n",
      "Epoch: {68}\n",
      "Batch: 1\n",
      "Image: 1/2\n",
      "63.68826088787611\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-f32539433e1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mparams_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m120\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.008\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-112-986c09d6526d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, Y, epochs, learning_rate, dropout, GN)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m                 \u001b[0mdconv1_2\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmaxpoolBackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdpl1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv1_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#Very important += merge with the gradients from concat backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGN\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-107-244185b13ab8>\u001b[0m in \u001b[0;36mmaxpoolBackward\u001b[1;34m(dpool, conv, f, s)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_h\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_w\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnanargmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Getting the indexes from the max value in this area\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m                 \u001b[1;31m#put it on the new array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[0mdout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdpool\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-107-244185b13ab8>\u001b[0m in \u001b[0;36mnanargmax\u001b[1;34m(arr)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnanargmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0midxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0midxs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munravel_index\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###### Training ######\n",
    "#train_images, train_labels, test_images, test_labels\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "params_values = train(train_images, train_labels, 120, 0.008, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Forward step . . .\n",
      "Cost: 0.08   -   Accuracy: 92.39%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = os.path.join(os.path.expanduser('~/'), 'data', 'salt')\n",
    "with open (path+'/weights', 'rb') as fp:\n",
    "    params = pickle.load(fp)  \n",
    "fp.close()\n",
    "Validate(train_images[0:1,:,:,:], train_labels[0:1,:,:,:], params, 1) #GN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Forward step . . .\n",
      "Cost: 0.02   -   Accuracy: 98.37%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = os.path.join(os.path.expanduser('~/'), 'data', 'salt')\n",
    "with open (path+'/weights', 'rb') as fp:\n",
    "    params = pickle.load(fp)  \n",
    "fp.close()\n",
    "Validate(train_images[1:2,:,:,:], train_labels[1:2,:,:,:], params, 1) #GN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Forward step . . .\n",
      "Cost: 0.06   -   Accuracy: 93.71%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = os.path.join(os.path.expanduser('~/'), 'data', 'salt')\n",
    "with open (path+'/weights', 'rb') as fp:\n",
    "    params = pickle.load(fp)  \n",
    "fp.close()\n",
    "Validate(train_images[2:3,:,:,:], train_labels[2:3,:,:,:], params, 1) #GN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate(test_images[1:2,:,:,:], test_labels[1:2,:,:,:], params_values);\n",
    "Validate(test_images[0:1,:,:,:], test_labels[0:1,:,:,:], params, 1) #GN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Forward step . . .\n",
      "Cost: 0.29   -   Accuracy: 75.01%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAATnElEQVR4nO3dfZBU1ZkG8OcRQxkTJcwmUBhZiRalm7J0tJBoueIAwUWWQsuSVYrF0VJHl08VdEAYlQXkw0EDCluicUGixI9EoMZoHAYBY6VUFE0kgLjiCjrL7BqppPwjRHn3j75c7u1MT/d034/uPs+viupz+nbf+9Ld79xz7rn3XJoZRKT6HZd2ACKSDCW7iCOU7CKOULKLOELJLuIIJbuII0pKdpIjSe4h+SHJmVEFJSLRY7Hj7CR7APgAwAgABwC8BWCcmf0huvBEJCrHl/DewQA+NLOPAIDkzwFcASBnspMs6i9Lnz59/HJHR0cxqxBxhpmxs+dLSfbvA9gfqB8A8KMS1pfTuHHj/PKyZcvi2IRI1Ssl2Tv76/E3e26SDQAaStiOiESglD77RQDuM7N/8uqzAMDMFnbxHn9j8+bNCy2bM2dOUXEUatOmTX55xIgRsW5LJE25mvGlHI1/C8BAkj8g2RPAtQA2lrA+EYlR0c14M/uK5GQAvwbQA8ATZrYzsshEJFKl9NlhZr8C8KuIYhGRGBXdZy9qY4E+e7lcR0922r0RqVhx9NlFpIIo2UUcUVKfvRpkdydWr17tl2+44YaEoxGJj/bsIo5Qsos4Qsku4ojUht6ylctQXJCG5crHlClT/PLDDz8cWtbY2OiXFy9enFhM5UpDbyKOU7KLOKJsht4ef/xxv3zTTTelGMkx2c3Fmpoavzx+/Pikw6kKDz30UKh+++23++XNmzeHlg0bNswvX3XVVX45+3s58cQTowyxamnPLuIIJbuII8rmaHxQ0hNbFENH6oH169f75S+++CK07Prrry9oHa+++qpfHjp0aGjZ7Nmz/fKCBQv8cvZnH/wN63vR0XgR5ynZRRyhZBdxRNkMvQU1NTWF6nH22VtbW0P14GSU27ZtCy0bMmSIX542bZpf7t+/f+h1M2bMiDLExAU/7+zjJ1HL7qcHffnllwWtY/78+VGFU9W0ZxdxhJJdxBFlOfTWlblz5/rle+65p9TVxaIShn+CQ151dXXpBdKFtrY2vzx8+HC/nP355nqdqzT0JuI4JbuII5TsIo6ouD570HPPPReqjx071i9n/7+C/bzgshUrVoReN2nSpChDxJNPPhmq19fXR7r+rmzdutUvB4cNK0Wu76wSjomkqeg+O8knSHaQfD/wXA3JVpJ7vcfeUQYrItErpBm/GsDIrOdmAmgzs4EA2ry6iJSxgprxJAcAaDGzs736HgB1ZtZOsh+ALWZ2ZgHrSazPkP3/Cg7Z3XvvvUmF8TeiboIuW7YsVJ86dWqk609T8EzE5uZmv6xmfNeiHnrra2bt3orbAfQpNjARSUbs58aTbADQEPd2RKRrVduMv/vuu0P1+++/3y8H/8+zZs0KvW7hwoXxBtaFYBM8eHHNSSedFHrdrbfemlhMcWtpafHLo0ePDi1Tc704UTfjNwI4OoZUD2BDkesRkYQUMvS2DsBvAZxJ8gDJGwEsAjCC5F4AI7y6iJSxvH12MxuXY5GuOBCpIGU5eUUUgn30rvTq1SvmSAq3fPnytENI3O7du/1ydp9doqVz40UcoWQXcURFXwgTh+DnkX0Ry3XXXZd0OFXh+eef98tXX311aJmG16KnyStEHKdkF3GEkl3EEeqzd0OSn1U1CfbLV65cGVo2ceLEpMOpeuqzizhOyS7iCDXjixScTGH69OkpRlIess/+GzhwoF8eNWpU0uE4Tc14Eccp2UUcoWZ8BHSUXmfClRM140Ucp2QXcYSSXcQRVTt5hUQveGtkQLdHrjTas4s4Qsku4gg146Vgra2tofq+fftSikSKoT27iCOU7CKOULKLOEKny0bAldNldUpsZSj6dFmS/Um+SnIXyZ0kp3nP15BsJbnXe+wdddAiEp28e3bvLq39zOwdkicBeBvAlQCuB/BHM1tEciaA3mbWmGddVbkL1J5dykmuPXu3m/EkNwB4xPvXrds2V1OyV3OCr1271i9PmDDBLyvZu7Z161a/fOmll4aWPfLII3558uTJscYRyVVv3n3azwPwBoC+ZtburbwdQJ/SQhSROBV8Ug3JbwP4BYDbzOxPhf6VJ9kAoKG48EQkKgXt2Ul+A5lEf8rMfuk9fdBrvh/t13d09l4zW2Vmg8xsUBQBi0hx8u7ZmdmF/xTALjN7MLBoI4B6AIu8xw2xRFhGnn766bRDSETwnnZvvvlmipFUlo6OTvd3AOLvpxeikGb8xQAmAPg9yXe95+5GJsmfJXkjgE8AjI0nRBGJQt5kN7PfAMjVQdcFzSIVQmfQdUM1Dbc99thjfvnmm28OLdMQW3GCv4/sz7CrYbkY4tCEkyIuU7KLOEKTV3Sh0pvtDz54bPDkjjvuCC1raDh26sPhw4cTi8lVO3fuTDsE7dlFXKFkF3GEkl3EERp6y1Jp/fSVK1eG6hMnTvTLweGf7P9XcNmKFStCyyZNmhRliFUl+3Pctm2bXx4yZIhfXr9+feh1V155pV+Oe2hTQ28ijlOyizhCQ29l5LXXXvPLl1xyiV/ObvYFm5LZTe6zzjqr03UvXbo053bVbC9esOketGvXrlD9gw8+SCKcLmnPLuIIJbuII5TsIo5wvs9eTkNtwf5fU1NTztc1NzfnXJZrAoWePXsWH5h02/nnnx+q79+/P6VIjtGeXcQRSnYRRzh5Bl2aTffglWjTp09PLQ4p3PLly/3ylClTCnrP3LlzQ/X77rsvypC6pDPoRBynZBdxhDNH4zdt2pR2CADUdK8EUXTzjhw5EkEk0dKeXcQRSnYRRyjZRRzhzNBbkv/PBQsW+OU5c+Yktl2JRhS/lTTn3i966I3kCSTfJPkeyZ0k53rP15BsJbnXe+wdddAiEp1CmvF/ATDMzM4FUAtgJMkLAcwE0GZmAwG0eXURKVPdasaTPBHAbwD8G4AnAdSZWbt3y+YtZnZmnvcn1pZO8yw53T6psjnbjAcAkj28O7h2AGg1szcA9DWzdm/l7QD6RBWsiESvoGQ3s6/NrBbAqQAGkzy70A2QbCC5neT2YoMUkdJ1a+jNzA4B2AJgJICDXvMd3mOnF1Kb2SozG2Rmg0qMVURKkPd0WZLfA/BXMztE8psAfgxgMYCNAOoBLPIeN8QZaDlTH73ybd68Oe0QYlfIufH9AKwh2QOZlsCzZtZC8rcAniV5I4BPAIyNMU4RKVHeZDez3wE4r5PnPwcwPI6gRCR6zlz1FjU13bu2bNkyvzxt2jS/nH3r6OBkHkmKY2i2paUl8nVGSefGizhCyS7iCDXjJRbHHdf5fuSCCy5IOJJ4NTY2+uUlS5akGEl+2rOLOELJLuIIJbuII9Rn74aFCxemHUJZCQ5fZQ9FTp482S8Hbz117bXXhl63b98+vzxr1qzQsiiGN+O++rHc++lB2rOLOELJLuKIqmrG33nnnZGuT2fJReOUU07JuazQobiVK1f65YkTJ4aWTZ061S8Hb9UkYdqzizhCyS7iCCW7iCMqet742bNnh+qDBw/2y2PGjCl5/eqzdy3423nmmWdCy6655prYtrt27dpQfcKECbFtK59y/I3ols0ijlOyiziioofeevToEarv3r3bL0fRjJeuzZ8/3y83NTWFlsXZjO/fv39s6+5MOTbVi6E9u4gjlOwijqjoZvwZZ5wRqre3t3d7HYsWLQrVzznnnJJickl20z0pdXV1qWy30mnPLuIIJbuII5TsIo6o6DPoZs4M3xI+2P/O/n81Nzf75RkzZgRjijIk8QQ//23btvnlIUOGpBFOXsHfwcaNG0PLKm0Yt+Qz6LzbNu8g2eLVa0i2ktzrPfaOKlgRiV53mvHTAOwK1GcCaDOzgQDavLqIlKmCht5IngrgnwEsAHD0/j1XAKjzymuQuZVzY/Z743TaaacV/NrgxBbBZrzEIzifXPB7SrMZ/9RTT/nl8ePH53zdli1bEogmeYXu2X8C4C4ARwLP9TWzdgDwHvtEHJuIRChvspMcDaDDzN4uZgMkG0huJ7m9mPeLSDQKacZfDGAMyVEATgBwMsmfAThIsp+ZtZPsB6Cjszeb2SoAq4Doj8aLSOG6NfRGsg7ADDMbTfIBAJ+b2SKSMwHUmNlded6fWLK//PLLofrIkSP98vTp0/3y0qVLkwpJEM087tm3eQ7eBjp7KDXX3PZr1qwJva6+vr7kuMpFHJNXLAIwguReACO8uoiUqW5dCGNmW5A56g4z+xzA8OhDEpE4VPRVb105cOBAzmVquicr6ttm9erVq+R17NixI4JIKovOjRdxhJJdxBEVfSGMlK8ofle5LqDJPuL+0ksv+eXLL7+85O1WOk0lLeI4JbuII5TsIo6o2qE3iV/cx3sKvULu8OHDscZRLbRnF3GEkl3EERp6ky4l+fvIFhxie+WVV/zyZZddlkY4FUNDbyKOU7KLOELJLuIIDb05YuvWraH6O++845dvu+22pMPptk8//TTtECqe9uwijlCyizhCQ29VJvh9PvDAA345OG9+Odm8ebNfHjZsWGiZbs1VHA29iThOyS7iCB2Nr0DBI+svvvhizteVa9M96PXXX/fLR44c6eKVUirt2UUcoWQXcYSSXcQRGnqrQMHvbN26daFl48aNSzqckmh4LXq5ht4KvT/7xwD+DOBrAF+Z2SCSNQCeATAAwMcA/sXMvogiWBGJXnea8UPNrNbMBnn1mQDazGwggDavLiJlqpShtysA1HnlNcjcA66xxHjEs2TJEr981125b45bCc32YFP90UcfTTEStxW6ZzcAr5B8m2SD91xfM2sHAO+xTxwBikg0Ct2zX2xmn5HsA6CV5O5CN+D9cWjI+0IRiVVBe3Yz+8x77ADwAoDBAA6S7AcA3mNHjveuMrNBgb6+iKQg756d5LcAHGdmf/bKlwH4dwAbAdQDWOQ9bogzUNccOnQo7RC6JXsIbfHixZ2+7pZbbkkiHOlEIc34vgBe8L7M4wE8bWYvk3wLwLMkbwTwCYCx8YUpIqXKm+xm9hGAczt5/nMAw+MISkSip6veytSCBQv8cm1tbYqRHJPdVJ8zZ07O1zY2ahS23OjceBFHKNlFHKFkF3GErnorU8HvJbuvHMV3tnr1ar88YMCA0LKhQ4eWvH5JjyacFHGckl3EERp6q3Br164N1SdMmOCXg83/efPmhV7X1NQUb2BSdrRnF3GEkl3EEToaX4FmzJjhl5ubm1OMRMqRjsaLOE7JLuIIJbuII9RnF6ky6rOLOE7JLuIIJbuII5TsIo5Qsos4Qsku4gglu4gjlOwijlCyizhCyS7iiIKSneR3SD5PcjfJXSQvIllDspXkXu+xd9zBikjxCt2zLwPwspmdhcytoHYBmAmgzcwGAmjz6iJSpvJeCEPyZADvATjdAi8muQdAnZm1e7ds3mJmZ+ZZly6EEYlZKRfCnA7gfwH8J8kdJB/3bt3c18zavZW3A+gTWbQiErlCkv14AOcD+A8zOw/Al+hGk51kA8ntJLcXGaOIRKCQZD8A4ICZveHVn0cm+Q96zXd4jx2dvdnMVpnZIDMbFEXAIlKcvMluZv8DYD/Jo/3x4QD+AGAjgHrvuXoAG2KJUEQiUdBMNSRrATwOoCeAjwDcgMwfimcB/D2ATwCMNbM/5lmPDtCJxCzXATpNSyVSZTQtlYjjlOwijlCyizhCyS7iCCW7iCOU7CKOULKLOOL4hLf3fwD+G8B3vXLaFEeY4ggrhzi6G8NpuRYkelKNv1FyezmcK684FEe5xxFlDGrGizhCyS7iiLSSfVVK282mOMIUR1g5xBFZDKn02UUkeWrGizgi0WQnOZLkHpIfkkxsNlqST5DsIPl+4LnEp8Im2Z/kq9503DtJTksjFpInkHyT5HteHHPTiCMQTw9vfsOWtOIg+THJ35N89+gUainFEdu07YklO8keAFYAuBzADwGMI/nDhDa/GsDIrOfSmAr7KwDTzewfAFwIYJL3GSQdy18ADDOzcwHUAhhJ8sIU4jhqGjLTkx+VVhxDzaw2MNSVRhzxTdtuZon8A3ARgF8H6rMAzEpw+wMAvB+o7wHQzyv3A7AnqVgCMWwAMCLNWACcCOAdAD9KIw4Ap3o/4GEAWtL6bgB8DOC7Wc8lGgeAkwHsg3csLeo4kmzGfx/A/kD9gPdcWlKdCpvkAADnAXgjjVi8pvO7yEwU2mqZCUXT+Ex+AuAuAEcCz6URhwF4heTbJBtSiiPWaduTTPbOpspxciiA5LcB/ALAbWb2pzRiMLOvzawWmT3rYJJnJx0DydEAOszs7aS33YmLzex8ZLqZk0gOSSGGkqZtzyfJZD8AoH+gfiqAzxLcfraCpsKOGslvIJPoT5nZL9OMBQDM7BCALcgc00g6josBjCH5MYCfAxhG8mcpxAEz+8x77ADwAoDBKcRR0rTt+SSZ7G8BGEjyByR7ArgWmemo05L4VNgkCeCnAHaZ2YNpxULyeyS/45W/CeDHAHYnHYeZzTKzU81sADK/h81m9q9Jx0HyWyRPOloGcBmA95OOw+Ketj3uAx9ZBxpGAfgAwH8BmJ3gdtcBaAfwV2T+et4I4O+QOTC013usSSCOf0Sm6/I7AO96/0YlHQuAcwDs8OJ4H8A93vOJfyaBmOpw7ABd0p/H6cjcz/A9ADuP/jZT+o3UAtjufTfrAfSOKg6dQSfiCJ1BJ+IIJbuII5TsIo5Qsos4Qsku4gglu4gjlOwijlCyizji/wEvqiQad55wPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Validate(test_images[1:2,:,:,:], test_labels[1:2,:,:,:], params, 1) #GN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
