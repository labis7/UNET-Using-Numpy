{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "#import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "from matplotlib import image\n",
    "\n",
    "def mnist(path=None):\n",
    "    r\"\"\"Return (train_images, train_labels, test_images, test_labels).\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory containing MNIST. Default is\n",
    "            /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist.\n",
    "            Create if nonexistant. Download any missing files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, train_labels, test_images, test_labels), each\n",
    "            a matrix. Rows are examples. Columns of images are pixel values.\n",
    "            Columns of labels are a onehot encoding of the correct class.\n",
    "\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz',\n",
    "             'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz',\n",
    "             't10k-labels-idx1-ubyte.gz']\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        # Set path to /home/USER/data/mnist or C:\\Users\\USER\\data\\salt\n",
    "        path = os.path.join(os.path.expanduser('~/'), 'data', 'salt2')\n",
    "\n",
    "    # Create path if it doesn't exist\n",
    "    #os.makedirs(path, exist_ok=True)\n",
    "    \"\"\"\"\n",
    "    # Download any missing files\n",
    "    for file in files:\n",
    "        if file not in os.listdir(path):\n",
    "            urlretrieve(url + file, os.path.join(path, file))\n",
    "            print(\"Downloaded %s to %s\" % (file, path))\n",
    "    print(\"All files are ready to gzip!\")\n",
    "    \"\"\"\n",
    "    dim=64\n",
    "    def _images(path,dim):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        folder = path + \"/images/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(dim, dim)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,dim,dim).astype('float32')#/255\n",
    "        return pixels[:2,:,:,:]\n",
    "\n",
    "    def _labels(path,dim):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        \n",
    "        folder = path + \"/labels/\"\n",
    "        #onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(dim, dim)) for f in os.listdir(folder)]\n",
    "        onlyfiles = [cv2.resize(image.imread(folder+f),(dim, dim)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,dim,dim).astype('float32') #/255\n",
    "        return pixels[:2,:,:,:]\n",
    "    \n",
    "    def _t_images(path,dim):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        folder = path + \"/t_images/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(dim, dim)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,dim,dim).astype('float32')#/255\n",
    "        return pixels[0:2,:,:,:]\n",
    "    def _t_labels(path,dim):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        \n",
    "        folder = path + \"/t_labels/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(dim, dim)) for f in os.listdir(folder)]\n",
    "        #onlyfiles = [cv2.resize(image.imread(folder+f),(32, 32)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,dim,dim).astype('float32') #/255\n",
    "        return pixels[0:2,:,:,:]\n",
    "\n",
    "    print(\"Train Images : Loading . . .\")\n",
    "    train_images = _images(path,dim)\n",
    "    print(\"Train Labels : Loading . . .\")\n",
    "    train_labels = _labels(path,dim)\n",
    "    print(\"Test Images  : Loading . . .\")\n",
    "    #test_images = _t_images(path,dim)\n",
    "    print(\"Test Labels  : Loading . . .\")\n",
    "    #test_labels = _t_labels(path,dim)\n",
    "    print(\"Done!\")\n",
    "    return train_images, train_labels , test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images : Loading . . .\n",
      "Train Labels : Loading . . .\n",
      "Test Images  : Loading . . .\n",
      "Test Labels  : Loading . . .\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, test_images, test_labels= mnist()  #, test_images, test_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 64, 64)\n",
      "(1, 1, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "ch = 1 #Number of channels\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape) # ONE-HOT !!!\n",
    "#print(train_images.T.shape)\n",
    "#print(train_labels.reshape((train_images.shape[0], 10)).shape)\n",
    "\n",
    "##### Getting Ready for the Conv Layers #####\n",
    "#train_images = train_images.reshape(train_images.shape[0], ch, 28, 28).squeeze()\n",
    "#test_images = test_images.reshape(test_images.shape[0], ch, 28, 28).squeeze()\n",
    "\n",
    "#############################################\n",
    "\n",
    "#y = train_images[1].reshape(1,28,28)\n",
    "#print(y.squeeze().shape)\n",
    "'''\n",
    "temp = []\n",
    "#train_labels = train_labels.sum(1)\n",
    "for i in range(int(len(train_labels[:]))):\n",
    "    temp.append(list(train_labels[i][:]).index(1))\n",
    "    \n",
    "    \n",
    "train_labels = np.array(temp.copy())\n",
    "#print(train_labels[0:5])\n",
    "'''\n",
    "#plt.imshow(train_images[0].squeeze(), cmap='Greys_r');\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[0].squeeze(), cmap='Greys_r');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_images[0].squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Filter/Parameter Initializattions  ##################\n",
    "\n",
    "def init_filters(layers, init_n_f ,trim):\n",
    "    filters = []\n",
    "    bias = []\n",
    "    f_dc = []\n",
    "    \n",
    "    trimf = trim\n",
    "    trimb = trim*5\n",
    "    n_f = init_n_f #initial number of filters/kernels\n",
    "    ch_in = 1      #input channels\n",
    "    \"\"\"\n",
    "    for i in range(layers):\n",
    "        if(i != 0):\n",
    "            n_f = n_f*2 #16,32,64,128,256\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trimb\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "    \n",
    "    #Deconvolution filters    \n",
    "    for i in range(1,layers):\n",
    "        n_f = n_f//2 #128,64,32,16\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        fdc = np.random.randn(n_f,ch_in,2,2)*trimf #upsampling filter, its result will be conc with conv4 output so the channels will be doubled again\n",
    "        bdc = np.random.randn(fdc.shape[0],1)* trimb\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trimb\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        dc = [fdc, bdc]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "        f_dc.append(dc)\n",
    "        \n",
    "    \"\"\"\n",
    "    trimbr = trim\n",
    "    locbr = 0\n",
    "    for i in range(layers):\n",
    "        if(i != 0):\n",
    "            n_f = n_f*2 #16,32,64,128,256\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.normal(loc = locbr, scale = trimbr , size = f1) #np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.normal(loc = locbr, scale = trimbr*5 , size = (f1.shape[0],1)) #np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.normal(loc = locbr, scale = trimbr , size = f2)\n",
    "        b2 = np.random.normal(loc = locbr, scale = trimbr*5 , size = (f2.shape[0],1))\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "    \n",
    "    #Deconvolution filters    \n",
    "    for i in range(1,layers):\n",
    "        n_f = n_f//2 #128,64,32,16\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        fdc =  np.random.normal(loc = locbr, scale = trimbr , size = (n_f,ch_in,2,2))#upsampling filter, its result will be conc with conv4 output so the channels will be doubled again\n",
    "        bdc = np.random.normal(loc = locbr, scale = trimbr , size = (fdc.shape[0],1))\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.normal(loc = locbr, scale = trimbr , size = (n_f, ch_in, 3, 3))\n",
    "        b1 = np.random.normal(loc = locbr, scale = trimbr*5 , size = (f1.shape[0],1))\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.normal(loc = locbr, scale = trimbr , size = f2)\n",
    "        b2 = np.random.normal(loc = locbr, scale = trimbr*5 , size = (f2.shape[0],1))\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        dc = [fdc, bdc]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "        f_dc.append(dc)\n",
    "    return filters, bias, f_dc   \n",
    "\n",
    "\n",
    "def init_groupnorm_params(bias, out_b, norm_batch, locbr, trimbr):\n",
    "    [b1,b2,b3,b4,b5,b6,b7,b8,b9]= bias \n",
    "    \n",
    "    \n",
    "    t_1,_ = b1\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma1_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) #MAKE IT FLOAT\n",
    "    beta1_1  = np.random.normal( scale = trimbr , size = gb_size)\n",
    "    gamma1_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta1_2  = np.random.normal( scale = trimbr , size = gb_size)\n",
    "    tempg_1 = [gamma1_1,gamma1_2]\n",
    "    tempb_1 = [beta1_1,beta1_2]\n",
    "    \n",
    "    t_1,_ = b2\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma2_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta2_1  = np.random.normal( scale = trimbr , size = gb_size)\n",
    "    gamma2_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta2_2  = np.random.normal(scale = trimbr , size = gb_size)  \n",
    "    tempg_2 = [gamma2_1,gamma2_2]\n",
    "    tempb_2 = [beta2_1,beta2_2]\n",
    "    \n",
    "    t_1,_ = b3\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma3_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta3_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma3_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta3_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_3 = [gamma3_1,gamma3_2]\n",
    "    tempb_3 = [beta3_1,beta3_2]\n",
    "    \n",
    "    t_1,_ = b4\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma4_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta4_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma4_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta4_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_4 = [gamma4_1,gamma4_2]\n",
    "    tempb_4 = [beta4_1,beta4_2]\n",
    "    \n",
    "    t_1,_ = b5\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma5_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta5_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma5_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta5_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_5 = [gamma5_1,gamma5_2]\n",
    "    tempb_5 = [beta5_1,beta5_2]\n",
    "    \n",
    "    t_1,_ = b6\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma6_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta6_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma6_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta6_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_6 = [gamma6_1,gamma6_2]\n",
    "    tempb_6 = [beta6_1,beta6_2]\n",
    "    \n",
    "    t_1,_ = b7\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma7_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta7_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma7_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta7_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_7 = [gamma7_1,gamma7_2]\n",
    "    tempb_7 = [beta7_1,beta7_2]\n",
    "    \n",
    "    t_1,_ = b8\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma8_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta8_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma8_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta8_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_8 = [gamma8_1,gamma8_2]\n",
    "    tempb_8 = [beta8_1,beta8_2]\n",
    "    \n",
    "    t_1,_ = b9\n",
    "    gb_size =(t_1.shape[0]//norm_batch,1)\n",
    "    gamma9_1 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta9_1  = np.random.normal(scale = trimbr , size = gb_size)\n",
    "    gamma9_2 = np.random.normal(loc = locbr, scale = trimbr , size = gb_size) \n",
    "    beta9_2  = np.random.normal(scale = trimbr ,size =  gb_size)  \n",
    "    tempg_9 = [gamma9_1,gamma9_2]\n",
    "    tempb_9 = [beta9_1,beta9_2]\n",
    "    \n",
    "    ga =[tempg_1,tempg_2,tempg_3,tempg_4,tempg_5,tempg_6, tempg_7,tempg_8,tempg_9]\n",
    "    be =[tempb_1,tempb_2,tempb_3,tempb_4,tempb_5,tempb_6, tempb_7,tempb_8,tempb_9]\n",
    "    \n",
    "    gamma_out = np.random.normal(loc = locbr, scale = trimbr , size = (out_b.shape[0]//norm_batch,1))\n",
    "    beta_out =   np.random.normal(scale = trimbr , size = (out_b.shape[0]//norm_batch,1))\n",
    "    \n",
    "    return ga, be , gamma_out, beta_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation Functions ###\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UTILITIES #############\n",
    "\n",
    "def conv(image, params, s = 1, pad = 1 ): # s = 1 (conv stride)\n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    [f, b] = params\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((image.shape[0],image.shape[1]+2*pad ,image.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = image\n",
    "        image = tmp    \n",
    "        \n",
    "    f_num = f.shape[0]\n",
    "    f_depth = f.shape[1] #f1 = 1 , f2 = 8\n",
    "    f_size = f.shape[2] \n",
    "        \n",
    "\n",
    "    h_range = int((image.shape[1] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    np_o = np.zeros((f_num, h_range, w_range))\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                np_o[z, _h, _w] = np.sum(np.multiply(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] , f[z, :, :, :])) + b[z]\n",
    "    \n",
    "    return np_o\n",
    "\n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s = 1, pad = 1 ):\n",
    "    #filt =np.rot90(filt, 2)  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!! A T T E N T I O N !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    f_num, n_c, f, _ = filt.shape\n",
    "    \n",
    "    _ ,h , w = dconv_prev.shape\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((conv_in.shape[0],conv_in.shape[1]+2*pad ,conv_in.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = conv_in\n",
    "        conv_in = tmp\n",
    "    \n",
    "    dconv_in = np.zeros(conv_in.shape)\n",
    "    db = np.zeros((f_num,1))\n",
    "\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                #each entry of the dconv_prev will try to affect the idxs from which was made of.\n",
    "                dfilt[z] += dconv_prev[z, _h, _w] * conv_in[:, _h*s : _h*s + f, _w*s : _w*s + f]\n",
    "                dconv_in[:, _h*s : _h*s + f, _w*s : _w*s + f] += dconv_prev[z, _h, _w] * filt[z]  \n",
    "        db[z] = np.sum(dconv_prev[z])  #, axis =1) ## AXIS?\n",
    "    \n",
    "    if(pad >=1 ):\n",
    "        dconv_in = dconv_in[:, pad:-pad, pad:-pad]  # Cropping\n",
    "        \n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "def convTransp1(image, params, s = 2, pad = 1):\n",
    "    [f, b] = params\n",
    "    n_f, n_c, f_s, _ = f.shape\n",
    "    input_s = image.shape[1]\n",
    "    target_dim = input_s*2\n",
    "    res = np.zeros((n_f, target_dim, target_dim))\n",
    "    temp =np.zeros((n_c, target_dim, target_dim))\n",
    "    for z in range(n_f):\n",
    "        for _h in range(input_s):      \n",
    "            for _w in range(input_s):\n",
    "                temp[:, _h*s:_h*s+f_s, _w*s:_w*s+f_s] += image[:, _h, _w].reshape(n_c,1,1)*f[z,:,:,:] #bias will be added at the end\n",
    "        res[z] = np.sum(temp , axis = 0) + b[z]\n",
    "    return res, image\n",
    "\n",
    "def convTranspBackward1(dconv_prev, new_in, filt, s = 2):\n",
    "    n_f, n_c, f_s, _ = filt.shape\n",
    "    _, input_s, _ = new_in.shape\n",
    "    #final_dim = (new_in.shape[1] - 2)//2 + 1 \n",
    "    dc_s=dconv_prev.shape[1]\n",
    "    temp = np.zeros((n_c,dc_s,dc_s))\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dconv_in = np.zeros(new_in.shape)\n",
    "    db = np.zeros((n_f,1))\n",
    "    for z in range(n_f):\n",
    "        for _h in range(input_s):      \n",
    "            for _w in range(input_s): \n",
    "                dfilt[z] += dconv_prev[z, _h*s:_h*s+f_s, _w*s:_w*s+f_s]*new_in[:,_h,_w].reshape(n_c,1,1)\n",
    "                temp[:, _h*s:_h*s+f_s, _w*s:_w*s+f_s] += dconv_prev[z, _h*s:_h*s+f_s, _w*s:_w*s+f_s] * filt[z]\n",
    "                for ch in range(n_c):\n",
    "                    dconv_in[ch, _h, _w] += np.sum(temp[ch, _h*s:_h*s+f_s, _w*s:_w*s+f_s])\n",
    "        db[z] = np.sum(dconv_prev[z])        \n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "    \n",
    "def convTransp(image, params, s = 1, pad = 1):\n",
    "    #s is always 1, upsample kernel = 2\n",
    "    #zero insertions between pixels s_downsampled -1 = 2-1 = 1\n",
    "    #required padding in order to double my dimensions with the given data:\n",
    "    #(i-1)*2 + k -2p = output size, where our padding is k - p -1 = 2-0-1=1(we assume p=0)\n",
    "    [f, b]=params\n",
    "    f = np.rot90(f, 1, (2,3))\n",
    "    params = [f, b]\n",
    "    input_s = image.shape[1]\n",
    "    target_dim = input_s*2 #final dim, after conv\n",
    "    required_pad = 1 #always for filter 2x2 ,stride 1, zero insertion 1 and main target to double dim\n",
    "    #make our new custom input\n",
    "    size = input_s*2 +1\n",
    "    new_in = np.zeros((image.shape[0], size, size))\n",
    "    \n",
    "    ### OR just: np.pad(image[:,:,:],2,'constant') # Important, we must loop with respect to the 1st dim\n",
    "    for i in range(1, target_dim, 2):\n",
    "        for j in range(1, target_dim, 2):\n",
    "                new_in[:, i, j] = image[:, i//2, j//2]\n",
    "    #now we do a normal convolution(pad = 0)\n",
    "    res = conv(new_in, params, 1, 0) #thats the final result with target_dim\n",
    "    return res, new_in # we will need new_in so we wont calc it again for the backprop\n",
    "    \n",
    "def convTranspBackward(dconv_prev, new_in, filt, s = 1):\n",
    "    #First, we do a backward convolution on new_in,d_conv_prev,\n",
    "    #then we will choose form the d_conv_new the entries that match the initial 'smaller' input image\n",
    "    #by selecting the odd matrix cells 1,3,5... because we had applied a standard pad=1,zero inser=1\n",
    "    dconv_in, dfilt, db = convolutionBackward(dconv_prev, new_in, filt,1,0)\n",
    "    #Now its time to choose the right entries to build the gradients of the initial input image\n",
    "    dim = dconv_in.shape[1]\n",
    "    final_dim = (new_in.shape[1] - 2)//2 + 1 #based on dimen of image before final conv that gives the result,..\n",
    "    #e.g. for new_in 7x7 that is going to convoluted with a 2x2 kernel and give a 6x6 upsampled from 3x3 init image\n",
    "    # now from this 7x7 --> apply the formula above,we get the 3x3 dimension number\n",
    "    res = np.zeros((dconv_in.shape[0], final_dim, final_dim))\n",
    "    for i in range(1, dim, 2):\n",
    "        for j in range(1, dim, 2):\n",
    "                res[:, i//2, j//2] = dconv_in[:, i, j]\n",
    "    return res, dfilt, db\n",
    "    \n",
    "    \n",
    "def maxpool(image, f=2 , s=2):\n",
    "    \n",
    "    h_range = int((image.shape[1] - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f) / s) + 1     # (W - F + 2P) / S \n",
    "    out = np.zeros((image.shape[0], h_range, w_range))\n",
    "    \n",
    "    for z in range(image.shape[0]): # Number of channels\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                out[z, _h, _w] = np.max(image[z, _h*s : _h*s + f, _w*s : _w*s + f])\n",
    "    return out\n",
    "\n",
    "def nanargmax(arr):\n",
    "    #print(arr.shape)\n",
    "    try:\n",
    "        idx = np.nanargmax(arr)\n",
    "        #print (idx)\n",
    "    except:\n",
    "        idx = 0\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "def maxpoolBackward(dpool, conv, f=2 , s=2):\n",
    "    num_c, h, w = conv.shape\n",
    "    h = int((h - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w = int((w - f) / s) + 1     # (W - F + 2P) / S \n",
    "    \n",
    "    dout = np.zeros(conv.shape)\n",
    "    #print(conv.shape)\n",
    "    for z in range(num_c): # Number of channels\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                (a, b) = nanargmax(conv[z, _h*s : _h*s + f, _w*s : _w*s + f]) #Getting the indexes from the max value in this area\n",
    "                #put it on the new array\n",
    "                dout[z, _h + a, _w + b] = dpool[z, _h, _w]\n",
    "    \n",
    "    \n",
    "    return dout\n",
    "\n",
    "\n",
    "def reshape(img, reshape_dim):\n",
    "    pad = reshape_dim - img.shape[1]\n",
    "    if(pad == 0):\n",
    "        return img\n",
    "    res = np.zeros((img.shape[0], reshape_dim, reshape_dim))\n",
    "    if(pad > 1):\n",
    "        res[:, pad//2:-(pad//2), pad//2:-(pad//2)] = img\n",
    "    else:\n",
    "        res[:, 0:-(pad), 0:-(pad)] = img\n",
    "    return res\n",
    "\n",
    "\n",
    "def crop(img, crop_dim): #TODO : look at it..future upgrade to downsample..maybe\n",
    "    start = img.shape[1]//2 - crop_dim//2\n",
    "    return img[:,(start):(start +crop_dim),(start):(start +crop_dim)]\n",
    "\n",
    "    \n",
    "    \n",
    "def crop2half(img):\n",
    "    #return gradients for decoder side and gradients for encoder side\n",
    "    n_ch = img.shape[0]//2\n",
    "    return img[n_ch:n_ch*2 ,:,:], img[0:n_ch,:,:]\n",
    "    \n",
    "def concat(img1_true, img2):\n",
    "    n_ch = img1_true.shape[0]\n",
    "    dim = img1_true.shape[1]\n",
    "    if(img2.shape[1] != dim):\n",
    "        img2 = crop(img2, dim)\n",
    "        print(\"Warning: Extra crop needed and handled!(%d --> %d)\" %(dim, img2.shape[1]))\n",
    "    res = np.zeros((n_ch*2, dim, dim))\n",
    "    res[0:n_ch,:,:] = img2\n",
    "    res[n_ch:n_ch*2 ,:,:] = img1_true\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cost Calculations ######\n",
    "def Cross_Entropy(logs, targets):  # Pixel-Wise Cross entropy --> average accuracy\n",
    "    out = np.zeros(logs.shape)\n",
    "    mylen = logs.shape[0]*logs.shape[1]*logs.shape[2]\n",
    "    #print(len(targets))\n",
    "    #logs.shape (dim x dim) like target\n",
    "    for i in range(logs.shape[1]):\n",
    "        for j in range(logs.shape[2]):\n",
    "            if(targets[:,i,j] >= logs[:,i,j]):#Gray and above\n",
    "                out[:,i,j] = logs[:,i,j]/targets[:,i,j] \n",
    "            else:\n",
    "                out[:,i,j] = (1 - logs[:,i,j])/(1 - targets[:,i,j]) # e.g if logs[i,j]= 0.4 and we want(target) 0 --> 1-0.4=0.6 prob. for zero \n",
    "    #or\n",
    "    #out = targets*logs + ((-1*(targets-1)) - (-1*(targets-1))*logs)\n",
    "    res =out.sum()/mylen\n",
    "    return -np.log(res),res\n",
    "\n",
    "\n",
    "def Dice_Coef(logs, targets):\n",
    "    out = np.zeros(logs.shape)\n",
    "    mylen = logs.shape[0]*logs.shape[1]*logs.shape[2]\n",
    "    #Apply Dice coefficient\n",
    "    numerator = (logs*targets)\n",
    "    denominator = logs + targets\n",
    "    loss = 1 - (2*np.sum(numerator))/(np.sum(denominator))\n",
    "    return loss, np.exp(-loss)\n",
    "                \n",
    "    \n",
    "    \n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    \n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost) ### ATTENTION!\n",
    "\n",
    "def normalize(output):\n",
    "    output[output<-4]=-4\n",
    "    output[output>4] = 4\n",
    "    return output\n",
    "\n",
    "###### Accuracy Calculation ######\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupnorm_forward(X, gamma_, beta_, eps=1e-5):\n",
    "    \"\"\"\n",
    "    # extract the dimensions\n",
    "    C, H, W = X.shape\n",
    "    # mini-batch mean\n",
    "    mean = nd.mean(X, axis=(1,2))\n",
    "    # mini-batch variance\n",
    "    variance = nd.mean((X - mean.reshape((C, 1, 1))) ** 2, axis=(1, 2))\n",
    "    # normalize\n",
    "    X_hat = (X - mean.reshape((C, 1, 1))) * 1.0 / nd.sqrt(variance.reshape((C, 1, 1)) + eps)\n",
    "    #if is_training:\n",
    "    # while training, we normalize the data using its mean and variance\n",
    "    X_hat = (X - mean.reshape((C, 1, 1))) * 1.0 / nd.sqrt(variance.reshape((C, 1, 1)) + eps)\n",
    "    #else:\n",
    "    # while testing, we normalize the data using the pre-computed mean and variance\n",
    "    #    X_hat = (X - _BN_MOVING_MEANS[scope_name].reshape((1, C, 1, 1))) * 1.0 \\\n",
    "    #        / nd.sqrt(_BN_MOVING_VARS[scope_name].reshape((1, C, 1, 1)) + eps)\n",
    "    # scale and shift\n",
    "    out = gamma.reshape((C, 1, 1)) * X_hat + beta.reshape((C, 1, 1))\n",
    "    \"\"\"\n",
    "    C_all=X.shape[0]\n",
    "    \n",
    "    if(C_all == 1):\n",
    "        batch = 1\n",
    "    else:\n",
    "        batch =2\n",
    "    C= batch\n",
    "    \n",
    "    mu_= np.zeros(C_all//batch)\n",
    "    var_=np.zeros(C_all//batch)\n",
    "    xmu_=np.zeros((C_all,X.shape[1],X.shape[2]))\n",
    "    sqrtvar_= np.zeros(C_all//batch)\n",
    "    ivar_= np.zeros(C_all//batch)\n",
    "    xhat_= np.zeros((C_all,X.shape[1],X.shape[2]))\n",
    "    #gammax_= np.zeros((C_all,1,1))\n",
    "    out_= np.zeros((C_all,X.shape[1],X.shape[2]))\n",
    "    \n",
    "    \n",
    "    for i in range(0, C_all, batch):\n",
    "        \n",
    "        x = X[i:i+C,:,:]\n",
    "        gamma = gamma_[i//batch]  #there is a gamma,beta for each batch of channels\n",
    "        beta = beta_[i//batch]\n",
    "        ###################################################################\n",
    "        _, H, W = x.shape  #WAS N, D\n",
    "\n",
    "        #step1: calculate mean\n",
    "        mu = np.mean(x) #scalar\n",
    "        #print(mu)\n",
    "\n",
    "        #step2: subtract mean vector of every trainings example\n",
    "        xmu = (x - mu)\n",
    "        #step3: following the lower branch - calculation denominator\n",
    "        #step4: calculate variance\n",
    "        var = np.mean(xmu ** 2)\n",
    "\n",
    "        #step5: add eps for numerical stability, then sqrt\n",
    "        sqrtvar = np.sqrt(var + eps)\n",
    "\n",
    "        #step6: invert sqrtwar\n",
    "        ivar = 1./sqrtvar\n",
    "\n",
    "        #step7: execute normalization\n",
    "        xhat = xmu * ivar\n",
    "\n",
    "        #step8: Nor the two transformation steps\n",
    "        gammax = gamma * xhat\n",
    "        #gamma,beta : scalar\n",
    "        #step9\n",
    "        out = gammax + beta\n",
    "        \n",
    "        xhat_[i:i+C,:,:]   =xhat   #.copy()\n",
    "        #gamma_[i:i+2,:,:]  =gamma\n",
    "        xmu_[i:i+C,:,:]    =xmu\n",
    "        ivar_[i//batch]  =ivar\n",
    "        sqrtvar_[i//batch]=sqrtvar\n",
    "        var_[i//batch]   =var\n",
    "        out_[i:i+C,:,:]   =out\n",
    "    #store intermediate\n",
    "    cache = (xhat_,gamma_,xmu_,ivar_,sqrtvar_,var_,eps)\n",
    "    return out_, cache\n",
    "\n",
    "def groupnorm_backward(dout_, cache):\n",
    "\n",
    "    #unfold the variables stored in cache\n",
    "    xhat_,gamma_,xmu_,ivar_,sqrtvar_,var_,eps = cache\n",
    "\n",
    "    \n",
    "    C_all =dout_.shape[0]\n",
    "    if(C_all == 1):\n",
    "        C = 1\n",
    "    else:\n",
    "        C = 2\n",
    "    \n",
    "    batch = C\n",
    "    dx_    = np.zeros((C_all,dout_.shape[1],dout_.shape[2]))\n",
    "    dgamma_= np.zeros(C_all//batch)\n",
    "    dbeta_ = np.zeros(C_all//batch)\n",
    "    \n",
    "    for i in range(0, C_all, batch): \n",
    "        dout = dout_[i:i+C,:,:]\n",
    "        xhat   =xhat_[i:i+C,:,:]\n",
    "        gamma  = gamma_[i//batch]\n",
    "        xmu    =xmu_[i:i+C,:,:]\n",
    "        ivar   =ivar_[i//batch]\n",
    "        sqrtvar=sqrtvar_[i//batch]\n",
    "        var    =var_[i//batch]\n",
    "        \n",
    "        #get the dimensions of the input/output\n",
    "        _, H, W = dout.shape #N,D = dout.shape\n",
    "\n",
    "        #step9\n",
    "        dbeta = np.sum(dout)\n",
    "        dgammax = dout #not necessary, but more understandable\n",
    "\n",
    "        #step8\n",
    "        dgamma = np.sum(dgammax*xhat)\n",
    "        dxhat = dgammax * gamma\n",
    "\n",
    "        #step7\n",
    "        divar = np.sum(dxhat*xmu)\n",
    "        dxmu1 = dxhat * ivar\n",
    "\n",
    "        #step6\n",
    "        dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "\n",
    "        #step5\n",
    "        dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
    "\n",
    "        #step4\n",
    "        dsq = 1./(batch*H*W) * np.ones((C,H,W)) * dvar  #1./C\n",
    "\n",
    "        #step3\n",
    "        dxmu2 = 2 * xmu * dsq\n",
    "\n",
    "        #step2\n",
    "        dx1 = (dxmu1 + dxmu2)\n",
    "        dmu = -1 * np.sum(dxmu1+dxmu2)\n",
    "\n",
    "        #step1\n",
    "        dx2 =  1./(batch*H*W) *np.ones((C,H,W)) * dmu #1. /C *\n",
    "\n",
    "        #step0\n",
    "        dx = dx1 + dx2\n",
    "        dx_[i:i+C,:,:]    = dx\n",
    "        dgamma_[i//batch]= dgamma\n",
    "        dbeta_[i//batch] = dbeta\n",
    "\n",
    "    return dx_, dgamma_.reshape(C_all//batch,1), dbeta_.reshape(C_all//batch,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validate(X, Y, params, GN):\n",
    "    ### Unpacking ###\n",
    "    [filters, bias, f_dc, out_fb, GN_params] = params\n",
    "    [ga, be, gamma_out, beta_out] = GN_params\n",
    "    [f1,f2,f3,f4,f5,f6,f7,f8,f9] = filters\n",
    "    [b1,b2,b3,b4,b5,b6,b7,b8,b9]= bias \n",
    "    [fb6_dc, fb7_dc, fb8_dc, fb9_dc] = f_dc\n",
    "    [out_f, out_b] = out_fb\n",
    "    #################\n",
    "    \n",
    "    \n",
    "    dropout = 0\n",
    "    print('Calculating Forward step . . .')\n",
    "    \n",
    "    batch = 1\n",
    "    for c in range(0, X.shape[0], batch):\n",
    "        if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "            batch = X.shape[0] - c\n",
    "        X_t = X[c:(c + batch)]\n",
    "        Y_t = Y[c:(c + batch)]\n",
    "        for b in range(batch):\n",
    "            ########### 1st Big Layer ###########    (with zero padding ='same',so with stride =1 we get same dim as the input)\n",
    "            conv1_1, conv1_2, normcache1_1, normcache1_2 = Conv_Block(\"Forward\", f1, b1, X_t[b], dropout, GN, ga[0], be[0])\n",
    "            ##################################### conv1_2: 128x128x16\n",
    "\n",
    "            pl1 = maxpool(conv1_2, 2, 2) #   pl1 : (128-2)/2+1  = 64 \n",
    "\n",
    "            ########### 2nd Big Layer ########### \n",
    "            conv2_1, conv2_2, normcache2_1, normcache2_2 = Conv_Block(\"Forward\", f2, b2, pl1, dropout, GN, ga[1], be[1])          \n",
    "            #####################################  64x64x32\n",
    "\n",
    "            pl2 = maxpool(conv2_2, 2, 2) #pool_f = 2 , pool_s = 2    , (64 -2)/2 +1 = 32\n",
    "\n",
    "            ########### 3rd Big Layer ###########\n",
    "            conv3_1, conv3_2, normcache3_1, normcache3_2 = Conv_Block(\"Forward\", f3, b3, pl2, dropout, GN, ga[2], be[2])          \n",
    "            #####################################  32x32x64\n",
    "\n",
    "            pl3 = maxpool(conv3_2, 2, 2) #pool_f = 2 , pool_s = 2   ,  (32-2)/2 +1 = 16\n",
    "\n",
    "            ########### 4th Big Layer ###########\n",
    "            conv4_1, conv4_2, normcache4_1, normcache4_2 = Conv_Block(\"Forward\", f4, b4, pl3, dropout, GN, ga[3], be[3])             \n",
    "            #####################################     16x16x128\n",
    "\n",
    "            pl4 = maxpool(conv4_2, 2, 2) #pool_f = 2 , pool_s = 2  , (16-2)/2 +1 =8  : 8x8x128\n",
    "\n",
    "            ########### 5th Big Layer ###########   8x8x128-->8x8x256\n",
    "            conv5_1, conv5_2, normcache5_1, normcache5_2 = Conv_Block(\"Forward\", f5, b5, pl4, dropout, GN, ga[4], be[4])       \n",
    "            #####################################  8x8x256\n",
    "\n",
    "            #####################################\n",
    "            #Because of ambigious size after the upsampling the concat func must take care possible crop of the conv#_2 \n",
    "            #####################################\n",
    "            #Deconvolution/Upsampling\n",
    "            # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "            params = [fb6_dc[0], fb6_dc[1]] # deconv filter, deconv bias\n",
    "            dc6, new_in6 = convTransp(conv5_2, params, 1, 0)   #result:   =  16x16x128 , # conv5_2 requires NO crop\n",
    "            #Concat dc6 with conv4_2 so we get 256 channels (16x16x256)\n",
    "            c6 = concat(dc6, conv4_2) # 1st one is the right one size  \n",
    "\n",
    "            ########### 6th Big Layer ###########          16x16x256     \n",
    "            conv6_1, conv6_2, normcache6_1, normcache6_2 = Conv_Block(\"Forward\", f6, b6, c6, dropout, GN, ga[5], be[5])  \n",
    "            #####################################    16x16x128\n",
    "            #(16-1)*2 + 2 =32\n",
    "            params = [fb7_dc[0], fb7_dc[1]] # deconv filter, deconv bias\n",
    "            dc7, new_in7 = convTransp(conv6_2, params, 1, 0)   #result:   =  32x32x64\n",
    "            #Concat dc7 with conv3_2 so we get  channels (32x32x128)\n",
    "            c7 = concat(dc7, conv3_2)   \n",
    "\n",
    "            ########### 7th Big Layer ###########          32x32x128     \n",
    "            conv7_1, conv7_2, normcache7_1, normcache7_2 = Conv_Block(\"Forward\", f7, b7, c7, dropout, GN, ga[6], be[6]) \n",
    "            #####################################    32x32x64\n",
    "            #(24-1)*2 + 2 = 48\n",
    "            params = [fb8_dc[0], fb8_dc[1]] # deconv filter, deconv bias\n",
    "            dc8, new_in8 = convTransp(conv7_2, params, 1, 0)   #result:   =  64x64x32\n",
    "            #Concat dc8 with conv2_2 so we get  channels (64x64x64)\n",
    "            c8 = concat(dc8 ,conv2_2)   \n",
    "\n",
    "            ########### 8th Big Layer ###########          64x64x64    \n",
    "            conv8_1, conv8_2, normcache8_1, normcache8_2 = Conv_Block(\"Forward\", f8, b8, c8, dropout, GN, ga[7], be[7])\n",
    "            #####################################    64x64x32                              \n",
    "            #(64-1)*2 + 2 = 128\n",
    "            params = [fb9_dc[0], fb9_dc[1]] # deconv filter, deconv bias\n",
    "            dc9, new_in9 = convTransp(conv8_2, params, 1, 0)   #result:   =  128x128x16\n",
    "            #Concat dc9 with conv1_2 so we get  channels (128x128x32)\n",
    "            c9 = concat(dc9, conv1_2)                   \n",
    "\n",
    "            ########### 9th Big Layer ###########          128x128x32   \n",
    "            conv9_1, conv9_2, normcache9_1, normcache9_2 = Conv_Block(\"Forward\", f9, b9, c9, dropout, GN, ga[8], be[8])\n",
    "            #####################################    128x128x16\n",
    "\n",
    "            ############################# Last Layer conv(1x1) --> 128x128x1 ##########################\n",
    "            params = [out_f, out_b]\n",
    "            output = conv(conv9_2, params, 1, 0) #output.shape: 128x128x1\n",
    "\n",
    "            #print(output[:,0:10,0:10])\n",
    "            output = normalize(output)\n",
    "            ## Sigmoid ##\n",
    "            Y_hat = sigmoid(output)\n",
    "            \n",
    "            Y_hat[Y_hat>0.65]=1\n",
    "            Y_hat[Y_hat<0.35]=0\n",
    "\n",
    "            plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "            cost_,accuracy_ = Dice_Coef(Y_hat, Y_t[b])#Cross_Entropy(Y_hat, Y_t[b])\n",
    "            cost = cost_\n",
    "            accuracy = accuracy_\n",
    "            print(\"Cost: {:.2f}   -   Accuracy: {:.2f}%\".format(cost/batch, (accuracy*100)/batch))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv_Block(step, f, b, myin, dropout, GN, ga, be):\n",
    "    if(step == \"Forward\"):\n",
    "        bc1 = 0\n",
    "        bc2 = 0\n",
    "        ### DROPOUT ###\n",
    "        if(dropout>0):\n",
    "            d = (np.random.rand(myin.shape[0],myin.shape[1],myin.shape[2])<dropout)\n",
    "            d = d*1 #Bool --> int(0s and 1s)\n",
    "            myin = d*myin\n",
    "        ###############\n",
    "        params = [f[0], b[0]]  \n",
    "        conv1 = conv(myin, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "        ### GROUP NORM ###\n",
    "        if(GN == 1):\n",
    "            conv1, bc1 = groupnorm_forward(conv1, ga[0], be[0]) \n",
    "        ##################\n",
    "        conv1[conv1<=0] = 0 #Relu\n",
    "\n",
    "        params = [f[1], b[1]]\n",
    "        conv2 = conv(conv1, params, 1)\n",
    "        ### GROUP NORM ###\n",
    "        if(GN == 1):\n",
    "            conv2, bc2 = groupnorm_forward(conv2, ga[1], be[1]) \n",
    "        ##################\n",
    "        conv2[conv2<=0] = 0 #Relu\n",
    "        return conv1, conv2, bc1, bc2\n",
    "    else: #Backward\n",
    "        if(isinstance(GN, int)):\n",
    "            dconv_prev = b\n",
    "            conv_prev = myin\n",
    "            conv_prev1 = dropout\n",
    "            conc = ga\n",
    "            dconv_prev[conv_prev<=0] = 0\n",
    "            dconv1, df2, db2 = convolutionBackward(dconv_prev, conv_prev1, f[1], 1) #\n",
    "            #pack data\n",
    "            dconv1[conv_prev1<=0] = 0\n",
    "            conc_dconv1, df1, db1 = convolutionBackward(dconv1, conc, f[0], 1) #\n",
    "            return conc_dconv1, df2, db2, df1, db1\n",
    "        else:\n",
    "            dconv_prev = b\n",
    "            conv_prev = myin\n",
    "            conv_prev1 = dropout\n",
    "            conc = GN\n",
    "            normcache1 = ga\n",
    "            normcache2 = be\n",
    "            dconv_prev[conv_prev<=0] = 0 \n",
    "            dconv_prev, dgamma1_2, dbeta1_2 = groupnorm_backward(dconv_prev, normcache2)\n",
    "            dconv1_1, df1_2, db1_2 = convolutionBackward(dconv_prev, conv_prev1, f[1], 1) #\n",
    "            #pack data\n",
    "            dconv1_1[conv_prev1<=0] = 0\n",
    "            dconv1_1, dgamma1_1, dbeta1_1 = groupnorm_backward(dconv1_1, normcache1)\n",
    "            dga= [dgamma1_1,dgamma1_2]\n",
    "            dbe= [dbeta1_1,dbeta1_2]\n",
    "            conc_dconv1, df1_1, db1_1 = convolutionBackward(dconv1_1, conc, f[0], 1) #C9 is not needed for input,we know how to select the right gradients   \n",
    "            return conc_dconv1, df1_2, db1_2, df1_1, db1_1, dga, dbe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  TRAIN  ######\n",
    "#import torch\n",
    "import time\n",
    "time.time()\n",
    "\n",
    "\n",
    "def train(X, Y, epochs, learning_rate, dropout, GN):\n",
    "    verbose=True\n",
    "    # initiation of neural net parameters\n",
    "    #params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    trim = 0.0001\n",
    "    #f1,f2,f3,f4,f5,fd1,fd2,fd3,fd4\n",
    "    filters,bias, f_dc = init_filters(5, 16, trim) #Double the channel-filter 4 times  (up to 256 and back again)\n",
    "    ##Final 1x1 filter\n",
    "    \n",
    "    \n",
    "    #out_f = np.random.randn(1,16,1,1)*trim\n",
    "    #out_b = np.random.randn(out_f.shape[0],1)*trim \n",
    "    out_f = (1,16,1,1)\n",
    "    out_f = np.random.normal(loc = 0, scale = trim , size = out_f) #np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "    out_b = (out_f.shape[0],1)\n",
    "    out_b = np.random.normal(loc = 0, scale = trim , size = out_b) #np.random.randn(f1.shape[0],1)* trimb\n",
    "    out_fb = [out_f, out_b]\n",
    "    \n",
    "    ### Initialize group normalization parameters\n",
    "    ga, be, gamma_out, beta_out = init_groupnorm_params(bias, out_b, 2, 0, 0.05)#norm_batch, lockbr, trimbr\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    if(GN>0):\n",
    "        print(\"Group Normalization Enabled!\")\n",
    "    else:\n",
    "        print(\"Group Normalization Disabled!\")\n",
    "    if(dropout>0):\n",
    "        print(\"Dropout Enabled! -  Value: {}\".format(dropout))\n",
    "    else:\n",
    "        print(\"Dropout Disabled!\")\n",
    "    print(\"Learning rate: {}\".format(learning_rate))\n",
    "    print(\"Dataset Size: {}\".format(X.shape[0]))\n",
    "    print(\"Weight scale: {}\".format(trim))\n",
    "    print(\"************************************\")\n",
    "    \n",
    "    #filter shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "\n",
    "    v_adam =  [] #filter1,filter2\n",
    "    s_adam =  [] \n",
    "    bv_adam = [] #bias1,bias2\n",
    "    bs_adam = []\n",
    "    fdc_v_adam=[] #filter,bias\n",
    "    fdc_s_adam=[]\n",
    "    #format: [v1,v2,v3,v4,v5,vd1,vd2,vd3,vd4] ,same for the rest, each of these include a tuple for the 2 conv filter\n",
    "    #[s1,s2,s3,s4,s5,sd1,sd2,vs3,sd4]\n",
    "    # upsampling filters : [v1_dc,v2_dc,v3_dc]  \n",
    "\n",
    "            \n",
    "\n",
    "    # performing calculations for subsequent iterations\n",
    "    \n",
    "    \n",
    "    [f1,f2,f3,f4,f5,f6,f7,f8,f9] = filters\n",
    "    [b1,b2,b3,b4,b5,b6,b7,b8,b9]= bias \n",
    "    [fb6_dc, fb7_dc, fb8_dc, fb9_dc] = f_dc\n",
    "    \n",
    "    last_acc = 0\n",
    "    for e in range(epochs):\n",
    "        # step forward\n",
    "        running_loss = 0\n",
    "        cost = 0\n",
    "        accuracy = 0\n",
    "        batch = 1\n",
    "        print(\"Epoch: {%d}\" %(e+1))\n",
    "        for c in range(0, X.shape[0], batch):\n",
    "            print('Batch: {}'.format(int(c/batch +1)))\n",
    "            #### Reset Gradients (Every batch) ####\n",
    "            beta1= 0.92\n",
    "            beta2= 0.995\n",
    "            lr = learning_rate\n",
    "            batch_size= batch\n",
    "            if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "                batch = X.shape[0] - c\n",
    "            X_t = X[c:(c + batch)]  # shape:(m, ch, h, w)\n",
    "            Y_t = Y[c:(c + batch)]\n",
    "            \n",
    "            \n",
    "            df =  []\n",
    "            db =  []\n",
    "            dfb=  []\n",
    "            for i in filters:\n",
    "                v1 = np.zeros(i[0].shape)\n",
    "                v2 = np.zeros(i[1].shape)\n",
    "                s1 = np.zeros(i[0].shape)\n",
    "                s2 = np.zeros(i[1].shape)\n",
    "                v_a = [v1, v2]\n",
    "                s_a = [s1, s2]\n",
    "                v_adam.append(v_a)\n",
    "                s_adam.append(s_a)\n",
    "                \n",
    "                df1_t = np.zeros(i[0].shape)\n",
    "                df2_t = np.zeros(i[1].shape)\n",
    "                f_temp = [df1_t, df2_t]\n",
    "                df.append(f_temp)\n",
    "                \n",
    "            for i in bias:\n",
    "                bv1 = np.zeros(i[0].shape)\n",
    "                bv2 = np.zeros(i[1].shape)\n",
    "                bs1 = np.zeros(i[0].shape)\n",
    "                bs2 = np.zeros(i[1].shape)    \n",
    "                bv_a = [bv1, bv2]\n",
    "                bs_a = [bs1, bs2]\n",
    "                bv_adam.append(bv_a)\n",
    "                bs_adam.append(bs_a)\n",
    "                \n",
    "                \n",
    "                db1_t = np.zeros(i[0].shape)\n",
    "                db2_t = np.zeros(i[1].shape)\n",
    "                b_temp = [db1_t, db2_t]\n",
    "                db.append(b_temp)\n",
    "            \n",
    "            for i in f_dc:\n",
    "                fdc_v1 = np.zeros(i[0].shape)\n",
    "                bdc_v2 = np.zeros(i[1].shape)\n",
    "                fdc_s1 = np.zeros(i[0].shape)\n",
    "                bdc_s2 = np.zeros(i[1].shape)    \n",
    "                fdc_v_a = [fdc_v1, bdc_v2]\n",
    "                fdc_s_a = [fdc_s1, bdc_s2]\n",
    "                fdc_v_adam.append(fdc_v_a)\n",
    "                fdc_s_adam.append(fdc_s_a)\n",
    "                \n",
    "                \n",
    "                df1_t = np.zeros(i[0].shape)\n",
    "                db1_t = np.zeros(i[1].shape)\n",
    "                fb_temp = [df1_t, db1_t]\n",
    "                dfb.append(fb_temp)\n",
    "            \n",
    "            \n",
    "            #Final layer 1x1 filter setup\n",
    "\n",
    "            v_out_f = np.zeros(out_f.shape)\n",
    "            s_out_f = np.zeros(out_f.shape)\n",
    "            bv_out_b = np.zeros(out_b.shape)\n",
    "            bs_out_b = np.zeros(out_b.shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "            dout_f = np.zeros(out_f.shape)\n",
    "            dout_b = np.zeros(out_b.shape)\n",
    "            \n",
    "            ######################################\n",
    "            \n",
    "            \n",
    "            #timestamp1 = time.time()\n",
    "            \n",
    "            \n",
    "            [df1,df2,df3,df4,df5,df6,df7,df8,df9] = df\n",
    "            [db1,db2,db3,db4,db5,db6,db7,db8,db9] = db \n",
    "            [dfb6_dc,dfb7_dc,dfb8_dc,dfb9_dc]     = dfb\n",
    "            \n",
    "            \n",
    "            for b in range(batch):\n",
    "                \n",
    "                print('Image: {}/{}'.format((b+1),batch))\n",
    "                #################### TODO: BLOCK IMPLEMENTATION - FUTURE UPDATE ######################\n",
    "                    \n",
    "                    \n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ######################## Forward Propagation Convolution Part  ##########################\n",
    "\n",
    "                \n",
    "                ########### 1st Big Layer ###########    (with zero padding ='same',so with stride =1 we get same dim as the input)\n",
    "                conv1_1, conv1_2, normcache1_1, normcache1_2 = Conv_Block(\"Forward\", f1, b1, X_t[b], dropout, GN, ga[0], be[0])\n",
    "                ##################################### conv1_2: 128x128x16\n",
    "                \n",
    "                pl1 = maxpool(conv1_2, 2, 2) #   pl1 : (128-2)/2+1  = 64 \n",
    "                \n",
    "                ########### 2nd Big Layer ########### \n",
    "                conv2_1, conv2_2, normcache2_1, normcache2_2 = Conv_Block(\"Forward\", f2, b2, pl1, dropout, GN, ga[1], be[1])          \n",
    "                #####################################  64x64x32\n",
    "\n",
    "                pl2 = maxpool(conv2_2, 2, 2) #pool_f = 2 , pool_s = 2    , (64 -2)/2 +1 = 32\n",
    "\n",
    "                ########### 3rd Big Layer ###########\n",
    "                conv3_1, conv3_2, normcache3_1, normcache3_2 = Conv_Block(\"Forward\", f3, b3, pl2, dropout, GN, ga[2], be[2])          \n",
    "                #####################################  32x32x64\n",
    "\n",
    "                pl3 = maxpool(conv3_2, 2, 2) #pool_f = 2 , pool_s = 2   ,  (32-2)/2 +1 = 16\n",
    "\n",
    "                ########### 4th Big Layer ###########\n",
    "                conv4_1, conv4_2, normcache4_1, normcache4_2 = Conv_Block(\"Forward\", f4, b4, pl3, dropout, GN, ga[3], be[3])             \n",
    "                #####################################     16x16x128\n",
    "\n",
    "                pl4 = maxpool(conv4_2, 2, 2) #pool_f = 2 , pool_s = 2  , (16-2)/2 +1 =8  : 8x8x128\n",
    "                \n",
    "                ########### 5th Big Layer ###########   8x8x128-->8x8x256\n",
    "                conv5_1, conv5_2, normcache5_1, normcache5_2 = Conv_Block(\"Forward\", f5, b5, pl4, dropout, GN, ga[4], be[4])       \n",
    "                #####################################  8x8x256\n",
    "\n",
    "                #####################################\n",
    "                #Because of ambigious size after the upsampling the concat func must take care possible crop of the conv#_2 \n",
    "                #####################################\n",
    "                #Deconvolution/Upsampling\n",
    "                # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "                params = [fb6_dc[0], fb6_dc[1]] # deconv filter, deconv bias\n",
    "                dc6, new_in6 = convTransp(conv5_2, params, 1, 0)   #result:   =  16x16x128 , # conv5_2 requires NO crop\n",
    "                #Concat dc6 with conv4_2 so we get 256 channels (16x16x256)\n",
    "                c6 = concat(dc6, conv4_2) # 1st one is the right one size  \n",
    "                \n",
    "                ########### 6th Big Layer ###########          16x16x256     \n",
    "                conv6_1, conv6_2, normcache6_1, normcache6_2 = Conv_Block(\"Forward\", f6, b6, c6, dropout, GN, ga[5], be[5])  \n",
    "                #####################################    16x16x128\n",
    "                #(16-1)*2 + 2 =32\n",
    "                params = [fb7_dc[0], fb7_dc[1]] # deconv filter, deconv bias\n",
    "                dc7, new_in7 = convTransp(conv6_2, params, 1, 0)   #result:   =  32x32x64\n",
    "                #Concat dc7 with conv3_2 so we get  channels (32x32x128)\n",
    "                c7 = concat(dc7, conv3_2)   \n",
    "                \n",
    "                ########### 7th Big Layer ###########          32x32x128     \n",
    "                conv7_1, conv7_2, normcache7_1, normcache7_2 = Conv_Block(\"Forward\", f7, b7, c7, dropout, GN, ga[6], be[6]) \n",
    "                #####################################    32x32x64\n",
    "                #(24-1)*2 + 2 = 48\n",
    "                params = [fb8_dc[0], fb8_dc[1]] # deconv filter, deconv bias\n",
    "                dc8, new_in8 = convTransp(conv7_2, params, 1, 0)   #result:   =  64x64x32\n",
    "                #Concat dc8 with conv2_2 so we get  channels (64x64x64)\n",
    "                c8 = concat(dc8 ,conv2_2)   \n",
    "                \n",
    "                ########### 8th Big Layer ###########          64x64x64    \n",
    "                conv8_1, conv8_2, normcache8_1, normcache8_2 = Conv_Block(\"Forward\", f8, b8, c8, dropout, GN, ga[7], be[7])\n",
    "                #####################################    64x64x32                              \n",
    "                #(64-1)*2 + 2 = 128\n",
    "                params = [fb9_dc[0], fb9_dc[1]] # deconv filter, deconv bias\n",
    "                dc9, new_in9 = convTransp(conv8_2, params, 1, 0)   #result:   =  128x128x16\n",
    "                #Concat dc9 with conv1_2 so we get  channels (128x128x32)\n",
    "                c9 = concat(dc9, conv1_2)                   \n",
    "               \n",
    "                ########### 9th Big Layer ###########          128x128x32   \n",
    "                conv9_1, conv9_2, normcache9_1, normcache9_2 = Conv_Block(\"Forward\", f9, b9, c9, dropout, GN, ga[8], be[8])\n",
    "                #####################################    128x128x16\n",
    "                \n",
    "                ############################# Last Layer conv(1x1) --> 128x128x1 ##########################\n",
    "                params = [out_f, out_b]\n",
    "                output = conv(conv9_2, params, 1, 0) #output.shape: 128x128x1\n",
    "                \n",
    "                #print(output[:,0:10,0:10])\n",
    "                #if(GN == 0):\n",
    "                output = normalize(output)\n",
    "                ## Sigmoid ##\n",
    "                Y_hat = sigmoid(output)\n",
    "                \n",
    "                #label crop is needed\n",
    "                #Y_t_b = crop(Y_t[b], Y_hat.shape[1])\n",
    "                #plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "                cost_,accuracy_ = Dice_Coef(Y_hat, Y_t[b])\n",
    "                cost += cost_\n",
    "                accuracy += accuracy_\n",
    "                print(accuracy_*100)\n",
    "                if((c+1) == X.shape[0]): #assuming that batch is always  1\n",
    "                    if (accuracy/(c+1)>last_acc):\n",
    "                        #plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "                        last_acc = accuracy/(c+1)\n",
    "                        print(\"New parameters Saved!\")\n",
    "                        GN_params = [ga, be, gamma_out, beta_out]\n",
    "                        parameters = [filters, bias, f_dc, out_fb, GN_params]\n",
    "                        path = os.path.join(os.path.expanduser('~/'), 'data', 'salt')\n",
    "                        with open(path+'/weights', 'wb') as fp:\n",
    "                            pickle.dump(parameters, fp)\n",
    "                    if ((accuracy/(c+1))>0.89):\n",
    "                        print(\"Latest Accuracy: {}%\".format(accuracy*100))\n",
    "                        params_values = [filters, bias, f_dc, out_fb]\n",
    "                        return params_values\n",
    "                \n",
    "                \n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ################################# Backward Propagation ##################################\n",
    "                \n",
    "                \n",
    "                #Pixel-wise sub, we we can get the diff(Y includes the 2 classes 0 and 1)\n",
    "                \n",
    "                dA_prev = Y_hat - Y_t[b]\n",
    "                dZ_prev = sigmoid_backward(dA_prev, output)\n",
    "                conv_s =1 \n",
    "                dconv9_2, dout_f_, dout_b_ = convolutionBackward(dZ_prev, conv9_2, out_f, conv_s) #       \n",
    "                \n",
    "                if(GN == 0):\n",
    "                    conc_dconv9, df9_2, db9_2, df9_1, db9_1 = Conv_Block(\"Backward\", f9, dconv9_2, conv9_2, conv9_1, 0, c9, 0)\n",
    "                else:\n",
    "                    conc_dconv9, df9_2, db9_2, df9_1, db9_1, dga9, dbe9 = Conv_Block(\"Backward\", f9, dconv9_2, conv9_2, conv9_1, c9, normcache9_1, normcache9_2)\n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv9, dconv1_2 = crop2half(conc_dconv9)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #conv8_2 is not needed for input,we know how to select the right gradients            \n",
    "                #dconv1_2 = reshape(dconv1_2, conv1_2.shape[1])\n",
    "                \n",
    "                dconv8_2, df9_dc, db9_dc = convTranspBackward(dconv9, new_in9, fb9_dc[0],conv_s)\n",
    "                #pack data\n",
    "\n",
    "                if(GN == 0):\n",
    "                    conc_dconv8, df8_2, db8_2, df8_1, db8_1 = Conv_Block(\"Backward\", f8, dconv8_2, conv8_2, conv8_1, 0, c8, 0)\n",
    "                else:\n",
    "                    conc_dconv8, df8_2, db8_2, df8_1, db8_1, dga8, dbe8 = Conv_Block(\"Backward\", f8, dconv8_2, conv8_2, conv8_1, c8, normcache8_1, normcache8_2)\n",
    "                    \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv8, dconv2_2 = crop2half(conc_dconv8)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #dconv2_2 = reshape(dconv2_2, conv2_2.shape[1])\n",
    "                \n",
    "                dconv7_2, df8_dc, db8_dc = convTranspBackward(dconv8, new_in8, fb8_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    conc_dconv7, df7_2, db7_2, df7_1, db7_1 = Conv_Block(\"Backward\", f7, dconv7_2, conv7_2, conv7_1, 0, c7, 0)\n",
    "                else:\n",
    "                    conc_dconv7, df7_2, db7_2, df7_1, db7_1, dga7, dbe7 = Conv_Block(\"Backward\", f7, dconv7_2, conv7_2, conv7_1, c7, normcache7_1, normcache7_2)\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv7, dconv3_2 = crop2half(conc_dconv7)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #Make sure that dconv3_2 is the same dim with the dconv3_2 that will come from maxpool in decoding side\n",
    "                #dconv3_2 = reshape(dconv3_2, conv3_2.shape[1])\n",
    "                \n",
    "                dconv6_2, df7_dc, db7_dc = convTranspBackward(dconv7, new_in7, fb7_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    conc_dconv6, df6_2, db6_2, df6_1, db6_1 = Conv_Block(\"Backward\", f6, dconv6_2, conv6_2, conv6_1, 0, c6, 0)\n",
    "                else:     \n",
    "                    conc_dconv6, df6_2, db6_2, df6_1, db6_1, dga6, dbe6 = Conv_Block(\"Backward\", f6, dconv6_2, conv6_2, conv6_1, c6, normcache6_1, normcache6_2)\n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv6, dconv4_2 = crop2half(conc_dconv6)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #dconv4_2 = reshape(dconv4_2, conv4_2.shape[1])\n",
    "                \n",
    "                dconv5_2, df6_dc, db6_dc = convTranspBackward(dconv6, new_in6, fb6_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    dpl4, df5_2, db5_2, df5_1, db5_1 = Conv_Block(\"Backward\", f5, dconv5_2, conv5_2, conv5_1, 0, pl4, 0)\n",
    "                else:     \n",
    "                    dpl4, df5_2, db5_2, df5_1, db5_1, dga5, dbe5 = Conv_Block(\"Backward\", f5, dconv5_2, conv5_2, conv5_1, pl4, normcache5_1, normcache5_2)\n",
    "\n",
    "                \n",
    "                dconv4_2 += maxpoolBackward(dpl4, conv4_2, f=2 , s=2) #Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    dpl3, df4_2, db4_2, df4_1, db4_1 = Conv_Block(\"Backward\", f4, dconv4_2, conv4_2, conv4_1, 0, pl3, 0)\n",
    "                else:     \n",
    "                    dpl3, df4_2, db4_2, df4_1, db4_1, dga4, dbe4 = Conv_Block(\"Backward\", f4, dconv4_2, conv4_2, conv4_1, pl3, normcache4_1, normcache4_2)\n",
    "\n",
    "\n",
    "                dconv3_2 += maxpoolBackward(dpl3, conv3_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    dpl2, df3_2, db3_2, df3_1, db3_1 = Conv_Block(\"Backward\", f3, dconv3_2, conv3_2, conv3_1, 0, pl2, 0)\n",
    "                else:     \n",
    "                    dpl2, df3_2, db3_2, df3_1, db3_1, dga3, dbe3 = Conv_Block(\"Backward\", f3, dconv3_2, conv3_2, conv3_1, pl2, normcache3_1, normcache3_2)\n",
    "\n",
    "                \n",
    "                dconv2_2 += maxpoolBackward(dpl2, conv2_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    dpl1, df2_2, db2_2, df2_1, db2_1 = Conv_Block(\"Backward\", f2, dconv2_2, conv2_2, conv2_1, 0, pl1, 0)\n",
    "                else:     \n",
    "                    dpl1, df2_2, db2_2, df2_1, db2_1, dga2, dbe2 = Conv_Block(\"Backward\", f2, dconv2_2, conv2_2, conv2_1, pl1, normcache2_1, normcache2_2)\n",
    "\n",
    "                \n",
    "                dconv1_2 += maxpoolBackward(dpl1, conv1_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                if(GN == 0):\n",
    "                    _, df1_2, db1_2, df1_1, db1_1 = Conv_Block(\"Backward\", f1, dconv1_2, conv1_2, conv1_1, 0, X_t[b], 0)\n",
    "                else:     \n",
    "                    _, df1_2, db1_2, df1_1, db1_1, dga1, dbe1 = Conv_Block(\"Backward\", f1, dconv1_2, conv1_2, conv1_1, X_t[b], normcache1_1, normcache1_2)\n",
    "\n",
    "                \n",
    "                \n",
    "                if(GN == 1):\n",
    "                    dgamma = [dga1,dga2,dga3,dga4,dga5,dga6,dga7,dga8,dga9]\n",
    "                    dbeta = [dbe1,dbe2,dbe3,dbe4,dbe5,dbe6,dbe7,dbe8,dbe9]\n",
    "                \n",
    "\n",
    "                [df1,df2,df3,df4,df5,df6,df7,df8,df9] = df\n",
    "                [db1,db2,db3,db4,db5,db6,db7,db8,db9] = db \n",
    "                [dfb6_dc,dfb7_dc,dfb8_dc,dfb9_dc]     = dfb\n",
    "\n",
    "                \n",
    "                df1[0] += df1_1\n",
    "                df1[1] += df1_2\n",
    "                df2[0] += df2_1\n",
    "                df2[1] += df2_2\n",
    "                df3[0] += df3_1\n",
    "                df3[1] += df3_2\n",
    "                df4[0] += df4_1\n",
    "                df4[1] += df4_2\n",
    "                df5[0] += df5_1\n",
    "                df5[1] += df5_2\n",
    "                df6[0] += df6_1\n",
    "                df6[1] += df6_2\n",
    "                df7[0] += df7_1\n",
    "                df7[1] += df7_2\n",
    "                df8[0] += df8_1\n",
    "                df8[1] += df8_2\n",
    "                df9[0] += df9_1\n",
    "                df9[1] += df9_2\n",
    "                \n",
    "                db1[0] += db1_1\n",
    "                db1[1] += db1_2\n",
    "                db2[0] += db2_1\n",
    "                db2[1] += db2_2\n",
    "                db3[0] += db3_1\n",
    "                db3[1] += db3_2\n",
    "                db4[0] += db4_1\n",
    "                db4[1] += db4_2\n",
    "                db5[0] += db5_1\n",
    "                db5[1] += db5_2\n",
    "                db6[0] += db6_1\n",
    "                db6[1] += db6_2\n",
    "                db7[0] += db7_1\n",
    "                db7[1] += db7_2\n",
    "                db8[0] += db8_1\n",
    "                db8[1] += db8_2\n",
    "                db9[0] += db9_1\n",
    "                db9[1] += db9_2\n",
    "\n",
    "                dfb6_dc[0] += df6_dc\n",
    "                dfb6_dc[1] += db6_dc\n",
    "                dfb7_dc[0] += df7_dc\n",
    "                dfb7_dc[1] += db7_dc\n",
    "                dfb8_dc[0] += df8_dc\n",
    "                dfb8_dc[1] += db8_dc\n",
    "                dfb9_dc[0] += df9_dc\n",
    "                dfb9_dc[1] += db9_dc\n",
    "\n",
    "                dout_f += dout_f_\n",
    "                dout_b += dout_b_\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                ############## Adam Optimization ################\n",
    "                #changing the main structures(which are also updated)\n",
    "                #TODO: Future update - remove uneccessary memory loads/stores, v,s dont need to be saved \n",
    "                for i in range(len(filters)):\n",
    "                    v_adam[i][0] = beta1*v_adam[i][0] + (1-beta1)*df[i][0]/batch_size #f1\n",
    "                    s_adam[i][0] = beta2*s_adam[i][0] + (1-beta2)*(df[i][0]/batch_size)**2 #f1\n",
    "                    filters[i][0] -= lr*v_adam[i][0]/np.sqrt(s_adam[i][0] + 1e-7)\n",
    "\n",
    "                    v_adam[i][1] = beta1*v_adam[i][1] + (1-beta1)*df[i][1]/batch_size #f2\n",
    "                    s_adam[i][1] = beta2*s_adam[i][1] + (1-beta2)*(df[i][1]/batch_size)**2  #f2\n",
    "                    filters[i][1] -= lr*v_adam[i][1]/np.sqrt(s_adam[i][1] + 1e-7)\n",
    "\n",
    "                for i in range(len(bias)):\n",
    "                    bv_adam[i][0] = beta1*bv_adam[i][0] + (1-beta1)*db[i][0]/batch_size #b1\n",
    "                    bs_adam[i][0] = beta2*bs_adam[i][0] + (1-beta2)*(db[i][0]/batch_size)**2  #b1\n",
    "                    bias[i][0] -= lr*bv_adam[i][0]/np.sqrt(bs_adam[i][0] + 1e-7)\n",
    "\n",
    "                    bv_adam[i][1] = beta1*bv_adam[i][1] + (1-beta1)*db[i][1]/batch_size #b2\n",
    "                    bs_adam[i][1] = beta2*bs_adam[i][1] + (1-beta2)*(db[i][1]/batch_size)**2  #b2\n",
    "                    bias[i][1] -= lr*bv_adam[i][1]/np.sqrt(bs_adam[i][1] + 1e-7)\n",
    "\n",
    "                for i in range(len(f_dc)):\n",
    "                    fdc_v_adam[i][0] = beta1*fdc_v_adam[i][0] + (1-beta1)*dfb[i][0]/batch_size #f1\n",
    "                    fdc_s_adam[i][0] = beta2*fdc_s_adam[i][0] + (1-beta2)*(dfb[i][0]/batch_size)**2  #f1\n",
    "                    f_dc[i][0] -= lr*fdc_v_adam[i][0]/np.sqrt(fdc_s_adam[i][0] + 1e-7)\n",
    "\n",
    "                    fdc_v_adam[i][1] = beta1*fdc_v_adam[i][1] + (1-beta1)*dfb[i][1]/batch_size #b2\n",
    "                    fdc_s_adam[i][1] = beta2*fdc_s_adam[i][1] + (1-beta2)*(dfb[i][1]/batch_size)**2  #b2\n",
    "                    f_dc[i][1] -= lr*fdc_v_adam[i][1]/np.sqrt(fdc_s_adam[i][1] + 1e-7)    \n",
    "\n",
    "                v_out_f = beta1*v_out_f + (1 - beta1)*dout_f/batch_size #f\n",
    "                s_out_f = beta2*s_out_f + (1 - beta2)*(dout_f/batch_size)**2  #f\n",
    "                out_fb[0] -= lr*v_out_f/np.sqrt(s_out_f + 1e-7)\n",
    "\n",
    "                bv_out_b = beta1*bv_out_b + (1 - beta1)*dout_b/batch_size #b\n",
    "                bs_out_b = beta2*bs_out_b + (1 - beta2)*(dout_b/batch_size)**2  #b\n",
    "                out_fb[1] -= lr*bv_out_b/np.sqrt(bs_out_b + 1e-7)\n",
    "\n",
    "                if(GN == 1):\n",
    "                    mytrim = 20\n",
    "                    for i in range(len(ga)):\n",
    "                        ga[i][0] -= lr*mytrim*dgamma[i][0]\n",
    "                        ga[i][1] -= lr*mytrim*dgamma[i][1]\n",
    "\n",
    "                        be[i][0] -= lr*mytrim*dbeta[i][0]\n",
    "                        be[i][1] -= lr*mytrim*dbeta[i][1]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            for i in range(len(filters)):\n",
    "                filters[i][0] -= lr*df[i][0]\n",
    "                bias[i][0] -= lr*db[i][0]\n",
    "            \n",
    "            for i in range(len(f_dc)):\n",
    "                f_dc[i][0] -= lr*dfb[i][0]\n",
    "            \n",
    "            out_fb[0] -= lr*dout_f\n",
    "            out_fb[1] -= lr*dout_b\n",
    "            \n",
    "            #Updating Conv Part\n",
    "            \n",
    "\n",
    "            #print(Yh)\n",
    "            #print(t)\n",
    "            #print(\"Batch:{}\".format(c+12))\n",
    "            \n",
    "           \n",
    "            f1 -= (learning_rate) * (df1/batch)\n",
    "            f2 -= (learning_rate) * (df2/batch)\n",
    "            b1 -= (learning_rate) * (db1/batch)\n",
    "            b2 -= (learning_rate) * (db2/batch)\n",
    "            filter_params = [f1, f2, b1, b2]\n",
    "                \n",
    "            # updating FC Part\n",
    "            #params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "            grads_values['dW1'] = dW1#/(batch)\n",
    "            grads_values['dW2'] = dW2#/(batch)\n",
    "            grads_values['db1'] = dB1#/(batch)\n",
    "            grads_values['db2'] = dB2#/(batch)\n",
    "                   \n",
    "            for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "                \n",
    "                params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "                params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "            \n",
    "            '''\n",
    "            \n",
    "        #END OF LOOP - EPOCH\n",
    "        #timestamp2 = time.time()\n",
    "        #print (\"This took %.2f seconds\" %(timestamp2 - timestamp1))\n",
    "        if(batch == 1):\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/(c+1), (accuracy*100)/(c+1)))\n",
    "        else:\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/batch, (accuracy*100)/batch))\n",
    "        #if(callback is not None):\n",
    "        #    callback(i, params_values)\n",
    "    #pack filters\n",
    "    #params_values = [filters, bias, f_dc, out_fb]\n",
    "    fp.close()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************\n",
      "Group Normalization Enabled!\n",
      "Dropout Disabled!\n",
      "Learning rate: 0.008\n",
      "Dataset Size: 1\n",
      "Weight scale: 0.0001\n",
      "************************************\n",
      "Epoch: {1}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "63.11205264560722\n",
      "New parameters Saved!\n",
      "Epoch:     1   -   cost: 0.46   -   Accuracy: 63.11%\n",
      "Epoch: {2}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "63.22252823940963\n",
      "New parameters Saved!\n",
      "Epoch:     2   -   cost: 0.46   -   Accuracy: 63.22%\n",
      "Epoch: {3}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "63.54607805117817\n",
      "New parameters Saved!\n",
      "Epoch:     3   -   cost: 0.45   -   Accuracy: 63.55%\n",
      "Epoch: {4}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "64.34085906481029\n",
      "New parameters Saved!\n",
      "Epoch:     4   -   cost: 0.44   -   Accuracy: 64.34%\n",
      "Epoch: {5}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "65.04609740413476\n",
      "New parameters Saved!\n",
      "Epoch:     5   -   cost: 0.43   -   Accuracy: 65.05%\n",
      "Epoch: {6}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "66.42948666118183\n",
      "New parameters Saved!\n",
      "Epoch:     6   -   cost: 0.41   -   Accuracy: 66.43%\n",
      "Epoch: {7}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "66.63051515604644\n",
      "New parameters Saved!\n",
      "Epoch:     7   -   cost: 0.41   -   Accuracy: 66.63%\n",
      "Epoch: {8}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "67.39591091243103\n",
      "New parameters Saved!\n",
      "Epoch:     8   -   cost: 0.39   -   Accuracy: 67.40%\n",
      "Epoch: {9}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "66.7142645892817\n",
      "Epoch:     9   -   cost: 0.40   -   Accuracy: 66.71%\n",
      "Epoch: {10}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "64.74998106312266\n",
      "Epoch:    10   -   cost: 0.43   -   Accuracy: 64.75%\n",
      "Epoch: {11}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "67.28665989986597\n",
      "Epoch:    11   -   cost: 0.40   -   Accuracy: 67.29%\n",
      "Epoch: {12}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "65.72128708491965\n",
      "Epoch:    12   -   cost: 0.42   -   Accuracy: 65.72%\n",
      "Epoch: {13}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "67.30326021431844\n",
      "Epoch:    13   -   cost: 0.40   -   Accuracy: 67.30%\n",
      "Epoch: {14}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "66.05820956985379\n",
      "Epoch:    14   -   cost: 0.41   -   Accuracy: 66.06%\n",
      "Epoch: {15}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "68.16054374626692\n",
      "New parameters Saved!\n",
      "Epoch:    15   -   cost: 0.38   -   Accuracy: 68.16%\n",
      "Epoch: {16}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "66.49316357407292\n",
      "Epoch:    16   -   cost: 0.41   -   Accuracy: 66.49%\n",
      "Epoch: {17}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "69.7288123124119\n",
      "New parameters Saved!\n",
      "Epoch:    17   -   cost: 0.36   -   Accuracy: 69.73%\n",
      "Epoch: {18}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "66.2279438419228\n",
      "Epoch:    18   -   cost: 0.41   -   Accuracy: 66.23%\n",
      "Epoch: {19}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "72.21355644030659\n",
      "New parameters Saved!\n",
      "Epoch:    19   -   cost: 0.33   -   Accuracy: 72.21%\n",
      "Epoch: {20}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "70.48171035628265\n",
      "Epoch:    20   -   cost: 0.35   -   Accuracy: 70.48%\n",
      "Epoch: {21}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "74.07873507884497\n",
      "New parameters Saved!\n",
      "Epoch:    21   -   cost: 0.30   -   Accuracy: 74.08%\n",
      "Epoch: {22}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "69.30727375874109\n",
      "Epoch:    22   -   cost: 0.37   -   Accuracy: 69.31%\n",
      "Epoch: {23}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "74.15450935066852\n",
      "New parameters Saved!\n",
      "Epoch:    23   -   cost: 0.30   -   Accuracy: 74.15%\n",
      "Epoch: {24}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "72.73505442014681\n",
      "Epoch:    24   -   cost: 0.32   -   Accuracy: 72.74%\n",
      "Epoch: {25}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "75.1894639616758\n",
      "New parameters Saved!\n",
      "Epoch:    25   -   cost: 0.29   -   Accuracy: 75.19%\n",
      "Epoch: {26}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "75.14829216140726\n",
      "Epoch:    26   -   cost: 0.29   -   Accuracy: 75.15%\n",
      "Epoch: {27}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "77.62132746451032\n",
      "New parameters Saved!\n",
      "Epoch:    27   -   cost: 0.25   -   Accuracy: 77.62%\n",
      "Epoch: {28}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "74.84977736636948\n",
      "Epoch:    28   -   cost: 0.29   -   Accuracy: 74.85%\n",
      "Epoch: {29}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "78.49023073253912\n",
      "New parameters Saved!\n",
      "Epoch:    29   -   cost: 0.24   -   Accuracy: 78.49%\n",
      "Epoch: {30}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "75.45018214076318\n",
      "Epoch:    30   -   cost: 0.28   -   Accuracy: 75.45%\n",
      "Epoch: {31}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "79.36323298627468\n",
      "New parameters Saved!\n",
      "Epoch:    31   -   cost: 0.23   -   Accuracy: 79.36%\n",
      "Epoch: {32}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "73.6893258758407\n",
      "Epoch:    32   -   cost: 0.31   -   Accuracy: 73.69%\n",
      "Epoch: {33}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "78.51080350469151\n",
      "Epoch:    33   -   cost: 0.24   -   Accuracy: 78.51%\n",
      "Epoch: {34}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "82.48689052208033\n",
      "New parameters Saved!\n",
      "Epoch:    34   -   cost: 0.19   -   Accuracy: 82.49%\n",
      "Epoch: {35}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "85.86091857514866\n",
      "New parameters Saved!\n",
      "Epoch:    35   -   cost: 0.15   -   Accuracy: 85.86%\n",
      "Epoch: {36}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "82.39940780270683\n",
      "Epoch:    36   -   cost: 0.19   -   Accuracy: 82.40%\n",
      "Epoch: {37}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "85.15014782286539\n",
      "Epoch:    37   -   cost: 0.16   -   Accuracy: 85.15%\n",
      "Epoch: {38}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "82.50027955605312\n",
      "Epoch:    38   -   cost: 0.19   -   Accuracy: 82.50%\n",
      "Epoch: {39}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "86.81989140106651\n",
      "New parameters Saved!\n",
      "Epoch:    39   -   cost: 0.14   -   Accuracy: 86.82%\n",
      "Epoch: {40}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "84.81008438827332\n",
      "Epoch:    40   -   cost: 0.16   -   Accuracy: 84.81%\n",
      "Epoch: {41}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "84.15144225041608\n",
      "Epoch:    41   -   cost: 0.17   -   Accuracy: 84.15%\n",
      "Epoch: {42}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "77.88573002684669\n",
      "Epoch:    42   -   cost: 0.25   -   Accuracy: 77.89%\n",
      "Epoch: {43}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "87.17860068588288\n",
      "New parameters Saved!\n",
      "Epoch:    43   -   cost: 0.14   -   Accuracy: 87.18%\n",
      "Epoch: {44}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "89.28493259443188\n",
      "New parameters Saved!\n",
      "Latest Accuracy: 89.28493259443188%\n"
     ]
    }
   ],
   "source": [
    "###### Training ######\n",
    "#train_images, train_labels, test_images, test_labels\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "params_values = train(train_images, train_labels, 80, 0.008, 0, 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Forward step . . .\n",
      "Cost: 0.11   -   Accuracy: 89.28%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = os.path.join(os.path.expanduser('~/'), 'data', 'salt')\n",
    "with open (path+'/weights', 'rb') as fp:\n",
    "    params = pickle.load(fp)  \n",
    "fp.close()\n",
    "Validate(train_images[0:1,:,:,:], train_labels[0:1,:,:,:], params, 1) #GN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Forward step . . .\n",
      "Cost: 0.11   -   Accuracy: 89.32%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Validate(test_images[1:2,:,:,:], test_labels[1:2,:,:,:], params_values);\n",
    "Validate(test_images[0:1,:,:,:], test_labels[0:1,:,:,:], params, 1) #GN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Forward step . . .\n",
      "Cost: 0.29   -   Accuracy: 75.01%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Validate(test_images[1:2,:,:,:], test_labels[1:2,:,:,:], params, 1) #GN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
