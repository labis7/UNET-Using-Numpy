{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "#import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "from matplotlib import image\n",
    "\n",
    "def mnist(path=None):\n",
    "    r\"\"\"Return (train_images, train_labels, test_images, test_labels).\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory containing MNIST. Default is\n",
    "            /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist.\n",
    "            Create if nonexistant. Download any missing files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, train_labels, test_images, test_labels), each\n",
    "            a matrix. Rows are examples. Columns of images are pixel values.\n",
    "            Columns of labels are a onehot encoding of the correct class.\n",
    "\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz',\n",
    "             'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz',\n",
    "             't10k-labels-idx1-ubyte.gz']\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        # Set path to /home/USER/data/mnist or C:\\Users\\USER\\data\\salt\n",
    "        path = os.path.join(os.path.expanduser('~/'), 'data', 'salt')\n",
    "\n",
    "    # Create path if it doesn't exist\n",
    "    #os.makedirs(path, exist_ok=True)\n",
    "    \"\"\"\"\n",
    "    # Download any missing files\n",
    "    for file in files:\n",
    "        if file not in os.listdir(path):\n",
    "            urlretrieve(url + file, os.path.join(path, file))\n",
    "            print(\"Downloaded %s to %s\" % (file, path))\n",
    "    print(\"All files are ready to gzip!\")\n",
    "    \"\"\"\n",
    "    def _images(path):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        folder = path + \"/images/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(32, 32)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,32,32).astype('float32')#/255\n",
    "        return pixels[:1,:,:,:]\n",
    "\n",
    "    def _labels(path):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        \n",
    "        folder = path + \"/labels/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(32, 32)) for f in os.listdir(folder)]\n",
    "        #onlyfiles = [cv2.resize(image.imread(folder+f),(32, 32)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,32,32).astype('float32') #/255\n",
    "        return pixels[:1,:,:,:]\n",
    "    \n",
    "    def _t_images(path):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        folder = path + \"/t_images/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(32, 32)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,32,32).astype('float32')#/255\n",
    "        return pixels[0:2,:,:,:]\n",
    "    def _t_labels(path):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        \n",
    "        folder = path + \"/t_labels/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(32, 32)) for f in os.listdir(folder)]\n",
    "        #onlyfiles = [cv2.resize(image.imread(folder+f),(32, 32)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,32,32).astype('float32') #/255\n",
    "        return pixels[0:2,:,:,:]\n",
    "\n",
    "    print(\"Train Images : Loading . . .\")\n",
    "    train_images = _images(path)\n",
    "    print(\"Train Labels : Loading . . .\")\n",
    "    train_labels = _labels(path)\n",
    "    print(\"Test Images  : Loading . . .\")\n",
    "    test_images = _t_images(path)\n",
    "    print(\"Test Labels  : Loading . . .\")\n",
    "    test_labels = _t_labels(path)\n",
    "    print(\"Done!\")\n",
    "    return train_images, train_labels , test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images : Loading . . .\n",
      "Train Labels : Loading . . .\n",
      "Test Images  : Loading . . .\n",
      "Test Labels  : Loading . . .\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, test_images, test_labels= mnist()  #, test_images, test_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 32, 32)\n",
      "(1, 1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "ch = 1 #Number of channels\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape) # ONE-HOT !!!\n",
    "#print(train_images.T.shape)\n",
    "#print(train_labels.reshape((train_images.shape[0], 10)).shape)\n",
    "\n",
    "##### Getting Ready for the Conv Layers #####\n",
    "#train_images = train_images.reshape(train_images.shape[0], ch, 28, 28).squeeze()\n",
    "#test_images = test_images.reshape(test_images.shape[0], ch, 28, 28).squeeze()\n",
    "\n",
    "#############################################\n",
    "\n",
    "#y = train_images[1].reshape(1,28,28)\n",
    "#print(y.squeeze().shape)\n",
    "'''\n",
    "temp = []\n",
    "#train_labels = train_labels.sum(1)\n",
    "for i in range(int(len(train_labels[:]))):\n",
    "    temp.append(list(train_labels[i][:]).index(1))\n",
    "    \n",
    "    \n",
    "train_labels = np.array(temp.copy())\n",
    "#print(train_labels[0:5])\n",
    "'''\n",
    "#plt.imshow(train_images[0].squeeze(), cmap='Greys_r');\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_labels[0].squeeze(), cmap='Greys_r');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Convolution Architecture - Downsampling/Upsampling  ##################\n",
    "\n",
    "def init_filters(layers, init_n_f ,trim):\n",
    "    filters = []\n",
    "    bias = []\n",
    "    f_dc = []\n",
    "    \n",
    "    trimf = trim\n",
    "    trimb = trim*5\n",
    "    n_f = init_n_f #initial number of filters/kernels\n",
    "    ch_in = 1      #input channels\n",
    "    \"\"\"\n",
    "    for i in range(layers):\n",
    "        if(i != 0):\n",
    "            n_f = n_f*2 #16,32,64,128,256\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trimb\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "    \n",
    "    #Deconvolution filters    \n",
    "    for i in range(1,layers):\n",
    "        n_f = n_f//2 #128,64,32,16\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        fdc = np.random.randn(n_f,ch_in,2,2)*trimf #upsampling filter, its result will be conc with conv4 output so the channels will be doubled again\n",
    "        bdc = np.random.randn(fdc.shape[0],1)* trimb\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trimb\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        dc = [fdc, bdc]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "        f_dc.append(dc)\n",
    "        \n",
    "    \"\"\"\n",
    "    trimbr = trim\n",
    "    locbr = 0\n",
    "    for i in range(layers):\n",
    "        if(i != 0):\n",
    "            n_f = n_f*2 #16,32,64,128,256\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.normal(loc = locbr, scale = trimbr , size = f1) #np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.normal(loc = locbr, scale = trimbr , size = (f1.shape[0],1)) #np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.normal(loc = locbr, scale = trimbr , size = f2)\n",
    "        b2 = np.random.normal(loc = locbr, scale = trimbr , size = (f2.shape[0],1))\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "    \n",
    "    #Deconvolution filters    \n",
    "    for i in range(1,layers):\n",
    "        n_f = n_f//2 #128,64,32,16\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        fdc =  np.random.normal(loc = locbr, scale = trimbr , size = (n_f,ch_in,2,2))#upsampling filter, its result will be conc with conv4 output so the channels will be doubled again\n",
    "        bdc = np.random.normal(loc = locbr, scale = trimbr , size = (fdc.shape[0],1))\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.normal(loc = locbr, scale = trimbr , size = (n_f, ch_in, 3, 3))\n",
    "        b1 = np.random.normal(loc = locbr, scale = trimbr , size = (f1.shape[0],1))\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.normal(loc = locbr, scale = trimbr , size = f2)\n",
    "        b2 = np.random.normal(loc = locbr, scale = trimbr , size = (f2.shape[0],1))\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        dc = [fdc, bdc]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "        f_dc.append(dc)\n",
    "    return filters, bias, f_dc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation Functions ###\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UTILITIES #############\n",
    "\n",
    "def conv(image, params, s = 1, pad = 1 ): # s = 1 (conv stride)\n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    [f, b] = params\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((image.shape[0],image.shape[1]+2*pad ,image.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = image\n",
    "        image = tmp    \n",
    "        \n",
    "    f_num = f.shape[0]\n",
    "    f_depth = f.shape[1] #f1 = 1 , f2 = 8\n",
    "    f_size = f.shape[2] \n",
    "        \n",
    "\n",
    "    h_range = int((image.shape[1] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    np_o = np.zeros((f_num, h_range, w_range))\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                np_o[z, _h, _w] = np.sum(np.multiply(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] , f[z, :, :, :])) + b[z]\n",
    "    \n",
    "    return np_o\n",
    "\n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s = 1, pad = 1 ):\n",
    "    #filt =np.rot90(filt, 2)  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!! A T T E N T I O N !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    f_num, n_c, f, _ = filt.shape\n",
    "    \n",
    "    _ ,h , w = dconv_prev.shape\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((conv_in.shape[0],conv_in.shape[1]+2*pad ,conv_in.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = conv_in\n",
    "        conv_in = tmp\n",
    "    \n",
    "    dconv_in = np.zeros(conv_in.shape)\n",
    "    db = np.zeros((f_num,1))\n",
    "\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                #each entry of the dconv_prev will try to affect the idxs from which was made of.\n",
    "                dfilt[z] += dconv_prev[z, _h, _w] * conv_in[:, _h*s : _h*s + f, _w*s : _w*s + f]\n",
    "                dconv_in[:, _h*s : _h*s + f, _w*s : _w*s + f] += dconv_prev[z, _h, _w] * filt[z]  \n",
    "        db[z] = np.sum(dconv_prev[z])  #, axis =1) ## AXIS?\n",
    "    \n",
    "    if(pad >=1 ):\n",
    "        dconv_in = dconv_in[:, pad:-pad, pad:-pad]  # Cropping\n",
    "        \n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "def convTransp1(image, params, s = 2, pad = 1):\n",
    "    [f, b] = params\n",
    "    n_f, n_c, f_s, _ = f.shape\n",
    "    input_s = image.shape[1]\n",
    "    target_dim = input_s*2\n",
    "    res = np.zeros((n_f, target_dim, target_dim))\n",
    "    temp =np.zeros((n_c, target_dim, target_dim))\n",
    "    for z in range(n_f):\n",
    "        for _h in range(input_s):      \n",
    "            for _w in range(input_s):\n",
    "                temp[:, _h*s:_h*s+f_s, _w*s:_w*s+f_s] += image[:, _h, _w].reshape(n_c,1,1)*f[z,:,:,:] #bias will be added at the end\n",
    "        res[z] = np.sum(temp , axis = 0) + b[z]\n",
    "    return res, image\n",
    "\n",
    "def convTranspBackward1(dconv_prev, new_in, filt, s = 2):\n",
    "    n_f, n_c, f_s, _ = filt.shape\n",
    "    _, input_s, _ = new_in.shape\n",
    "    #final_dim = (new_in.shape[1] - 2)//2 + 1 \n",
    "    dc_s=dconv_prev.shape[1]\n",
    "    temp = np.zeros((n_c,dc_s,dc_s))\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dconv_in = np.zeros(new_in.shape)\n",
    "    db = np.zeros((n_f,1))\n",
    "    for z in range(n_f):\n",
    "        for _h in range(input_s):      \n",
    "            for _w in range(input_s): \n",
    "                dfilt[z] += dconv_prev[z, _h*s:_h*s+f_s, _w*s:_w*s+f_s]*new_in[:,_h,_w].reshape(n_c,1,1)\n",
    "                temp[:, _h*s:_h*s+f_s, _w*s:_w*s+f_s] += dconv_prev[z, _h*s:_h*s+f_s, _w*s:_w*s+f_s] * filt[z]\n",
    "                for ch in range(n_c):\n",
    "                    dconv_in[ch, _h, _w] += np.sum(temp[ch, _h*s:_h*s+f_s, _w*s:_w*s+f_s])\n",
    "        db[z] = np.sum(dconv_prev[z])        \n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "    \n",
    "def convTransp(image, params, s = 1, pad = 1):\n",
    "    #s is always 1, upsample kernel = 2\n",
    "    #zero insertions between pixels s_downsampled -1 = 2-1 = 1\n",
    "    #required padding in order to double my dimensions with the given data:\n",
    "    #(i-1)*2 + k -2p = output size, where our padding is k - p -1 = 2-0-1=1(we assume p=0)\n",
    "    [f, b]=params\n",
    "    f = np.rot90(f, 1, (2,3))\n",
    "    params = [f, b]\n",
    "    input_s = image.shape[1]\n",
    "    target_dim = input_s*2 #final dim, after conv\n",
    "    required_pad = 1 #always for filter 2x2 ,stride 1, zero insertion 1 and main target to double dim\n",
    "    #make our new custom input\n",
    "    size = input_s*2 +1\n",
    "    new_in = np.zeros((image.shape[0], size, size))\n",
    "    \n",
    "    ### OR just: np.pad(image[:,:,:],2,'constant') # Important, we must loop with respect to the 1st dim\n",
    "    for i in range(1, target_dim, 2):\n",
    "        for j in range(1, target_dim, 2):\n",
    "                new_in[:, i, j] = image[:, i//2, j//2]\n",
    "    #now we do a normal convolution(pad = 0)\n",
    "    res = conv(new_in, params, 1, 0) #thats the final result with target_dim\n",
    "    return res, new_in # we will need new_in so we wont calc it again for the backprop\n",
    "    \n",
    "def convTranspBackward(dconv_prev, new_in, filt, s = 1):\n",
    "    #First, we do a backward convolution on new_in,d_conv_prev,\n",
    "    #then we will choose form the d_conv_new the entries that match the initial 'smaller' input image\n",
    "    #by selecting the odd matrix cells 1,3,5... because we had applied a standard pad=1,zero inser=1\n",
    "    dconv_in, dfilt, db = convolutionBackward(dconv_prev, new_in, filt,1,0)\n",
    "    #Now its time to choose the right entries to build the gradients of the initial input image\n",
    "    dim = dconv_in.shape[1]\n",
    "    final_dim = (new_in.shape[1] - 2)//2 + 1 #based on dimen of image before final conv that gives the result,..\n",
    "    #e.g. for new_in 7x7 that is going to convoluted with a 2x2 kernel and give a 6x6 upsampled from 3x3 init image\n",
    "    # now from this 7x7 --> apply the formula above,we get the 3x3 dimension number\n",
    "    res = np.zeros((dconv_in.shape[0], final_dim, final_dim))\n",
    "    for i in range(1, dim, 2):\n",
    "        for j in range(1, dim, 2):\n",
    "                res[:, i//2, j//2] = dconv_in[:, i, j]\n",
    "    return res, dfilt, db\n",
    "    \n",
    "    \n",
    "def maxpool(image, f=2 , s=2):\n",
    "    \n",
    "    h_range = int((image.shape[1] - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f) / s) + 1     # (W - F + 2P) / S \n",
    "    out = np.zeros((image.shape[0], h_range, w_range))\n",
    "    \n",
    "    for z in range(image.shape[0]): # Number of channels\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                out[z, _h, _w] = np.max(image[z, _h*s : _h*s + f, _w*s : _w*s + f])\n",
    "    return out\n",
    "\n",
    "def nanargmax(arr):\n",
    "    #print(arr.shape)\n",
    "    try:\n",
    "        idx = np.nanargmax(arr)\n",
    "        #print (idx)\n",
    "    except:\n",
    "        idx = 0\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "def maxpoolBackward(dpool, conv, f=2 , s=2):\n",
    "    num_c, h, w = conv.shape\n",
    "    h = int((h - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w = int((w - f) / s) + 1     # (W - F + 2P) / S \n",
    "    \n",
    "    dout = np.zeros(conv.shape)\n",
    "    #print(conv.shape)\n",
    "    for z in range(num_c): # Number of channels\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                (a, b) = nanargmax(conv[z, _h*s : _h*s + f, _w*s : _w*s + f]) #Getting the indexes from the max value in this area\n",
    "                #put it on the new array\n",
    "                dout[z, _h + a, _w + b] = dpool[z, _h, _w]\n",
    "    \n",
    "    \n",
    "    return dout\n",
    "\n",
    "\n",
    "def reshape(img, reshape_dim):\n",
    "    pad = reshape_dim - img.shape[1]\n",
    "    if(pad == 0):\n",
    "        return img\n",
    "    res = np.zeros((img.shape[0], reshape_dim, reshape_dim))\n",
    "    if(pad > 1):\n",
    "        res[:, pad//2:-(pad//2), pad//2:-(pad//2)] = img\n",
    "    else:\n",
    "        res[:, 0:-(pad), 0:-(pad)] = img\n",
    "    return res\n",
    "\n",
    "\n",
    "def crop(img, crop_dim): #TODO : look at it..future upgrade to downsample..maybe\n",
    "    start = img.shape[1]//2 - crop_dim//2\n",
    "    return img[:,(start):(start +crop_dim),(start):(start +crop_dim)]\n",
    "\n",
    "    \n",
    "    \n",
    "def crop2half(img):\n",
    "    #return gradients for decoder side and gradients for encoder side\n",
    "    n_ch = img.shape[0]//2\n",
    "    return img[n_ch:n_ch*2 ,:,:], img[0:n_ch,:,:]\n",
    "    \n",
    "def concat(img1_true, img2):\n",
    "    n_ch = img1_true.shape[0]\n",
    "    dim = img1_true.shape[1]\n",
    "    if(img2.shape[1] != dim):\n",
    "        img2 = crop(img2, dim)\n",
    "        print(\"Warning: Extra crop needed and handled!(%d --> %d)\" %(dim, img2.shape[1]))\n",
    "    res = np.zeros((n_ch*2, dim, dim))\n",
    "    res[0:n_ch,:,:] = img2\n",
    "    res[n_ch:n_ch*2 ,:,:] = img1_true\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cost Calculations ######\n",
    "def Cross_Entropy(logs, targets):  # Pixel-Wise Cross entropy --> average accuracy\n",
    "    out = np.zeros(logs.shape)\n",
    "    mylen = logs.shape[0]*logs.shape[1]*logs.shape[2]\n",
    "    #print(len(targets))\n",
    "    #logs.shape (dim x dim) like target\n",
    "    for i in range(logs.shape[1]):\n",
    "        for j in range(logs.shape[2]):\n",
    "            if(targets[:,i,j] >= logs[:,i,j]):#Gray and above\n",
    "                out[:,i,j] = logs[:,i,j]/targets[:,i,j] \n",
    "            else:\n",
    "                out[:,i,j] = (1 - logs[:,i,j])/(1 - targets[:,i,j]) # e.g if logs[i,j]= 0.4 and we want(target) 0 --> 1-0.4=0.6 prob. for zero \n",
    "    #or\n",
    "    #out = targets*logs + ((-1*(targets-1)) - (-1*(targets-1))*logs)\n",
    "    res =out.sum()/mylen\n",
    "    return -np.log(res),res\n",
    "\n",
    "\n",
    "def Dice_Coef(logs, targets):\n",
    "    out = np.zeros(logs.shape)\n",
    "    mylen = logs.shape[0]*logs.shape[1]*logs.shape[2]\n",
    "    #Apply Dice coefficient\n",
    "    numerator = (logs*targets)\n",
    "    denominator = logs + targets\n",
    "    loss = 1 - (2*np.sum(numerator))/(np.sum(denominator))\n",
    "    return loss, np.exp(-loss)\n",
    "                \n",
    "    \n",
    "    \n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    \n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost) ### ATTENTION!\n",
    "\n",
    "def normalize(output):\n",
    "    output[output<-4]=-4\n",
    "    output[output>4] = 4\n",
    "    return output\n",
    "\n",
    "###### Accuracy Calculation ######\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validate(X, Y, params):\n",
    "    ### Unpacking ###\n",
    "    [filters, bias, f_dc, out_fb] = params\n",
    "    [f1,f2,f3,f4,f5,f6,f7,f8,f9] = filters\n",
    "    [b1,b2,b3,b4,b5,b6,b7,b8,b9]= bias \n",
    "    [fb6_dc, fb7_dc, fb8_dc, fb9_dc] = f_dc\n",
    "    [out_f, out_b] = out_fb\n",
    "    #################\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Calculating Forward step . . .')\n",
    "    \n",
    "    batch = 1\n",
    "    for c in range(0, X.shape[0], batch):\n",
    "        if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "            batch = X.shape[0] - c\n",
    "        X_t = X[c:(c + batch)]\n",
    "        Y_t = Y[c:(c + batch)]\n",
    "        for b in range(batch):\n",
    "            ########### 1st Big Layer ###########    (with zero padding ='same',so with stride =1 we get same dim as the input)\n",
    "            params = [f1[0], b1[0]]  \n",
    "            conv1_1 = conv(X_t[b], params, 1)   #conv1 shape = (num_channels, h, w), padding = 1 (same output dim)\n",
    "            conv1_1[conv1_1<=0] = 0 #Relu\n",
    "\n",
    "            params = [f1[1], b1[1]]\n",
    "            conv1_2 = conv(conv1_1, params, 1)\n",
    "            conv1_2[conv1_2<=0] = 0 #Relu\n",
    "            ##################################### conv1_2: 128x128x16\n",
    "\n",
    "            pl1 = maxpool(conv1_2, 2, 2) #   pl1 : (128-2)/2+1  = 64 \n",
    "            ## ADD DROPOUT HERE(on pl1)\n",
    "\n",
    "            ########### 2nd Big Layer ###########\n",
    "            params = [f2[0], b2[0]]  \n",
    "            conv2_1 = conv(pl1, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "            conv2_1[conv2_1<=0] = 0 #Relu\n",
    "\n",
    "            params = [f2[1], b2[1]]\n",
    "            conv2_2 = conv(conv2_1, params, 1)\n",
    "            conv2_2[conv2_2<=0] = 0 #Relu             \n",
    "            #####################################  64x64x32\n",
    "\n",
    "            pl2 = maxpool(conv2_2, 2, 2) #pool_f = 2 , pool_s = 2    , (64 -2)/2 +1 = 32\n",
    "            ## ADD DROPOUT HERE\n",
    "\n",
    "            ########### 3rd Big Layer ###########\n",
    "            params = [f3[0], b3[0]]  \n",
    "            conv3_1 = conv(pl2, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "            conv3_1[conv3_1<=0] = 0 #Relu\n",
    "\n",
    "            params = [f3[1], b3[1]]\n",
    "            conv3_2 = conv(conv3_1, params, 1)\n",
    "            conv3_2[conv3_2<=0] = 0 #Relu             \n",
    "            #####################################  32x32x64\n",
    "\n",
    "            pl3 = maxpool(conv3_2, 2, 2) #pool_f = 2 , pool_s = 2   ,  (32-2)/2 +1 = 16\n",
    "            ## ADD DROPOUT HERE\n",
    "\n",
    "            ########### 4th Big Layer ###########\n",
    "            params = [f4[0], b4[0]]  \n",
    "            conv4_1 = conv(pl3, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "            conv4_1[conv4_1<=0] = 0 #Relu\n",
    "\n",
    "            params = [f4[1], b4[1]]\n",
    "            conv4_2 = conv(conv4_1, params, 1)\n",
    "            conv4_2[conv4_2<=0] = 0 #Relu             \n",
    "            #####################################     16x16x128\n",
    "\n",
    "            pl4 = maxpool(conv4_2, 2, 2) #pool_f = 2 , pool_s = 2  , (16-2)/2 +1 =8  : 8x8x128\n",
    "            ## ADD DROPOUT HERE\n",
    "\n",
    "            ########### 5th Big Layer ###########   8x8x128-->8x8x256\n",
    "            params = [f5[0], b5[0]]  \n",
    "            conv5_1 = conv(pl4, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "            conv5_1[conv5_1<=0] = 0 #Relu\n",
    "\n",
    "            params = [f5[1], b5[1]]\n",
    "            conv5_2 = conv(conv5_1, params, 1)\n",
    "            conv5_2[conv5_2<=0] = 0 #Relu             \n",
    "            #####################################  8x8x256\n",
    "\n",
    "            #####################################\n",
    "            #Because of ambigious size after the upsampling the concat func must take care possible crop of the conv#_2 \n",
    "            #####################################\n",
    "            #Deconvolution/Upsampling\n",
    "            # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "            params = [fb6_dc[0], fb6_dc[1]] # deconv filter, deconv bias\n",
    "            dc6, new_in6 = convTransp(conv5_2, params, 1, 0)   #result:   =  16x16x128 , # conv5_2 requires NO crop\n",
    "            #Concat dc6 with conv4_2 so we get 256 channels (16x16x256)\n",
    "            c6 = concat(dc6, conv4_2) # 1st one is the right one size  \n",
    "\n",
    "            ########### 1st Big dc Layer ###########          16x16x256     \n",
    "            params = [f6[0], b6[0]]  \n",
    "            conv6_1 = conv(c6, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "            conv6_1[conv6_1<=0] = 0 #Relu\n",
    "\n",
    "            params = [f6[1], b6[1]]\n",
    "            conv6_2 = conv(conv6_1, params, 1)\n",
    "            conv6_2[conv6_2<=0] = 0 #Relu   \n",
    "            #####################################    16x16x128\n",
    "            #(16-1)*2 + 2 =32\n",
    "            params = [fb7_dc[0], fb7_dc[1]] # deconv filter, deconv bias\n",
    "            dc7, new_in7 = convTransp(conv6_2, params, 1, 0)   #result:   =  32x32x64\n",
    "            #Concat dc7 with conv3_2 so we get  channels (32x32x128)\n",
    "            c7 = concat(dc7, conv3_2)   \n",
    "\n",
    "            ########### 2nd Big dc Layer ###########          32x32x128     \n",
    "            params = [f7[0], b7[0]]  \n",
    "            conv7_1 = conv(c7, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "            conv7_1[conv7_1<=0] = 0 #Relu\n",
    "\n",
    "            params = [f7[1], b7[1]]\n",
    "            conv7_2 = conv(conv7_1, params, 1)\n",
    "            conv7_2[conv7_2<=0] = 0 #Relu     \n",
    "            #####################################    32x32x64\n",
    "            #(24-1)*2 + 2 = 48\n",
    "            params = [fb8_dc[0], fb8_dc[1]] # deconv filter, deconv bias\n",
    "            dc8, new_in8 = convTransp(conv7_2, params, 1, 0)   #result:   =  64x64x32\n",
    "            #Concat dc8 with conv2_2 so we get  channels (64x64x64)\n",
    "            c8 = concat(dc8 ,conv2_2)   \n",
    "\n",
    "            ########### 3rd Big dc Layer ###########          64x64x64    \n",
    "            params = [f8[0], b8[0]]  \n",
    "            conv8_1 = conv(c8, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "            conv8_1[conv8_1<=0] = 0 #Relu\n",
    "\n",
    "            params = [f8[1], b8[1]]\n",
    "            conv8_2 = conv(conv8_1, params, 1)\n",
    "            conv8_2[conv8_2<=0] = 0 #Relu    \n",
    "            #####################################    64x64x32                              \n",
    "            #(64-1)*2 + 2 = 128\n",
    "            params = [fb9_dc[0], fb9_dc[1]] # deconv filter, deconv bias\n",
    "            dc9, new_in9 = convTransp(conv8_2, params, 1, 0)   #result:   =  128x128x16\n",
    "            #Concat dc9 with conv1_2 so we get  channels (128x128x32)\n",
    "            c9 = concat(dc9, conv1_2)                   \n",
    "\n",
    "            ########### 4th Big dc Layer ###########          128x128x32   \n",
    "            params = [f9[0], b9[0]]  \n",
    "            conv9_1 = conv(c9, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "            conv9_1[conv9_1<=0] = 0 #Relu\n",
    "\n",
    "            params = [f9[1], b9[1]]\n",
    "            conv9_2 = conv(conv9_1, params, 1)\n",
    "            conv9_2[conv9_2<=0] = 0 #Relu   \n",
    "            #####################################    128x128x16\n",
    "\n",
    "            ############################# Last Layer conv(1x1) --> 128x128x1 ##########################\n",
    "            params = [out_f, out_b]\n",
    "            output = conv(conv9_2, params, 1, 0) #output.shape: 128x128x1\n",
    "\n",
    "            #print(output[:,0:10,0:10])\n",
    "            output = normalize(output)\n",
    "            ## Sigmoid ##\n",
    "            Y_hat = sigmoid(output)\n",
    "            \n",
    "            #Y_hat[Y_hat>0.5]=1\n",
    "            #Y_hat[Y_hat<0.5]=0\n",
    "            plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "            cost_,accuracy_ = Dice_Coef(Y_hat, Y_t[b])#Cross_Entropy(Y_hat, Y_t[b])\n",
    "            cost = cost_\n",
    "            accuracy = accuracy_\n",
    "            print(\"Cost: {:.2f}   -   Accuracy: {:.2f}%\".format(cost/batch, (accuracy*100)/batch))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  TRAIN  ######\n",
    "#import torch\n",
    "import time\n",
    "time.time()\n",
    "\n",
    "\n",
    "def train(X, Y, epochs, learning_rate, dropout, verbose=True, callback=None):\n",
    "    # initiation of neural net parameters\n",
    "    #params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    trim = 0.000001\n",
    "    #f1,f2,f3,f4,f5,fd1,fd2,fd3,fd4\n",
    "    filters,bias, f_dc = init_filters(5, 16, trim) #Double the channel-filter 4 times  (up to 256 and back again)\n",
    "    ##Final 1x1 filter\n",
    "    \n",
    "    out_f = np.random.randn(1,16,1,1)*trim\n",
    "    out_b = np.random.randn(out_f.shape[0],1)*trim  \n",
    "    out_fb = [out_f, out_b]\n",
    "    #filter shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "\n",
    "    v_adam =  [] #filter1,filter2\n",
    "    s_adam =  [] \n",
    "    bv_adam = [] #bias1,bias2\n",
    "    bs_adam = []\n",
    "    fdc_v_adam=[] #filter,bias\n",
    "    fdc_s_adam=[]\n",
    "    #format: [v1,v2,v3,v4,v5,vd1,vd2,vd3,vd4] ,same for the rest, each of these include a tuple for the 2 conv filter\n",
    "    #[s1,s2,s3,s4,s5,sd1,sd2,vs3,sd4]\n",
    "    # upsampling filters : [v1_dc,v2_dc,v3_dc]  \n",
    "\n",
    "            \n",
    "\n",
    "    # performing calculations for subsequent iterations\n",
    "    \n",
    "    \n",
    "    [f1,f2,f3,f4,f5,f6,f7,f8,f9] = filters\n",
    "    [b1,b2,b3,b4,b5,b6,b7,b8,b9]= bias \n",
    "    [fb6_dc, fb7_dc, fb8_dc, fb9_dc] = f_dc\n",
    "    \n",
    "    last_acc = 0\n",
    "    for e in range(epochs):\n",
    "        # step forward\n",
    "        running_loss = 0\n",
    "        cost = 0\n",
    "        accuracy = 0\n",
    "        batch = 1\n",
    "        print(\"Epoch: {%d}\" %(e+1))\n",
    "        for c in range(0, X.shape[0], batch):\n",
    "            print('Batch: {}'.format(int(c/batch +1)))\n",
    "            #### Reset Gradients (Every batch) ####\n",
    "            beta1= 0.91\n",
    "            beta2= 0.99\n",
    "            lr = learning_rate\n",
    "            batch_size= batch\n",
    "            if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "                batch = X.shape[0] - c\n",
    "            X_t = X[c:(c + batch)]  # shape:(m, ch, h, w)\n",
    "            Y_t = Y[c:(c + batch)]\n",
    "            \n",
    "            \n",
    "            df =  []\n",
    "            db =  []\n",
    "            dfb=  []\n",
    "            for i in filters:\n",
    "                v1 = np.zeros(i[0].shape)\n",
    "                v2 = np.zeros(i[1].shape)\n",
    "                s1 = np.zeros(i[0].shape)\n",
    "                s2 = np.zeros(i[1].shape)\n",
    "                v_a = [v1, v2]\n",
    "                s_a = [s1, s2]\n",
    "                v_adam.append(v_a)\n",
    "                s_adam.append(s_a)\n",
    "                \n",
    "                df1_t = np.zeros(i[0].shape)\n",
    "                df2_t = np.zeros(i[1].shape)\n",
    "                f_temp = [df1_t, df2_t]\n",
    "                df.append(f_temp)\n",
    "                \n",
    "            for i in bias:\n",
    "                bv1 = np.zeros(i[0].shape)\n",
    "                bv2 = np.zeros(i[1].shape)\n",
    "                bs1 = np.zeros(i[0].shape)\n",
    "                bs2 = np.zeros(i[1].shape)    \n",
    "                bv_a = [bv1, bv2]\n",
    "                bs_a = [bs1, bs2]\n",
    "                bv_adam.append(bv_a)\n",
    "                bs_adam.append(bs_a)\n",
    "                \n",
    "                \n",
    "                db1_t = np.zeros(i[0].shape)\n",
    "                db2_t = np.zeros(i[1].shape)\n",
    "                b_temp = [db1_t, db2_t]\n",
    "                db.append(b_temp)\n",
    "            \n",
    "            for i in f_dc:\n",
    "                fdc_v1 = np.zeros(i[0].shape)\n",
    "                bdc_v2 = np.zeros(i[1].shape)\n",
    "                fdc_s1 = np.zeros(i[0].shape)\n",
    "                bdc_s2 = np.zeros(i[1].shape)    \n",
    "                fdc_v_a = [fdc_v1, bdc_v2]\n",
    "                fdc_s_a = [fdc_s1, bdc_s2]\n",
    "                fdc_v_adam.append(fdc_v_a)\n",
    "                fdc_s_adam.append(fdc_s_a)\n",
    "                \n",
    "                \n",
    "                df1_t = np.zeros(i[0].shape)\n",
    "                db1_t = np.zeros(i[1].shape)\n",
    "                fb_temp = [df1_t, db1_t]\n",
    "                dfb.append(fb_temp)\n",
    "            \n",
    "            \n",
    "            #Final layer 1x1 filter setup\n",
    "\n",
    "            v_out_f = np.zeros(out_f.shape)\n",
    "            s_out_f = np.zeros(out_f.shape)\n",
    "            bv_out_b = np.zeros(out_b.shape)\n",
    "            bs_out_b = np.zeros(out_b.shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "            dout_f = np.zeros(out_f.shape)\n",
    "            dout_b = np.zeros(out_b.shape)\n",
    "            \n",
    "            ######################################\n",
    "            \n",
    "            \n",
    "            #timestamp1 = time.time()\n",
    "            \n",
    "            \n",
    "            [df1,df2,df3,df4,df5,df6,df7,df8,df9] = df\n",
    "            [db1,db2,db3,db4,db5,db6,db7,db8,db9] = db \n",
    "            [dfb6_dc,dfb7_dc,dfb8_dc,dfb9_dc]     = dfb\n",
    "            \n",
    "            \n",
    "            for b in range(batch):\n",
    "                \n",
    "                print('Image: {}/{}'.format((b+1),batch))\n",
    "                #################### TODO: BLOCK IMPLEMENTATION - FUTURE UPDATE ######################\n",
    "                    \n",
    "                    \n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ######################## Forward Propagation Convolution Part  ##########################\n",
    "\n",
    "                \n",
    "                ########### 1st Big Layer ###########    (with zero padding ='same',so with stride =1 we get same dim as the input)\n",
    "                params = [f1[0], b1[0]]  \n",
    "                conv1_1 = conv(X_t[b], params, 1)   #conv1 shape = (num_channels, h, w), padding = 1 (same output dim)\n",
    "                conv1_1[conv1_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f1[1], b1[1]]\n",
    "                conv1_2 = conv(conv1_1, params, 1)\n",
    "                conv1_2[conv1_2<=0] = 0 #Relu\n",
    "                ##################################### conv1_2: 128x128x16\n",
    "                \n",
    "                pl1 = maxpool(conv1_2, 2, 2) #   pl1 : (128-2)/2+1  = 64 \n",
    "                ## ADD DROPOUT HERE(on pl1)\n",
    "                \n",
    "                ########### 2nd Big Layer ###########\n",
    "                params = [f2[0], b2[0]]  \n",
    "                conv2_1 = conv(pl1, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv2_1[conv2_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f2[1], b2[1]]\n",
    "                conv2_2 = conv(conv2_1, params, 1)\n",
    "                conv2_2[conv2_2<=0] = 0 #Relu             \n",
    "                #####################################  64x64x32\n",
    "\n",
    "                pl2 = maxpool(conv2_2, 2, 2) #pool_f = 2 , pool_s = 2    , (64 -2)/2 +1 = 32\n",
    "                ## ADD DROPOUT HERE\n",
    "\n",
    "                ########### 3rd Big Layer ###########\n",
    "                params = [f3[0], b3[0]]  \n",
    "                conv3_1 = conv(pl2, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv3_1[conv3_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f3[1], b3[1]]\n",
    "                conv3_2 = conv(conv3_1, params, 1)\n",
    "                conv3_2[conv3_2<=0] = 0 #Relu             \n",
    "                #####################################  32x32x64\n",
    "\n",
    "                pl3 = maxpool(conv3_2, 2, 2) #pool_f = 2 , pool_s = 2   ,  (32-2)/2 +1 = 16\n",
    "                ## ADD DROPOUT HERE\n",
    "                \n",
    "                ########### 4th Big Layer ###########\n",
    "                params = [f4[0], b4[0]]  \n",
    "                conv4_1 = conv(pl3, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv4_1[conv4_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f4[1], b4[1]]\n",
    "                conv4_2 = conv(conv4_1, params, 1)\n",
    "                conv4_2[conv4_2<=0] = 0 #Relu             \n",
    "                #####################################     16x16x128\n",
    "\n",
    "                pl4 = maxpool(conv4_2, 2, 2) #pool_f = 2 , pool_s = 2  , (16-2)/2 +1 =8  : 8x8x128\n",
    "                ## ADD DROPOUT HERE\n",
    "                \n",
    "                ########### 5th Big Layer ###########   8x8x128-->8x8x256\n",
    "                params = [f5[0], b5[0]]  \n",
    "                conv5_1 = conv(pl4, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv5_1[conv5_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f5[1], b5[1]]\n",
    "                conv5_2 = conv(conv5_1, params, 1)\n",
    "                conv5_2[conv5_2<=0] = 0 #Relu             \n",
    "                #####################################  8x8x256\n",
    "\n",
    "                #####################################\n",
    "                #Because of ambigious size after the upsampling the concat func must take care possible crop of the conv#_2 \n",
    "                #####################################\n",
    "                #Deconvolution/Upsampling\n",
    "                # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "                params = [fb6_dc[0], fb6_dc[1]] # deconv filter, deconv bias\n",
    "                dc6, new_in6 = convTransp(conv5_2, params, 1, 0)   #result:   =  16x16x128 , # conv5_2 requires NO crop\n",
    "                #Concat dc6 with conv4_2 so we get 256 channels (16x16x256)\n",
    "                c6 = concat(dc6, conv4_2) # 1st one is the right one size  \n",
    "                \n",
    "                ########### 1st Big dc Layer ###########          16x16x256     \n",
    "                params = [f6[0], b6[0]]  \n",
    "                conv6_1 = conv(c6, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv6_1[conv6_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f6[1], b6[1]]\n",
    "                conv6_2 = conv(conv6_1, params, 1)\n",
    "                conv6_2[conv6_2<=0] = 0 #Relu   \n",
    "                #####################################    16x16x128\n",
    "                #(16-1)*2 + 2 =32\n",
    "                params = [fb7_dc[0], fb7_dc[1]] # deconv filter, deconv bias\n",
    "                dc7, new_in7 = convTransp(conv6_2, params, 1, 0)   #result:   =  32x32x64\n",
    "                #Concat dc7 with conv3_2 so we get  channels (32x32x128)\n",
    "                c7 = concat(dc7, conv3_2)   \n",
    "                \n",
    "                ########### 2nd Big dc Layer ###########          32x32x128     \n",
    "                params = [f7[0], b7[0]]  \n",
    "                conv7_1 = conv(c7, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv7_1[conv7_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f7[1], b7[1]]\n",
    "                conv7_2 = conv(conv7_1, params, 1)\n",
    "                conv7_2[conv7_2<=0] = 0 #Relu     \n",
    "                #####################################    32x32x64\n",
    "                #(24-1)*2 + 2 = 48\n",
    "                params = [fb8_dc[0], fb8_dc[1]] # deconv filter, deconv bias\n",
    "                dc8, new_in8 = convTransp(conv7_2, params, 1, 0)   #result:   =  64x64x32\n",
    "                #Concat dc8 with conv2_2 so we get  channels (64x64x64)\n",
    "                c8 = concat(dc8 ,conv2_2)   \n",
    "                \n",
    "                ########### 3rd Big dc Layer ###########          64x64x64    \n",
    "                params = [f8[0], b8[0]]  \n",
    "                conv8_1 = conv(c8, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv8_1[conv8_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f8[1], b8[1]]\n",
    "                conv8_2 = conv(conv8_1, params, 1)\n",
    "                conv8_2[conv8_2<=0] = 0 #Relu    \n",
    "                #####################################    64x64x32                              \n",
    "                #(64-1)*2 + 2 = 128\n",
    "                params = [fb9_dc[0], fb9_dc[1]] # deconv filter, deconv bias\n",
    "                dc9, new_in9 = convTransp(conv8_2, params, 1, 0)   #result:   =  128x128x16\n",
    "                #Concat dc9 with conv1_2 so we get  channels (128x128x32)\n",
    "                c9 = concat(dc9, conv1_2)                   \n",
    "               \n",
    "                ########### 4th Big dc Layer ###########          128x128x32   \n",
    "                params = [f9[0], b9[0]]  \n",
    "                conv9_1 = conv(c9, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv9_1[conv9_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f9[1], b9[1]]\n",
    "                conv9_2 = conv(conv9_1, params, 1)\n",
    "                conv9_2[conv9_2<=0] = 0 #Relu   \n",
    "                #####################################    128x128x16\n",
    "                \n",
    "                ############################# Last Layer conv(1x1) --> 128x128x1 ##########################\n",
    "                params = [out_f, out_b]\n",
    "                output = conv(conv9_2, params, 1, 0) #output.shape: 128x128x1\n",
    "                \n",
    "                #print(output[:,0:10,0:10])\n",
    "                output = normalize(output)\n",
    "                ## Sigmoid ##\n",
    "                Y_hat = sigmoid(output)\n",
    "                \n",
    "                #label crop is needed\n",
    "                #Y_t_b = crop(Y_t[b], Y_hat.shape[1])\n",
    "                plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "                cost_,accuracy_ = Dice_Coef(Y_hat, Y_t[b])\n",
    "                cost += cost_\n",
    "                accuracy += accuracy_\n",
    "                if (accuracy>last_acc):\n",
    "                    last_acc = accuracy\n",
    "                    print(\"New parameters Saved!\")\n",
    "                    params_values = [filters, bias, f_dc, out_fb]\n",
    "                if ((accuracy)>0.885):\n",
    "                    print(\"Latest Accuracy: {}%\".format(accuracy*100))\n",
    "                    params_values = [filters, bias, f_dc, out_fb]\n",
    "                    return params_values\n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ################################# Backward Propagation ##################################\n",
    "                \n",
    "                \n",
    "                #Pixel-wise sub, we we can get the diff(Y includes the 2 classes 0 and 1)\n",
    "                \n",
    "                dA_prev = Y_hat - Y_t[b]\n",
    "                dZ_prev = sigmoid_backward(dA_prev, output)\n",
    "                conv_s =1 \n",
    "                dconv9_2, dout_f_, dout_b_ = convolutionBackward(dZ_prev, conv9_2, out_f, conv_s) #\n",
    "                #pack data\n",
    "                \n",
    "                \n",
    "                dconv9_2[conv9_2<=0] = 0             \n",
    "                dconv9_1, df9_2, db9_2 = convolutionBackward(dconv9_2, conv9_1, f9[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv9_1[conv9_1<=0] = 0\n",
    "                conc_dconv9, df9_1, db9_1 = convolutionBackward(dconv9_1, c9, f9[0], conv_s) #C9 is not needed for input,we know how to select the right gradients\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv9, dconv1_2 = crop2half(conc_dconv9)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #conv8_2 is not needed for input,we know how to select the right gradients            \n",
    "                #dconv1_2 = reshape(dconv1_2, conv1_2.shape[1])\n",
    "                \n",
    "                dconv8_2, df9_dc, db9_dc = convTranspBackward(dconv9, new_in9, fb9_dc[0],conv_s)\n",
    "                #pack data\n",
    "\n",
    "                dconv8_2[conv8_2<=0] = 0\n",
    "                dconv8_1, df8_2, db8_2 = convolutionBackward(dconv8_2, conv8_1, f8[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv8_1[conv8_1<=0] = 0\n",
    "                conc_dconv8, df8_1, db8_1 = convolutionBackward(dconv8_1, c8, f8[0], conv_s) #\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv8, dconv2_2 = crop2half(conc_dconv8)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #dconv2_2 = reshape(dconv2_2, conv2_2.shape[1])\n",
    "                \n",
    "                dconv7_2, df8_dc, db8_dc = convTranspBackward(dconv8, new_in8, fb8_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                dconv7_2[conv7_2<=0] = 0\n",
    "                dconv7_1, df7_2, db7_2 = convolutionBackward(dconv7_2, conv7_1, f7[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv7_1[conv7_1<=0] = 0\n",
    "                conc_dconv7, df7_1, db7_1 = convolutionBackward(dconv7_1, c7, f7[0], conv_s) #\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv7, dconv3_2 = crop2half(conc_dconv7)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #Make sure that dconv3_2 is the same dim with the dconv3_2 that will come from maxpool in decoding side\n",
    "                #dconv3_2 = reshape(dconv3_2, conv3_2.shape[1])\n",
    "                \n",
    "                dconv6_2, df7_dc, db7_dc = convTranspBackward(dconv7, new_in7, fb7_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                dconv6_2[conv6_2<=0] = 0\n",
    "                dconv6_1, df6_2, db6_2 = convolutionBackward(dconv6_2, conv6_1, f6[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv7_1[conv7_1<=0] = 0\n",
    "                conc_dconv6, df6_1, db6_1 = convolutionBackward(dconv6_1, c6, f6[0], conv_s) #\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv6, dconv4_2 = crop2half(conc_dconv6)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #dconv4_2 = reshape(dconv4_2, conv4_2.shape[1])\n",
    "                \n",
    "                dconv5_2, df6_dc, db6_dc = convTranspBackward(dconv6, new_in6, fb6_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                dconv5_2[conv5_2<=0] = 0\n",
    "                dconv5_1, df5_2, db5_2 = convolutionBackward(dconv5_2, conv5_1, f5[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv5_1[conv5_1<=0] = 0\n",
    "                dpl4, df5_1, db5_1 = convolutionBackward(dconv5_1, pl4, f5[0], conv_s) #\n",
    "                \n",
    "                dconv4_2 += maxpoolBackward(dpl4, conv4_2, f=2 , s=2) #Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                dconv4_2[conv4_2<=0] = 0\n",
    "                dconv4_1, df4_2, db4_2 = convolutionBackward(dconv4_2, conv4_1, f4[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv4_1[conv4_1<=0] = 0\n",
    "                dpl3, df4_1, db4_1 = convolutionBackward(dconv4_1, pl3, f4[0], conv_s) #\n",
    "\n",
    "                dconv3_2 += maxpoolBackward(dpl3, conv3_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                dconv3_2[conv3_2<=0] = 0\n",
    "                dconv3_1, df3_2, db3_2 = convolutionBackward(dconv3_2, conv3_1, f3[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv3_1[conv3_1<=0] = 0\n",
    "                dpl2, df3_1, db3_1 = convolutionBackward(dconv3_1, pl2, f3[0], conv_s) #\n",
    "                \n",
    "                dconv2_2 += maxpoolBackward(dpl2, conv2_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                dconv2_2[conv2_2<=0] = 0\n",
    "                dconv2_1, df2_2, db2_2 = convolutionBackward(dconv2_2, conv2_1, f2[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv2_1[conv2_1<=0] = 0\n",
    "                dpl1, df2_1, db2_1 = convolutionBackward(dconv2_1, pl1, f2[0], conv_s) #\n",
    "                \n",
    "                dconv1_2 += maxpoolBackward(dpl1, conv1_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                dconv1_2[conv1_2<=0] = 0\n",
    "                dconv1_1, df1_2, db1_2 = convolutionBackward(dconv1_2, conv1_1, f1[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv1_1[conv1_1<=0] = 0\n",
    "                _, df1_1, db1_1 = convolutionBackward(dconv1_1, X_t[b], f1[0], conv_s) #\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                [df1,df2,df3,df4,df5,df6,df7,df8,df9] = df\n",
    "                [db1,db2,db3,db4,db5,db6,db7,db8,db9] = db \n",
    "                [dfb6_dc,dfb7_dc,dfb8_dc,dfb9_dc]     = dfb\n",
    "\n",
    "                \n",
    "                df1[0] += df1_1\n",
    "                df1[1] += df1_2\n",
    "                df2[0] += df2_1\n",
    "                df2[1] += df2_2\n",
    "                df3[0] += df3_1\n",
    "                df3[1] += df3_2\n",
    "                df4[0] += df4_1\n",
    "                df4[1] += df4_2\n",
    "                df5[0] += df5_1\n",
    "                df5[1] += df5_2\n",
    "                df6[0] += df6_1\n",
    "                df6[1] += df6_2\n",
    "                df7[0] += df7_1\n",
    "                df7[1] += df7_2\n",
    "                df8[0] += df8_1\n",
    "                df8[1] += df8_2\n",
    "                df9[0] += df9_1\n",
    "                df9[1] += df9_2\n",
    "                \n",
    "                db1[0] += db1_1\n",
    "                db1[1] += db1_2\n",
    "                db2[0] += db2_1\n",
    "                db2[1] += db2_2\n",
    "                db3[0] += db3_1\n",
    "                db3[1] += db3_2\n",
    "                db4[0] += db4_1\n",
    "                db4[1] += db4_2\n",
    "                db5[0] += db5_1\n",
    "                db5[1] += db5_2\n",
    "                db6[0] += db6_1\n",
    "                db6[1] += db6_2\n",
    "                db7[0] += db7_1\n",
    "                db7[1] += db7_2\n",
    "                db8[0] += db8_1\n",
    "                db8[1] += db8_2\n",
    "                db9[0] += db9_1\n",
    "                db9[1] += db9_2\n",
    "\n",
    "                dfb6_dc[0] += df6_dc\n",
    "                dfb6_dc[1] += db6_dc\n",
    "                dfb7_dc[0] += df7_dc\n",
    "                dfb7_dc[1] += db7_dc\n",
    "                dfb8_dc[0] += df8_dc\n",
    "                dfb8_dc[1] += db8_dc\n",
    "                dfb9_dc[0] += df9_dc\n",
    "                dfb9_dc[1] += db9_dc\n",
    "\n",
    "                dout_f += dout_f_\n",
    "                dout_b += dout_b_\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ############## Adam Optimization ################\n",
    "            #changing the main structures(which are also updated)\n",
    "            #TODO: Future update - remove uneccessary memory loads/stores, v,s dont need to be saved \n",
    "            for i in range(len(filters)):\n",
    "                v_adam[i][0] = beta1*v_adam[i][0] + (1-beta1)*df[i][0]/batch_size #f1\n",
    "                s_adam[i][0] = beta2*s_adam[i][0] + (1-beta2)*(df[i][0]/batch_size)**2 #f1\n",
    "                filters[i][0] -= lr*v_adam[i][0]/np.sqrt(s_adam[i][0] + 1e-7)\n",
    "\n",
    "                v_adam[i][1] = beta1*v_adam[i][1] + (1-beta1)*df[i][1]/batch_size #f2\n",
    "                s_adam[i][1] = beta2*s_adam[i][1] + (1-beta2)*(df[i][1]/batch_size)**2  #f2\n",
    "                filters[i][1] -= lr*v_adam[i][1]/np.sqrt(s_adam[i][1] + 1e-7)\n",
    "\n",
    "            for i in range(len(bias)):\n",
    "                bv_adam[i][0] = beta1*bv_adam[i][0] + (1-beta1)*db[i][0]/batch_size #b1\n",
    "                bs_adam[i][0] = beta2*bs_adam[i][0] + (1-beta2)*(db[i][0]/batch_size)**2  #b1\n",
    "                bias[i][0] -= lr*bv_adam[i][0]/np.sqrt(bs_adam[i][0] + 1e-7)\n",
    "\n",
    "                bv_adam[i][1] = beta1*bv_adam[i][1] + (1-beta1)*db[i][1]/batch_size #b2\n",
    "                bs_adam[i][1] = beta2*bs_adam[i][1] + (1-beta2)*(db[i][1]/batch_size)**2  #b2\n",
    "                bias[i][1] -= lr*bv_adam[i][1]/np.sqrt(bs_adam[i][1] + 1e-7)\n",
    "\n",
    "            for i in range(len(f_dc)):\n",
    "                fdc_v_adam[i][0] = beta1*fdc_v_adam[i][0] + (1-beta1)*dfb[i][0]/batch_size #f1\n",
    "                fdc_s_adam[i][0] = beta2*fdc_s_adam[i][0] + (1-beta2)*(dfb[i][0]/batch_size)**2  #f1\n",
    "                f_dc[i][0] -= lr*fdc_v_adam[i][0]/np.sqrt(fdc_s_adam[i][0] + 1e-7)\n",
    "\n",
    "                fdc_v_adam[i][1] = beta1*fdc_v_adam[i][1] + (1-beta1)*dfb[i][1]/batch_size #b2\n",
    "                fdc_s_adam[i][1] = beta2*fdc_s_adam[i][1] + (1-beta2)*(dfb[i][1]/batch_size)**2  #b2\n",
    "                f_dc[i][1] -= lr*fdc_v_adam[i][1]/np.sqrt(fdc_s_adam[i][1] + 1e-7)    \n",
    "\n",
    "            v_out_f = beta1*v_out_f + (1 - beta1)*dout_f/batch_size #f\n",
    "            s_out_f = beta2*s_out_f + (1 - beta2)*(dout_f/batch_size)**2  #f\n",
    "            out_fb[0] -= lr*v_out_f/np.sqrt(s_out_f + 1e-7)\n",
    "\n",
    "            bv_out_b = beta1*bv_out_b + (1 - beta1)*dout_b/batch_size #b\n",
    "            bs_out_b = beta2*bs_out_b + (1 - beta2)*(dout_b/batch_size)**2  #b\n",
    "            out_fb[1] -= lr*bv_out_b/np.sqrt(bs_out_b + 1e-7)\n",
    "\n",
    "            \n",
    "            '''\n",
    "            for i in range(len(filters)):\n",
    "                filters[i][0] -= lr*df[i][0]\n",
    "                bias[i][0] -= lr*db[i][0]\n",
    "            \n",
    "            for i in range(len(f_dc)):\n",
    "                f_dc[i][0] -= lr*dfb[i][0]\n",
    "            \n",
    "            out_fb[0] -= lr*dout_f\n",
    "            out_fb[1] -= lr*dout_b\n",
    "            \n",
    "            #Updating Conv Part\n",
    "            \n",
    "\n",
    "            #print(Yh)\n",
    "            #print(t)\n",
    "            #print(\"Batch:{}\".format(c+12))\n",
    "            \n",
    "           \n",
    "            f1 -= (learning_rate) * (df1/batch)\n",
    "            f2 -= (learning_rate) * (df2/batch)\n",
    "            b1 -= (learning_rate) * (db1/batch)\n",
    "            b2 -= (learning_rate) * (db2/batch)\n",
    "            filter_params = [f1, f2, b1, b2]\n",
    "                \n",
    "            # updating FC Part\n",
    "            #params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "            grads_values['dW1'] = dW1#/(batch)\n",
    "            grads_values['dW2'] = dW2#/(batch)\n",
    "            grads_values['db1'] = dB1#/(batch)\n",
    "            grads_values['db2'] = dB2#/(batch)\n",
    "                   \n",
    "            for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "                \n",
    "                params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "                params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "            \n",
    "            '''\n",
    "            \n",
    "        #END OF LOOP - EPOCH\n",
    "        #timestamp2 = time.time()\n",
    "        #print (\"This took %.2f seconds\" %(timestamp2 - timestamp1))\n",
    "        if(verbose):\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/batch, (accuracy*100)/batch))\n",
    "        #if(callback is not None):\n",
    "        #    callback(i, params_values)\n",
    "    #pack filters\n",
    "    #params_values = [filters, bias, f_dc, out_fb]\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: {1}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:     1   -   cost: 0.75   -   Accuracy: 47.06%\n",
      "Epoch: {2}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     2   -   cost: 0.75   -   Accuracy: 47.04%\n",
      "Epoch: {3}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     3   -   cost: 0.75   -   Accuracy: 47.03%\n",
      "Epoch: {4}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     4   -   cost: 0.75   -   Accuracy: 47.02%\n",
      "Epoch: {5}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     5   -   cost: 0.75   -   Accuracy: 47.01%\n",
      "Epoch: {6}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     6   -   cost: 0.76   -   Accuracy: 46.99%\n",
      "Epoch: {7}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     7   -   cost: 0.76   -   Accuracy: 46.97%\n",
      "Epoch: {8}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     8   -   cost: 0.76   -   Accuracy: 46.94%\n",
      "Epoch: {9}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     9   -   cost: 0.76   -   Accuracy: 46.88%\n",
      "Epoch: {10}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    10   -   cost: 0.76   -   Accuracy: 46.76%\n",
      "Epoch: {11}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    11   -   cost: 0.77   -   Accuracy: 46.23%\n",
      "Epoch: {12}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    12   -   cost: 0.82   -   Accuracy: 43.94%\n",
      "Epoch: {13}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    13   -   cost: 0.90   -   Accuracy: 40.67%\n",
      "Epoch: {14}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    14   -   cost: 0.94   -   Accuracy: 39.03%\n",
      "Epoch: {15}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    15   -   cost: 0.94   -   Accuracy: 38.99%\n",
      "Epoch: {16}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    16   -   cost: 0.95   -   Accuracy: 38.61%\n",
      "Epoch: {17}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    17   -   cost: 0.95   -   Accuracy: 38.52%\n",
      "Epoch: {18}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    18   -   cost: 0.95   -   Accuracy: 38.64%\n",
      "Epoch: {19}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    19   -   cost: 0.95   -   Accuracy: 38.77%\n",
      "Epoch: {20}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    20   -   cost: 0.93   -   Accuracy: 39.35%\n",
      "Epoch: {21}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    21   -   cost: 0.90   -   Accuracy: 40.52%\n",
      "Epoch: {22}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    22   -   cost: 0.86   -   Accuracy: 42.47%\n",
      "Epoch: {23}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    23   -   cost: 0.80   -   Accuracy: 45.08%\n",
      "Epoch: {24}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    24   -   cost: 0.75   -   Accuracy: 47.29%\n",
      "Epoch: {25}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    25   -   cost: 0.77   -   Accuracy: 46.51%\n",
      "Epoch: {26}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    26   -   cost: 0.74   -   Accuracy: 47.78%\n",
      "Epoch: {27}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    27   -   cost: 0.77   -   Accuracy: 46.48%\n",
      "Epoch: {28}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    28   -   cost: 0.72   -   Accuracy: 48.83%\n",
      "Epoch: {29}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    29   -   cost: 0.70   -   Accuracy: 49.65%\n",
      "Epoch: {30}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    30   -   cost: 0.71   -   Accuracy: 49.14%\n",
      "Epoch: {31}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    31   -   cost: 0.67   -   Accuracy: 51.07%\n",
      "Epoch: {32}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    32   -   cost: 0.71   -   Accuracy: 49.27%\n",
      "Epoch: {33}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    33   -   cost: 0.64   -   Accuracy: 52.51%\n",
      "Epoch: {34}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    34   -   cost: 0.67   -   Accuracy: 51.27%\n",
      "Epoch: {35}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    35   -   cost: 0.59   -   Accuracy: 55.65%\n",
      "Epoch: {36}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    36   -   cost: 0.53   -   Accuracy: 58.81%\n",
      "Epoch: {37}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    37   -   cost: 0.50   -   Accuracy: 60.53%\n",
      "Epoch: {38}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    38   -   cost: 0.48   -   Accuracy: 61.71%\n",
      "Epoch: {39}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    39   -   cost: 0.51   -   Accuracy: 59.88%\n",
      "Epoch: {40}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    40   -   cost: 0.48   -   Accuracy: 62.09%\n",
      "Epoch: {41}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    41   -   cost: 0.57   -   Accuracy: 56.68%\n",
      "Epoch: {42}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    42   -   cost: 0.47   -   Accuracy: 62.55%\n",
      "Epoch: {43}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    43   -   cost: 0.55   -   Accuracy: 57.42%\n",
      "Epoch: {44}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    44   -   cost: 0.47   -   Accuracy: 62.34%\n",
      "Epoch: {45}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    45   -   cost: 0.48   -   Accuracy: 61.65%\n",
      "Epoch: {46}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    46   -   cost: 0.45   -   Accuracy: 63.56%\n",
      "Epoch: {47}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    47   -   cost: 0.46   -   Accuracy: 62.91%\n",
      "Epoch: {48}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    48   -   cost: 0.37   -   Accuracy: 69.21%\n",
      "Epoch: {49}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    49   -   cost: 0.32   -   Accuracy: 72.74%\n",
      "Epoch: {50}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    50   -   cost: 0.34   -   Accuracy: 70.91%\n",
      "Epoch: {51}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New parameters Saved!\n",
      "Epoch:    51   -   cost: 0.30   -   Accuracy: 73.77%\n",
      "Epoch: {52}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    52   -   cost: 0.31   -   Accuracy: 73.62%\n",
      "Epoch: {53}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    53   -   cost: 0.38   -   Accuracy: 68.45%\n",
      "Epoch: {54}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    54   -   cost: 0.35   -   Accuracy: 70.57%\n",
      "Epoch: {55}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    55   -   cost: 0.39   -   Accuracy: 67.41%\n",
      "Epoch: {56}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    56   -   cost: 0.79   -   Accuracy: 45.56%\n",
      "Epoch: {57}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    57   -   cost: 0.51   -   Accuracy: 60.21%\n",
      "Epoch: {58}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    58   -   cost: 0.85   -   Accuracy: 42.58%\n",
      "Epoch: {59}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    59   -   cost: 0.43   -   Accuracy: 65.19%\n",
      "Epoch: {60}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    60   -   cost: 0.86   -   Accuracy: 42.41%\n",
      "Epoch: {61}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    61   -   cost: 0.87   -   Accuracy: 42.01%\n",
      "Epoch: {62}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    62   -   cost: 0.86   -   Accuracy: 42.36%\n",
      "Epoch: {63}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    63   -   cost: 0.49   -   Accuracy: 61.08%\n",
      "Epoch: {64}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    64   -   cost: 0.51   -   Accuracy: 60.19%\n",
      "Epoch: {65}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    65   -   cost: 0.39   -   Accuracy: 67.70%\n",
      "Epoch: {66}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    66   -   cost: 0.78   -   Accuracy: 45.77%\n",
      "Epoch: {67}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    67   -   cost: 0.58   -   Accuracy: 55.74%\n",
      "Epoch: {68}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    68   -   cost: 0.55   -   Accuracy: 57.85%\n",
      "Epoch: {69}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    69   -   cost: 0.85   -   Accuracy: 42.67%\n",
      "Epoch: {70}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    70   -   cost: 0.86   -   Accuracy: 42.47%\n",
      "Epoch: {71}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    71   -   cost: 0.77   -   Accuracy: 46.26%\n",
      "Epoch: {72}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    72   -   cost: 0.58   -   Accuracy: 56.24%\n",
      "Epoch: {73}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    73   -   cost: 0.77   -   Accuracy: 46.17%\n",
      "Epoch: {74}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    74   -   cost: 0.82   -   Accuracy: 43.83%\n",
      "Epoch: {75}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    75   -   cost: 0.74   -   Accuracy: 47.50%\n",
      "Epoch: {76}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    76   -   cost: 0.65   -   Accuracy: 52.44%\n",
      "Epoch: {77}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    77   -   cost: 0.55   -   Accuracy: 57.55%\n",
      "Epoch: {78}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    78   -   cost: 0.58   -   Accuracy: 55.99%\n",
      "Epoch: {79}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    79   -   cost: 0.68   -   Accuracy: 50.61%\n",
      "Epoch: {80}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    80   -   cost: 0.69   -   Accuracy: 50.22%\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'params_values' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-47758beaac36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mparams_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.008\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#0.05 stable LR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-7727cc805460>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, Y, epochs, learning_rate, dropout, verbose, callback)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;31m#pack filters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[1;31m#params_values = [filters, bias, f_dc, out_fb]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 564\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'params_values' referenced before assignment"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### Training ######\n",
    "#train_images, train_labels, test_images, test_labels\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "params_values = train(train_images, train_labels, 80, 0.008, True) #0.05 stable LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Forward step . . .\n",
      "Cost: 0.54   -   Accuracy: 58.38%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Validate(train_images[0:1,:,:,:], train_labels[0:1,:,:,:], params_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validate(test_images[1:2,:,:,:], test_labels[1:2,:,:,:], params_values);"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
