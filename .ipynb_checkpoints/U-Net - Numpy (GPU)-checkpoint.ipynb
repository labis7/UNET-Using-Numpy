{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "#import torch\n",
    "#import numpy as nump\n",
    "import cupy as np\n",
    "#import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load from /home/USER/data/mnist or elsewhere; download if missing.\"\"\"\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from matplotlib import image\n",
    "\n",
    "def mnist(path=None):\n",
    "    r\"\"\"Return (train_images, train_labels, test_images, test_labels).\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory containing MNIST. Default is\n",
    "            /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist.\n",
    "            Create if nonexistant. Download any missing files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, train_labels, test_images, test_labels), each\n",
    "            a matrix. Rows are examples. Columns of images are pixel values.\n",
    "            Columns of labels are a onehot encoding of the correct class.\n",
    "\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz',\n",
    "             'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz',\n",
    "             't10k-labels-idx1-ubyte.gz']\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        # Set path to /home/USER/data/mnist or C:\\Users\\USER\\data\\salt\n",
    "        path = os.path.join(os.path.expanduser('~/'), 'data', 'salt')\n",
    "\n",
    "    # Create path if it doesn't exist\n",
    "    #os.makedirs(path, exist_ok=True)\n",
    "    \"\"\"\"\n",
    "    # Download any missing files\n",
    "    for file in files:\n",
    "        if file not in os.listdir(path):\n",
    "            urlretrieve(url + file, os.path.join(path, file))\n",
    "            print(\"Downloaded %s to %s\" % (file, path))\n",
    "    print(\"All files are ready to gzip!\")\n",
    "    \"\"\"\n",
    "    def _images(path):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        folder = path + \"/images/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(128, 128)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,128,128).astype('float32')#/255\n",
    "        return pixels[:1,:,:,:]\n",
    "\n",
    "    def _labels(path):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        \n",
    "        folder = path + \"/labels/\"\n",
    "        onlyfiles = [cv2.resize(image.imread(folder+f),(128, 128)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,128,128).astype('float32') #/255\n",
    "        return pixels[:1,:,:,:]\n",
    "    print(\"Train Images : Loading . . .\")\n",
    "    train_images = _images(path)\n",
    "    print(\"Train Labels : Loading . . .\")\n",
    "    train_labels = _labels(path)\n",
    "    \"\"\"\"\n",
    "    print(\"Test Images  : Loading . . .\")\n",
    "    test_images = _images(os.path.join(path, files[2]))\n",
    "    print(\"Test Labels  : Loading . . .\")\n",
    "    test_labels = _labels(os.path.join(path, files[3])) \n",
    "    \"\"\"\n",
    "    print(\"Done!\")\n",
    "    return train_images , train_labels #, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images : Loading . . .\n",
      "Train Labels : Loading . . .\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels= mnist()  #, test_images, test_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 128, 128)\n",
      "(1, 1, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "ch = 1 #Number of channels\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape) # ONE-HOT !!!\n",
    "#print(train_images.T.shape)\n",
    "#print(train_labels.reshape((train_images.shape[0], 10)).shape)\n",
    "\n",
    "##### Getting Ready for the Conv Layers #####\n",
    "#train_images = train_images.reshape(train_images.shape[0], ch, 28, 28).squeeze()\n",
    "#test_images = test_images.reshape(test_images.shape[0], ch, 28, 28).squeeze()\n",
    "\n",
    "#############################################\n",
    "\n",
    "#y = train_images[1].reshape(1,28,28)\n",
    "#print(y.squeeze().shape)\n",
    "'''\n",
    "temp = []\n",
    "#train_labels = train_labels.sum(1)\n",
    "for i in range(int(len(train_labels[:]))):\n",
    "    temp.append(list(train_labels[i][:]).index(1))\n",
    "    \n",
    "    \n",
    "train_labels = np.array(temp.copy())\n",
    "#print(train_labels[0:5])\n",
    "'''\n",
    "#plt.imshow(train_images[0].squeeze(), cmap='Greys_r');\n",
    "pass\n",
    "#print(train_labels[:,:,:30,:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(train_images[0].squeeze(), cmap='Greys_r');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Convolution Architecture - Downsampling/Upsampling  ##################\n",
    "\n",
    "def init_filters(layers, init_n_f ):\n",
    "    filters = []\n",
    "    bias = []\n",
    "    f_dc = []\n",
    "    trim = 0.1\n",
    "    n_f = init_n_f #initial number of filters/kernels\n",
    "    ch_in = 1      #input channels\n",
    "    for i in range(layers):\n",
    "        if(i != 0):\n",
    "            n_f = n_f*2 #16,32,64,128,256\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trim\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trim\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trim\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trim\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "    \n",
    "    #Deconvolution filters    \n",
    "    for i in range(1,layers):\n",
    "        n_f = n_f//2 #128,64,32,16\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        fdc = np.random.randn(n_f,ch_in,2,2)*trim #upsampling filter, its result will be conc with conv4 output so the channels will be doubled again\n",
    "        bdc = np.random.randn(fdc.shape[0],1)* trim\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trim\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trim\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trim\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trim\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        dc = [fdc, bdc]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "        f_dc.append(dc)\n",
    "        \n",
    "    \n",
    "    return filters, bias, f_dc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation Functions ###\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UTILITIES #############\n",
    "import numpy as nump\n",
    "\n",
    "def conv(image, params, s = 1, pad = 1 ): # s = 1 (conv stride)\n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    [f, b] = params\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((image.shape[0],image.shape[1]+2*pad ,image.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = image\n",
    "        image = tmp    \n",
    "        \n",
    "    f_num = f.shape[0]\n",
    "    f_depth = f.shape[1] #f1 = 1 , f2 = 8\n",
    "    f_size = f.shape[2] \n",
    "        \n",
    "    print(\"New Conv\")\n",
    "    h_range = int((image.shape[1] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    print(f_num*w_range*h_range)\n",
    "    np_o = np.zeros((f_num, h_range, w_range))\n",
    "    #np.cuda.Stream.null.synchronize()\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                np_o[z, _h, _w] = calc(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] , f[z, :, :, :],b[z,0])#(np.sum(np.multiply(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] , f[z, :, :, :])) + b[z])[0]\n",
    "\n",
    "    \n",
    "    return np_o\n",
    "def calc(img,f,b):\n",
    "    timestamp1 = time.time()\n",
    "    res = np.multiply(img , f)\n",
    "    res = np.sum(res)\n",
    "    res += b\n",
    "    \n",
    "    timestamp2 = time.time()\n",
    "    print (\"This took %.10f seconds\" %(timestamp2 - timestamp1))\n",
    "    return res\n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s = 1, pad = 1 ):\n",
    "    f_num, n_c, f, _ = filt.shape\n",
    "    \n",
    "    _ ,h , w = dconv_prev.shape\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((conv_in.shape[0],conv_in.shape[1]+2*pad ,conv_in.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = conv_in\n",
    "        conv_in = tmp\n",
    "    \n",
    "    dconv_in = np.zeros(conv_in.shape)\n",
    "    db = np.zeros((f_num,1))\n",
    "\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                #each entry of the dconv_prev will try to affect the idxs from which was made of.\n",
    "                dfilt[z] += dconv_prev[z, _h, _w] * conv_in[:, _h*s : _h*s + f, _w*s : _w*s + f]\n",
    "                dconv_in[:, _h*s : _h*s + f, _w*s : _w*s + f] += dconv_prev[z, _h, _w] * filt[z]  \n",
    "        db[z] = np.sum(dconv_prev[z])  #, axis =1) ## AXIS?\n",
    "    \n",
    "    if(pad >=1 ):\n",
    "        dconv_in = dconv_in[:, pad:-pad, pad:-pad]  # Cropping\n",
    "        \n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "\n",
    "def convTransp(image, params, s = 1, pad = 1):\n",
    "    #s is always 1, upsample kernel = 2\n",
    "    #zero insertions between pixels s_downsampled -1 = 2-1 = 1\n",
    "    #required padding in order to double my dimensions with the given data:\n",
    "    #(i-1)*2 + k -2p = output size, where our padding is k - p -1 = 2-0-1=1(we assume p=0)\n",
    "    input_s = image.shape[1]\n",
    "    target_dim = input_s*2 #final dim, after conv\n",
    "    required_pad = 1 #always for filter 2x2 ,stride 1, zero insertion 1 and main target to double dim\n",
    "    #make our new custom input\n",
    "    size = input_s*2 +1\n",
    "    new_in = np.zeros((image.shape[0], size, size))\n",
    "    for i in range(1, target_dim, 2):\n",
    "        for j in range(1, target_dim, 2):\n",
    "                new_in[:, i, j] = image[:, i//2, j//2]\n",
    "    #now we do a normal convolution(pad = 0)\n",
    "    res = conv(new_in, params, 1, 0) #thats the final result with target_dim\n",
    "    return res, new_in # we will need new_in so we wont calc it again for the backprop\n",
    "    \n",
    "def convTranspBackward(dconv_prev, new_in, filt, s = 1):\n",
    "    #First, we do a backward convolution on new_in,d_conv_prev,\n",
    "    #then we will choose form the d_conv_new the entries that match the initial 'smaller' input image\n",
    "    #by selecting the odd matrix cells 1,3,5... because we had applied a standard pad=1,zero inser=1\n",
    "    dconv_in, dfilt, db = convolutionBackward(dconv_prev, new_in, filt,1,0)\n",
    "    #Now its time to choose the right entries to build the gradients of the initial input image\n",
    "    dim = dconv_in.shape[1]\n",
    "    final_dim = (new_in.shape[1] - 2)//2 + 1 #based on dimen of image before final conv that gives the result,..\n",
    "    #e.g. for new_in 7x7 that is going to convoluted with a 2x2 kernel and give a 6x6 upsampled from 3x3 init image\n",
    "    # now from this 7x7 --> apply the formula above,we get the 3x3 dimension number\n",
    "    res = np.zeros((dconv_in.shape[0], final_dim, final_dim))\n",
    "    for i in range(1, dim, 2):\n",
    "        for j in range(1, dim, 2):\n",
    "                res[:, i//2, j//2] = dconv_in[:, i, j]\n",
    "    return res, dfilt, db\n",
    "    \n",
    "    \n",
    "def maxpool(image, f=2 , s=2):\n",
    "    \n",
    "    h_range = int((image.shape[1] - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f) / s) + 1     # (W - F + 2P) / S \n",
    "    out = np.zeros((image.shape[0], h_range, w_range))\n",
    "    \n",
    "    for z in range(image.shape[0]): # Number of channels\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                out[z, _h, _w] = np.max(image[z, _h*s : _h*s + f, _w*s : _w*s + f])\n",
    "    return out\n",
    "\n",
    "def nanargmax(arr):\n",
    "    #print(arr.shape)\n",
    "    try:\n",
    "        idx = np.nanargmax(arr)\n",
    "        #print (idx)\n",
    "    except:\n",
    "        idx = 0\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "def maxpoolBackward(dpool, conv, f=2 , s=2):\n",
    "    num_c, h, w = conv.shape\n",
    "    h = int((h - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w = int((w - f) / s) + 1     # (W - F + 2P) / S \n",
    "    \n",
    "    dout = np.zeros(conv.shape)\n",
    "    #print(conv.shape)\n",
    "    for z in range(num_c): # Number of channels\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                (a, b) = nanargmax(conv[z, _h*s : _h*s + f, _w*s : _w*s + f]) #Getting the indexes from the max value in this area\n",
    "                #put it on the new array\n",
    "                dout[z, _h + a, _w + b] = dpool[z, _h, _w]\n",
    "    \n",
    "    \n",
    "    return dout\n",
    "\n",
    "\n",
    "def reshape(img, reshape_dim):\n",
    "    pad = reshape_dim - img.shape[1]\n",
    "    if(pad == 0):\n",
    "        return img\n",
    "    res = np.zeros((img.shape[0], reshape_dim, reshape_dim))\n",
    "    if(pad > 1):\n",
    "        res[:, pad//2:-(pad//2), pad//2:-(pad//2)] = img\n",
    "    else:\n",
    "        res[:, 0:-(pad), 0:-(pad)] = img\n",
    "    return res\n",
    "\n",
    "\n",
    "def crop(img, crop_dim): #TODO : look at it..future upgrade to downsample..maybe\n",
    "    start = img.shape[1]//2 - crop_dim//2\n",
    "    return img[:,(start):(start +crop_dim),(start):(start +crop_dim)]\n",
    "\n",
    "    \n",
    "    \n",
    "def crop2half(img):\n",
    "    #return gradients for decoder side and gradients for encoder side\n",
    "    n_ch = img.shape[0]//2\n",
    "    return img[n_ch:n_ch*2 ,:,:], img[0:n_ch,:,:]\n",
    "    \n",
    "def concat(img1_true, img2):\n",
    "    n_ch = img1_true.shape[0]\n",
    "    dim = img1_true.shape[1]\n",
    "    if(img2.shape[1] != dim):\n",
    "        img2 = crop(img2, dim)\n",
    "        print(\"Warning: Extra crop needed and handled!(%d --> %d)\" %(dim, img2.shape[1]))\n",
    "    res = np.zeros((n_ch*2, dim, dim))\n",
    "    res[0:n_ch,:,:] = img2\n",
    "    res[n_ch:n_ch*2 ,:,:] = img1_true\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cost Calculations ######\n",
    "def NLLLoss(logs, targets):\n",
    "    out = np.zeros(logs.shape)\n",
    "    mylen = logs.shape[0]*logs.shape[1]*logs.shape[1]\n",
    "    #print(len(targets))\n",
    "    #logs.shape (dim x dim) like target\n",
    "    for i in range(logs.shape[1]):\n",
    "        for j in range(logs.shape[2]):\n",
    "            if(targets[:,i,j] == 1):\n",
    "                out[:,i,j] = logs[:,i,j] #in that case the propab. is correct with targen being the 1\n",
    "            else:\n",
    "                out[:,i,j] = 1 - logs[:,i,j] # e.g if logs[i,j]= 0.4 and we want(target) 0 --> 1-0.4=0.6 prob. for zero \n",
    "    #or\n",
    "    #out = targets*logs + ((-1*(targets-1)) - (-1*(targets-1))*logs)\n",
    "    return -np.log(out.sum()/mylen)\n",
    "\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    \n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost) ### ATTENTION!\n",
    "\n",
    "\n",
    "###### Accuracy Calculation ######\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  TRAIN  ######\n",
    "#import torch\n",
    "import time\n",
    "time.time()\n",
    "\n",
    "\n",
    "def train(X, Y, epochs, learning_rate, dropout, verbose=True, callback=None):\n",
    "    # initiation of neural net parameters\n",
    "    #params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    \n",
    "    #f1,f2,f3,f4,f5,fd1,fd2,fd3,fd4\n",
    "    filters,bias, f_dc = init_filters(5,16) #Double the channel-filter 4 times  (up to 256 and back again)\n",
    "    np.cuda.Stream.null.synchronize()\n",
    "    ##Final 1x1 filter\n",
    "    out_f = np.random.randn(1,16,1,1)*0.1\n",
    "    out_b = np.random.randn(out_f.shape[0],1)*0.1   \n",
    "    out_fb = [out_f, out_b]\n",
    "    #filter shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    \n",
    "    v_adam =  [] #filter1,filter2\n",
    "    s_adam =  [] \n",
    "    bv_adam = [] #bias1,bias2\n",
    "    bs_adam = []\n",
    "    fdc_v_adam=[] #filter,bias\n",
    "    fdc_s_adam=[]\n",
    "    #format: [v1,v2,v3,v4,v5,vd1,vd2,vd3,vd4] ,same for the rest, each of these include a tuple for the 2 conv filter\n",
    "    #[s1,s2,s3,s4,s5,sd1,sd2,vs3,sd4]\n",
    "    # upsampling filters : [v1_dc,v2_dc,v3_dc]  \n",
    "\n",
    "            \n",
    "\n",
    "    # performing calculations for subsequent iterations\n",
    "    \n",
    "    \n",
    "    [f1,f2,f3,f4,f5,f6,f7,f8,f9] = filters\n",
    "    [b1,b2,b3,b4,b5,b6,b7,b8,b9]= bias \n",
    "    [fb6_dc, fb7_dc, fb8_dc, fb9_dc] = f_dc\n",
    "    \n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # step forward\n",
    "        running_loss = 0\n",
    "        accuracy = 0\n",
    "        batch = 1\n",
    "        print(\"Epoch: {%d}\" %(e+1))\n",
    "        for c in range(0, X.shape[0], batch):\n",
    "            print('Batch: {}'.format(int(c/batch +1)))\n",
    "            #### Reset Gradients (Every batch) ####\n",
    "            beta1= 0.95\n",
    "            beta2= 0.99\n",
    "            lr = learning_rate\n",
    "            batch_size= batch\n",
    "            if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "                batch = X.shape[0] - c\n",
    "            X_t = X[c:(c + batch)]  # shape:(m, ch, h, w)\n",
    "            Y_t = Y[c:(c + batch)]\n",
    "            cost = 0\n",
    "            accuracy = 0\n",
    "            \n",
    "            df =  []\n",
    "            db =  []\n",
    "            dfb=  []\n",
    "            for i in filters:\n",
    "                v1 = np.zeros(i[0].shape)\n",
    "                v2 = np.zeros(i[1].shape)\n",
    "                s1 = np.zeros(i[0].shape)\n",
    "                s2 = np.zeros(i[1].shape)\n",
    "                v_a = [v1, v2]\n",
    "                s_a = [s1, s2]\n",
    "                v_adam.append(v_a)\n",
    "                s_adam.append(s_a)\n",
    "                \n",
    "                df1_t = np.zeros(i[0].shape)\n",
    "                df2_t = np.zeros(i[1].shape)\n",
    "                f_temp = [df1_t, df2_t]\n",
    "                df.append(f_temp)\n",
    "                \n",
    "            for i in bias:\n",
    "                bv1 = np.zeros(i[0].shape)\n",
    "                bv2 = np.zeros(i[1].shape)\n",
    "                bs1 = np.zeros(i[0].shape)\n",
    "                bs2 = np.zeros(i[1].shape)    \n",
    "                bv_a = [bv1, bv2]\n",
    "                bs_a = [bs1, bs2]\n",
    "                bv_adam.append(bv_a)\n",
    "                bs_adam.append(bs_a)\n",
    "                \n",
    "                \n",
    "                db1_t = np.zeros(i[0].shape)\n",
    "                db2_t = np.zeros(i[1].shape)\n",
    "                b_temp = [db1_t, db2_t]\n",
    "                db.append(b_temp)\n",
    "            \n",
    "            for i in f_dc:\n",
    "                fdc_v1 = np.zeros(i[0].shape)\n",
    "                bdc_v2 = np.zeros(i[1].shape)\n",
    "                fdc_s1 = np.zeros(i[0].shape)\n",
    "                bdc_s2 = np.zeros(i[1].shape)    \n",
    "                fdc_v_a = [fdc_v1, bdc_v2]\n",
    "                fdc_s_a = [fdc_s1, bdc_s2]\n",
    "                fdc_v_adam.append(fdc_v_a)\n",
    "                fdc_s_adam.append(fdc_s_a)\n",
    "                \n",
    "                \n",
    "                df1_t = np.zeros(i[0].shape)\n",
    "                db1_t = np.zeros(i[1].shape)\n",
    "                fb_temp = [df1_t, db1_t]\n",
    "                dfb.append(fb_temp)\n",
    "            \n",
    "            \n",
    "            #Final layer 1x1 filter setup\n",
    "\n",
    "            v_out_f = np.zeros(out_f.shape)\n",
    "            s_out_f = np.zeros(out_f.shape)\n",
    "            bv_out_b = np.zeros(out_b.shape)\n",
    "            bs_out_b = np.zeros(out_b.shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "            dout_f = np.zeros(out_f.shape)\n",
    "            dout_b = np.zeros(out_b.shape)\n",
    "            \n",
    "            ######################################\n",
    "            \n",
    "            \n",
    "            #timestamp1 = time.time()\n",
    "            \n",
    "            \n",
    "            [df1,df2,df3,df4,df5,df6,df7,df8,df9] = df\n",
    "            [db1,db2,db3,db4,db5,db6,db7,db8,db9] = db \n",
    "            [dfb6_dc,dfb7_dc,dfb8_dc,dfb9_dc]     = dfb\n",
    "            \n",
    "            \n",
    "            for b in range(batch):\n",
    "                \n",
    "                print('Image: {}/{}'.format((b+1),batch))\n",
    "                #################### TODO: BLOCK IMPLEMENTATION - FUTURE UPDATE ######################\n",
    "                    \n",
    "                    \n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ######################## Forward Propagation Convolution Part  ##########################\n",
    "\n",
    "                \n",
    "                ########### 1st Big Layer ###########    (with zero padding ='same',so with stride =1 we get same dim as the input)\n",
    "                params = [f1[0], b1[0]]  \n",
    "             \n",
    "                conv1_1 = conv(X_t[b], params, 1)   #conv1 shape = (num_channels, h, w), padding = 1 (same output dim)\n",
    "                conv1_1[conv1_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f1[1], b1[1]]\n",
    "                conv1_2 = conv(conv1_1, params, 1)\n",
    "                conv1_2[conv1_2<=0] = 0 #Relu\n",
    "                ##################################### conv1_2: 128x128x16\n",
    "                \n",
    "                pl1 = maxpool(conv1_2, 2, 2) #   pl1 : (128-2)/2+1  = 64 \n",
    "                ## ADD DROPOUT HERE(on pl1)\n",
    "                \n",
    "                ########### 2nd Big Layer ###########\n",
    "                params = [f2[0], b2[0]]  \n",
    "                conv2_1 = conv(pl1, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv2_1[conv2_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f2[1], b2[1]]\n",
    "                conv2_2 = conv(conv2_1, params, 1)\n",
    "                conv2_2[conv2_2<=0] = 0 #Relu             \n",
    "                #####################################  64x64x32\n",
    "\n",
    "                pl2 = maxpool(conv2_2, 2, 2) #pool_f = 2 , pool_s = 2    , (64 -2)/2 +1 = 32\n",
    "                ## ADD DROPOUT HERE\n",
    "\n",
    "                ########### 3rd Big Layer ###########\n",
    "                params = [f3[0], b3[0]]  \n",
    "                conv3_1 = conv(pl2, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv3_1[conv3_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f3[1], b3[1]]\n",
    "                conv3_2 = conv(conv3_1, params, 1)\n",
    "                conv3_2[conv3_2<=0] = 0 #Relu             \n",
    "                #####################################  32x32x64\n",
    "\n",
    "                pl3 = maxpool(conv3_2, 2, 2) #pool_f = 2 , pool_s = 2   ,  (32-2)/2 +1 = 16\n",
    "                ## ADD DROPOUT HERE\n",
    "                \n",
    "                ########### 4th Big Layer ###########\n",
    "                params = [f4[0], b4[0]]  \n",
    "                conv4_1 = conv(pl3, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv4_1[conv4_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f4[1], b4[1]]\n",
    "                conv4_2 = conv(conv4_1, params, 1)\n",
    "                conv4_2[conv4_2<=0] = 0 #Relu             \n",
    "                #####################################     16x16x128\n",
    "\n",
    "                pl4 = maxpool(conv4_2, 2, 2) #pool_f = 2 , pool_s = 2  , (16-2)/2 +1 =8  : 8x8x128\n",
    "                ## ADD DROPOUT HERE\n",
    "                \n",
    "                ########### 5th Big Layer ###########   8x8x128-->8x8x256\n",
    "                params = [f5[0], b5[0]]  \n",
    "                conv5_1 = conv(pl4, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv5_1[conv5_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f5[1], b5[1]]\n",
    "                conv5_2 = conv(conv5_1, params, 1)\n",
    "                conv5_2[conv5_2<=0] = 0 #Relu             \n",
    "                #####################################  8x8x256\n",
    "                \n",
    "                #####################################\n",
    "                #Because of ambigious size after the upsampling the concat func must take care possible crop of the conv#_2 \n",
    "                #####################################\n",
    "                #Deconvolution/Upsampling\n",
    "                # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "                params = [fb6_dc[0], fb6_dc[1]] # deconv filter, deconv bias\n",
    "                dc6, new_in6 = convTransp(conv5_2, params, 1, 0)   #result:   =  16x16x128 , # conv5_2 requires NO crop\n",
    "                #Concat dc6 with conv4_2 so we get 256 channels (16x16x256)\n",
    "                c6 = concat(dc6, conv4_2) # 1st one is the right one size  \n",
    "                \n",
    "                ########### 1st Big dc Layer ###########          16x16x256     \n",
    "                params = [f6[0], b6[0]]  \n",
    "                conv6_1 = conv(c6, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv6_1[conv6_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f6[1], b6[1]]\n",
    "                conv6_2 = conv(conv6_1, params, 1)\n",
    "                conv6_2[conv6_2<=0] = 0 #Relu   \n",
    "                #####################################    16x16x128\n",
    "                #(16-1)*2 + 2 =32\n",
    "                params = [fb7_dc[0], fb7_dc[1]] # deconv filter, deconv bias\n",
    "                dc7, new_in7 = convTransp(conv6_2, params, 1, 0)   #result:   =  32x32x64\n",
    "                #Concat dc7 with conv3_2 so we get  channels (32x32x128)\n",
    "                c7 = concat(dc7, conv3_2)   \n",
    "                \n",
    "                ########### 2nd Big dc Layer ###########          32x32x128     \n",
    "                params = [f7[0], b7[0]]  \n",
    "                conv7_1 = conv(c7, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv7_1[conv7_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f7[1], b7[1]]\n",
    "                conv7_2 = conv(conv7_1, params, 1)\n",
    "                conv7_2[conv7_2<=0] = 0 #Relu     \n",
    "                #####################################    32x32x64\n",
    "                #(24-1)*2 + 2 = 48\n",
    "                params = [fb8_dc[0], fb8_dc[1]] # deconv filter, deconv bias\n",
    "                dc8, new_in8 = convTransp(conv7_2, params, 1, 0)   #result:   =  64x64x32\n",
    "                #Concat dc8 with conv2_2 so we get  channels (64x64x64)\n",
    "                c8 = concat(dc8 ,conv2_2)   \n",
    "                \n",
    "                ########### 3rd Big dc Layer ###########          64x64x64    \n",
    "                params = [f8[0], b8[0]]  \n",
    "                conv8_1 = conv(c8, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv8_1[conv8_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f8[1], b8[1]]\n",
    "                conv8_2 = conv(conv8_1, params, 1)\n",
    "                conv8_2[conv8_2<=0] = 0 #Relu    \n",
    "                #####################################    64x64x32                              \n",
    "                #(64-1)*2 + 2 = 128\n",
    "                params = [fb9_dc[0], fb9_dc[1]] # deconv filter, deconv bias\n",
    "                dc9, new_in9 = convTransp(conv8_2, params, 1, 0)   #result:   =  128x128x16\n",
    "                #Concat dc9 with conv1_2 so we get  channels (128x128x32)\n",
    "                c9 = concat(dc9, conv1_2)                   \n",
    "               \n",
    "                ########### 4th Big dc Layer ###########          128x128x32   \n",
    "                params = [f9[0], b9[0]]  \n",
    "                conv9_1 = conv(c9, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv9_1[conv9_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f9[1], b9[1]]\n",
    "                conv9_2 = conv(conv9_1, params, 1)\n",
    "                conv9_2[conv9_2<=0] = 0 #Relu   \n",
    "                #####################################    128x128x16\n",
    "                \n",
    "                ############################# Last Layer conv(1x1) --> 128x128x1 ##########################\n",
    "                params = [out_f, out_b]\n",
    "                output = conv(conv9_2, params, 1, 0) #output.shape: 128x128x1\n",
    "                \n",
    "                print(output[:,0:30,0:30])\n",
    "                ## Sigmoid ##\n",
    "                Y_hat = sigmoid(output)\n",
    "                \n",
    "                #label crop is needed\n",
    "                #Y_t_b = crop(Y_t[b], Y_hat.shape[1])\n",
    "                plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "                cost += NLLLoss(Y_hat, Y_t[b])\n",
    "                print(cost/(b+1))\n",
    "                \n",
    "                accuracy += get_accuracy_value(Y_hat, Y_t[b])\n",
    "                print(accuracy/(b+1))\n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ################################# Backward Propagation ##################################\n",
    "                \n",
    "                \n",
    "                #Pixel-wise sub, we we can get the diff(Y includes the 2 classes 0 and 1)\n",
    "                \n",
    "                dA_prev = Y_hat - Y_t[b]\n",
    "                dZ_prev = sigmoid_backward(dA_prev, output)\n",
    "                conv_s =1 \n",
    "                dconv9_2, dout_f_, dout_b_ = convolutionBackward(dZ_prev, conv9_2, out_f, conv_s) #\n",
    "                #pack data\n",
    "                \n",
    "                \n",
    "                dconv9_2[conv9_2<=0] = 0             \n",
    "                dconv9_1, df9_2, db9_2 = convolutionBackward(dconv9_2, conv9_1, f9[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv9_1[conv9_1<=0] = 0\n",
    "                conc_dconv9, df9_1, db9_1 = convolutionBackward(dconv9_1, c9, f9[0], conv_s) #C9 is not needed for input,we know how to select the right gradients\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv9, dconv1_2 = crop2half(conc_dconv9)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #conv8_2 is not needed for input,we know how to select the right gradients            \n",
    "                #dconv1_2 = reshape(dconv1_2, conv1_2.shape[1])\n",
    "                \n",
    "                dconv8_2, df9_dc, db9_dc = convTranspBackward(dconv9, new_in9, fb9_dc[0],conv_s)\n",
    "                #pack data\n",
    "\n",
    "                dconv8_2[conv8_2<=0] = 0\n",
    "                dconv8_1, df8_2, db8_2 = convolutionBackward(dconv8_2, conv8_1, f8[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv8_1[conv8_1<=0] = 0\n",
    "                conc_dconv8, df8_1, db8_1 = convolutionBackward(dconv8_1, c8, f8[0], conv_s) #\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv8, dconv2_2 = crop2half(conc_dconv8)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #dconv2_2 = reshape(dconv2_2, conv2_2.shape[1])\n",
    "                \n",
    "                dconv7_2, df8_dc, db8_dc = convTranspBackward(dconv8, new_in8, fb8_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                dconv7_2[conv7_2<=0] = 0\n",
    "                dconv7_1, df7_2, db7_2 = convolutionBackward(dconv7_2, conv7_1, f7[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv7_1[conv7_1<=0] = 0\n",
    "                conc_dconv7, df7_1, db7_1 = convolutionBackward(dconv7_1, c7, f7[0], conv_s) #\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv7, dconv3_2 = crop2half(conc_dconv7)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #Make sure that dconv3_2 is the same dim with the dconv3_2 that will come from maxpool in decoding side\n",
    "                #dconv3_2 = reshape(dconv3_2, conv3_2.shape[1])\n",
    "                \n",
    "                dconv6_2, df7_dc, db7_dc = convTranspBackward(dconv7, new_in7, fb7_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                dconv6_2[conv6_2<=0] = 0\n",
    "                dconv6_1, df6_2, db6_2 = convolutionBackward(dconv6_2, conv6_1, f6[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv7_1[conv7_1<=0] = 0\n",
    "                conc_dconv6, df6_1, db6_1 = convolutionBackward(dconv6_1, c6, f6[0], conv_s) #\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv6, dconv4_2 = crop2half(conc_dconv6)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #dconv4_2 = reshape(dconv4_2, conv4_2.shape[1])\n",
    "                \n",
    "                dconv5_2, df6_dc, db6_dc = convTranspBackward(dconv6, new_in6, fb6_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                dconv5_2[conv5_2<=0] = 0\n",
    "                dconv5_1, df5_2, db5_2 = convolutionBackward(dconv5_2, conv5_1, f5[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv5_1[conv5_1<=0] = 0\n",
    "                dpl4, df5_1, db5_1 = convolutionBackward(dconv5_1, pl4, f5[0], conv_s) #\n",
    "                \n",
    "                dconv4_2 += maxpoolBackward(dpl4, conv4_2, f=2 , s=2) #Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                dconv4_2[conv4_2<=0] = 0\n",
    "                dconv4_1, df4_2, db4_2 = convolutionBackward(dconv4_2, conv4_1, f4[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv4_1[conv4_1<=0] = 0\n",
    "                dpl3, df4_1, db4_1 = convolutionBackward(dconv4_1, pl3, f4[0], conv_s) #\n",
    "\n",
    "                dconv3_2 += maxpoolBackward(dpl3, conv3_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                dconv3_2[conv3_2<=0] = 0\n",
    "                dconv3_1, df3_2, db3_2 = convolutionBackward(dconv3_2, conv3_1, f3[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv3_1[conv3_1<=0] = 0\n",
    "                dpl2, df3_1, db3_1 = convolutionBackward(dconv3_1, pl2, f3[0], conv_s) #\n",
    "                \n",
    "                dconv2_2 += maxpoolBackward(dpl2, conv2_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                dconv2_2[conv2_2<=0] = 0\n",
    "                dconv2_1, df2_2, db2_2 = convolutionBackward(dconv2_2, conv2_1, f2[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv2_1[conv2_1<=0] = 0\n",
    "                dpl1, df2_1, db2_1 = convolutionBackward(dconv2_1, pl1, f2[0], conv_s) #\n",
    "                \n",
    "                dconv1_2 += maxpoolBackward(dpl1, conv1_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                dconv1_2[conv1_2<=0] = 0\n",
    "                dconv1_1, df1_2, db1_2 = convolutionBackward(dconv1_2, conv1_1, f1[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv1_1[conv1_1<=0] = 0\n",
    "                _, df1_1, db1_1 = convolutionBackward(dconv1_1, X_t[b], f1[0], conv_s) #\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                [df1,df2,df3,df4,df5,df6,df7,df8,df9] = df\n",
    "                [db1,db2,db3,db4,db5,db6,db7,db8,db9] = db \n",
    "                [dfb6_dc,dfb7_dc,dfb8_dc,dfb9_dc]     = dfb\n",
    "\n",
    "                \n",
    "                df1[0] += df1_1\n",
    "                df1[1] += df1_2\n",
    "                df2[0] += df2_1\n",
    "                df2[1] += df2_2\n",
    "                df3[0] += df3_1\n",
    "                df3[1] += df3_2\n",
    "                df4[0] += df4_1\n",
    "                df4[1] += df4_2\n",
    "                df5[0] += df5_1\n",
    "                df5[1] += df5_2\n",
    "                df6[0] += df6_1\n",
    "                df6[1] += df6_2\n",
    "                df7[0] += df7_1\n",
    "                df7[1] += df7_2\n",
    "                df8[0] += df8_1\n",
    "                df8[1] += df8_2\n",
    "                df9[0] += df9_1\n",
    "                df9[1] += df9_2\n",
    "                \n",
    "                db1[0] += db1_1\n",
    "                db1[1] += db1_2\n",
    "                db2[0] += db2_1\n",
    "                db2[1] += db2_2\n",
    "                db3[0] += db3_1\n",
    "                db3[1] += db3_2\n",
    "                db4[0] += db4_1\n",
    "                db4[1] += db4_2\n",
    "                db5[0] += db5_1\n",
    "                db5[1] += db5_2\n",
    "                db6[0] += db6_1\n",
    "                db6[1] += db6_2\n",
    "                db7[0] += db7_1\n",
    "                db7[1] += db7_2\n",
    "                db8[0] += db8_1\n",
    "                db8[1] += db8_2\n",
    "                db9[0] += db9_1\n",
    "                db9[1] += db9_2\n",
    "\n",
    "                dfb6_dc[0] += df6_dc\n",
    "                dfb6_dc[1] += db6_dc\n",
    "                dfb7_dc[0] += df7_dc\n",
    "                dfb7_dc[1] += db7_dc\n",
    "                dfb8_dc[0] += df8_dc\n",
    "                dfb8_dc[1] += db8_dc\n",
    "                dfb9_dc[0] += df9_dc\n",
    "                dfb9_dc[1] += db9_dc\n",
    "\n",
    "                dout_f += dout_f_\n",
    "                dout_b += dout_b_\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ############## Adam Optimization ################\n",
    "            #changing the main structures(which are also updated)\n",
    "            #TODO: Future update - remove uneccessary memory loads/stores, v,s dont need to be saved \n",
    "            for i in range(len(filters)):\n",
    "                v_adam[i][0] = beta1*v_adam[i][0] + (1-beta1)*df[i][0]/batch_size #f1\n",
    "                s_adam[i][0] = beta2*s_adam[i][0] + (1-beta2)*(df[i][0]/batch_size)**2 #f1\n",
    "                filters[i][0] -= lr*v_adam[i][0]/np.sqrt(s_adam[i][0] + 1e-7)\n",
    "                \n",
    "                v_adam[i][1] = beta1*v_adam[i][1] + (1-beta1)*df[i][1]/batch_size #f2\n",
    "                s_adam[i][1] = beta2*s_adam[i][1] + (1-beta2)*(df[i][1]/batch_size)**2  #f2\n",
    "                filters[i][1] -= lr*v_adam[i][1]/np.sqrt(s_adam[i][1] + 1e-7)\n",
    "                \n",
    "            for i in range(len(bias)):\n",
    "                bv_adam[i][0] = beta1*bv_adam[i][0] + (1-beta1)*db[i][0]/batch_size #b1\n",
    "                bs_adam[i][0] = beta2*bs_adam[i][0] + (1-beta2)*(db[i][0]/batch_size)**2  #b1\n",
    "                bias[i][0] -= lr*bv_adam[i][0]/np.sqrt(bs_adam[i][0] + 1e-7)\n",
    "                \n",
    "                bv_adam[i][1] = beta1*bv_adam[i][1] + (1-beta1)*db[i][1]/batch_size #b2\n",
    "                bs_adam[i][1] = beta2*bs_adam[i][1] + (1-beta2)*(db[i][1]/batch_size)**2  #b2\n",
    "                bias[i][1] -= lr*bv_adam[i][1]/np.sqrt(bs_adam[i][1] + 1e-7)\n",
    "            \n",
    "            for i in range(len(f_dc)):\n",
    "                fdc_v_adam[i][0] = beta1*fdc_v_adam[i][0] + (1-beta1)*dfb[i][0]/batch_size #f1\n",
    "                fdc_s_adam[i][0] = beta2*fdc_s_adam[i][0] + (1-beta2)*(dfb[i][0]/batch_size)**2  #f1\n",
    "                f_dc[i][0] -= lr*fdc_v_adam[i][0]/np.sqrt(fdc_s_adam[i][0] + 1e-7)\n",
    "                \n",
    "                fdc_v_adam[i][1] = beta1*fdc_v_adam[i][1] + (1-beta1)*dfb[i][1]/batch_size #b2\n",
    "                fdc_s_adam[i][1] = beta2*fdc_s_adam[i][1] + (1-beta2)*(dfb[i][1]/batch_size)**2  #b2\n",
    "                f_dc[i][1] -= lr*fdc_v_adam[i][1]/np.sqrt(fdc_s_adam[i][1] + 1e-7)    \n",
    "            \n",
    "            v_out_f = beta1*v_out_f + (1 - beta1)*dout_f/batch_size #f\n",
    "            s_out_f = beta2*s_out_f + (1 - beta2)*(dout_f/batch_size)**2  #f\n",
    "            out_fb[0] -= lr*v_out_f/np.sqrt(s_out_f + 1e-7)\n",
    "            \n",
    "            bv_out_b = beta1*bv_out_b + (1 - beta1)*dout_b/batch_size #b\n",
    "            bs_out_b = beta2*bs_out_b + (1 - beta2)*(dout_b/batch_size)**2  #b\n",
    "            out_fb[1] -= lr*bv_out_b/np.sqrt(bs_out_b + 1e-7)\n",
    "            \n",
    "            \n",
    "            #Updating Conv Part\n",
    "            \n",
    "\n",
    "            #print(Yh)\n",
    "            #print(t)\n",
    "            #print(\"Batch:{}\".format(c+12))\n",
    "            \n",
    "            '''\n",
    "            f1 -= (learning_rate) * (df1/batch)\n",
    "            f2 -= (learning_rate) * (df2/batch)\n",
    "            b1 -= (learning_rate) * (db1/batch)\n",
    "            b2 -= (learning_rate) * (db2/batch)\n",
    "            filter_params = [f1, f2, b1, b2]\n",
    "                \n",
    "            # updating FC Part\n",
    "            #params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "            grads_values['dW1'] = dW1#/(batch)\n",
    "            grads_values['dW2'] = dW2#/(batch)\n",
    "            grads_values['db1'] = dB1#/(batch)\n",
    "            grads_values['db2'] = dB2#/(batch)\n",
    "                   \n",
    "            for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "                \n",
    "                params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "                params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "            \n",
    "            '''\n",
    "            print(\"Cost : {:.5f}\".format(cost/batch))\n",
    "            print(\"Accuracy : {:.5f}%\".format((accuracy*100)/batch))\n",
    "            \n",
    "        #END OF LOOP - EPOCH\n",
    "        #timestamp2 = time.time()\n",
    "        #print (\"This took %.2f seconds\" %(timestamp2 - timestamp1))\n",
    "        if(verbose):\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/batch, (accuracy*100)/batch))\n",
    "        #if(callback is not None):\n",
    "        #    callback(i, params_values)\n",
    "    #pack filters\n",
    "    params_values = [filters, bias, f_dc, out_fb]\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: {1}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "New Conv\n",
      "262144\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n",
      "This took 0.00 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a37408449a3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mparams_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#0.05 stable LR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-bae8f468a28d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, Y, epochs, learning_rate, dropout, verbose, callback)\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                 \u001b[0mconv1_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#conv1 shape = (num_channels, h, w), padding = 1 (same output dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m                 \u001b[0mconv1_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconv1_1\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m#Relu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-914036b26d37>\u001b[0m in \u001b[0;36mconv\u001b[0;34m(image, params, s, pad)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_h\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_w\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mnp_o\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_h\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0m_h\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#(np.sum(np.multiply(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] , f[z, :, :, :])) + b[z])[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-914036b26d37>\u001b[0m in \u001b[0;36mcalc\u001b[0;34m(img, f, b)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtimestamp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.8/site-packages/cupy/math/sumprod.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# TODO(okuta): check type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###### Training ######\n",
    "#train_images, train_labels, test_images, test_labels\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "params_values = train(train_images, train_labels, 10, 0.01, True) #0.05 stable LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Prediction ######\n",
    "Yt = test_labels.T\n",
    "temp1 = []\n",
    "for i in range(Yt.shape[1]):\n",
    "        for j in range(Yt.shape[0]):\n",
    "            if(Yt[j][i]==1):\n",
    "                temp1.append(j)\n",
    "Yt=np.array(temp1)\n",
    "Y_test_hat, _ = full_forward_propagation(np.transpose(test_images), params_values, NN_ARCHITECTURE)#multiple?!\n",
    "\n",
    "Yht = np.array(Y_test_hat.T)\n",
    "#x_log=np.log( np.exp(Yh) / np.sum(np.exp(Yh), axis = 1) )  #(60000,10) , we need to add along columns so we get sum of 1 on every example-row\n",
    "num = np.exp(Yht)\n",
    "den = np.sum(np.exp(Yht), axis = 1)\n",
    "for i in range(Yht.shape[0]): #60000\n",
    "                #for j in range(Yh.shape[1]): #10\n",
    "                Yht[i][:] = np.log(num[i][:] / den[i])  \n",
    "\n",
    "#cost = get_cost_value(Yht, Yt)\n",
    "\n",
    "#cost_history.append(cost)\n",
    "accuracy = get_accuracy_value(Y_test_hat, test_labels.T)\n",
    "#accuracy_history.append(accuracy)\n",
    "print(\"Accuracy: {:.5f}%\".format( accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
