{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "#import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load from /home/USER/data/mnist or elsewhere; download if missing.\"\"\"\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "from matplotlib import image\n",
    "\n",
    "def mnist(path=None):\n",
    "    r\"\"\"Return (train_images, train_labels, test_images, test_labels).\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory containing MNIST. Default is\n",
    "            /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist.\n",
    "            Create if nonexistant. Download any missing files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, train_labels, test_images, test_labels), each\n",
    "            a matrix. Rows are examples. Columns of images are pixel values.\n",
    "            Columns of labels are a onehot encoding of the correct class.\n",
    "\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz',\n",
    "             'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz',\n",
    "             't10k-labels-idx1-ubyte.gz']\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        # Set path to /home/USER/data/mnist or C:\\Users\\USER\\data\\salt\n",
    "        path = os.path.join(os.path.expanduser('~'), 'data', 'salt')\n",
    "\n",
    "    # Create path if it doesn't exist\n",
    "    #os.makedirs(path, exist_ok=True)\n",
    "    \"\"\"\"\n",
    "    # Download any missing files\n",
    "    for file in files:\n",
    "        if file not in os.listdir(path):\n",
    "            urlretrieve(url + file, os.path.join(path, file))\n",
    "            print(\"Downloaded %s to %s\" % (file, path))\n",
    "    print(\"All files are ready to gzip!\")\n",
    "    \"\"\"\n",
    "    def _images(path):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        folder = path + \"\\images\\\\\"\n",
    "        onlyfiles = [cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY) for f in os.listdir(folder)]\n",
    "        \n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,101,101).astype('float32')/255\n",
    "        return pixels\n",
    "\n",
    "    def _labels(path):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        \n",
    "        folder = path + \"\\labels\\\\\"\n",
    "        onlyfiles = [image.imread(folder+f) for f in os.listdir(folder)]\n",
    "\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,101,101).astype('float32')/255\n",
    "        return pixels\n",
    "    print(\"Train Images : Loading . . .\")\n",
    "    train_images = _images(path)\n",
    "    print(\"Train Labels : Loading . . .\")\n",
    "    train_labels = _labels(path)\n",
    "    \"\"\"\"\n",
    "    print(\"Test Images  : Loading . . .\")\n",
    "    test_images = _images(os.path.join(path, files[2]))\n",
    "    print(\"Test Labels  : Loading . . .\")\n",
    "    test_labels = _labels(os.path.join(path, files[3])) \n",
    "    \"\"\"\n",
    "    print(\"Done!\")\n",
    "    return train_images , train_labels #, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images : Loading . . .\n",
      "Train Labels : Loading . . .\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels= mnist()  #, test_images, test_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 1, 101, 101)\n",
      "(4000, 1, 101, 101)\n"
     ]
    }
   ],
   "source": [
    "ch = 1 #Number of channels\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape) # ONE-HOT !!!\n",
    "#print(train_images.T.shape)\n",
    "#print(train_labels.reshape((train_images.shape[0], 10)).shape)\n",
    "\n",
    "##### Getting Ready for the Conv Layers #####\n",
    "#train_images = train_images.reshape(train_images.shape[0], ch, 28, 28).squeeze()\n",
    "#test_images = test_images.reshape(test_images.shape[0], ch, 28, 28).squeeze()\n",
    "\n",
    "#############################################\n",
    "\n",
    "#y = train_images[1].reshape(1,28,28)\n",
    "#print(y.squeeze().shape)\n",
    "'''\n",
    "temp = []\n",
    "#train_labels = train_labels.sum(1)\n",
    "for i in range(int(len(train_labels[:]))):\n",
    "    temp.append(list(train_labels[i][:]).index(1))\n",
    "    \n",
    "    \n",
    "train_labels = np.array(temp.copy())\n",
    "#print(train_labels[0:5])\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD7CAYAAACSctrBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOy9X6gsbXbet6p7/z3fN6PJjKSZkW0YB4IkIwgJIQkxBIESCImIbiLhBAfZURgQxHb+EY1846vAQEKIIFeD4kQhhkhxDMqFiR0EusiNsOUYTCwUjKOMJU80VvRlNN93zv7blYtznt6/evpZVb137yM3x2dBU93VVW+9f9Z61rPW+1bVMI5jvZf38l7ey5ys/mFX4L28l/dy/PIeKN7Le3kvi/IeKN7Le3kvi/IeKN7Le3kvi/IeKN7Le3kvi/IeKN7Le3kvi/LWgGIYhn9lGIZfH4bh7wzD8JW3dZ338l7ey9uX4W2soxiGYV1V/2dV/ctV9ZtV9deq6t8cx/FvP/vF3st7eS9vXU7eUrn/bFX9nXEc/25V1TAM/0NV/UhVRaD4zGc+M37hC1+oqioHLv3mlsd0x3fnPsex++yfO2YYhsl3/X7sdkl4vD6r1WsSqW0qcxiGto99HMZxrM1ms91uNpvY5lRWV0f/T+ekds/px5yepDqkOqU6+jnpuNRP3fH7tq07N7XP+2Ecx/r617/+O+M4flfshAV5W0DxB6rq7+H3b1bVP8cDhmH4clV9uarq85//fP3sz/7sRMmohJvNpu7v72uz2dTt7W2N41j39/fbrc6pqq3S6vj7+/vtdx807bu7u9uew4+O4TYZR6ecEt+3Wq22RntyclKr1arW63UNw7Dze71eT47Tx42K19F+nXNyclKnp6d1enpaZ2dn2+0wDNvtycnJBEjYPxoDH4urq6u6u7urV69e1c3NTb169aqur6/r/v6+7u7uYj+oH9kXq9VqUie1mfXQsWwf63Z3d7ezTXri4uCp38Mw1OnpaRwD/eZH/act9VV12mw22/JYrrYaU+mTytA4aEzZF+xbt5m7u7u6v7+vm5ub2mw29ZM/+ZP/d+yEPeRtAUVyeRNrGcfxa1X1taqq7//+7x/Pzs5a70BPRSDgVufoNxWax1BZ9Vsd6lsOkhTPjceBh/XfdoZ5JDd2tWMYhq0y6X8p593dXa3X64lSdcoiub293ZZxcnJS6/W6Li4uar1e19nZWZ2cnNT9/f2O8ncA5OV7GwlwbtQ6Pym+gILt43VTWfq+Xq9rHMftliDiBpfqrj7XdVar1XYMxnGs1Wo1Kf/k5KQ2m822rTLcOUfhY0U9IHCoTPWT6qL6ex94v7Kdd3d3W4C6vb2NY/YYeVtA8ZtV9Yfw+w9W1d/vDmZHOl3kYNFQte/k5GRi/EtAwWvIy2hAZJCr1apub28n51bVVoFUh+TV54T02pVH9aOi67sfJ6/lx/MYlqH+cI9ID6f2EcDcyDtjcAD0/X4O99Ery2DSOX49bdkHCeDcKcwJAZuAqX36rv9Uvodqc+L9yw/bzz4QeHR1rqqJ00o2cKi8LaD4a1X1TwzD8Ier6req6o9V1b81W5GTviop3qKHcLYwFx6w03Q+mcTt7W3d399Ptona6v/1er3dx3CFYQAZAD/ePtaPoCQqK48rxe2MmaDj7OT6+rrW63VdXV3VarWq8/PzWq/XdXp6OgERgYorLfszgSnb5iCkc6j0BC+yiU65E0gQ6HX+3d3dbNyfyuV/6mPuk8HSqa1Wqwkr03eyFHcCc0Do7XMmkVg3wwyG29TVfYGsk7cCFOM43g3D8O9V1V+pqnVV/flxHP+P7vhEo7UfZe5svbPcUAgUfg6FHauYcr1eb70w4zwpRlVNFMaNiQPutJ4KTS9QtRu+yMt4OxO9J8hQUXWs2ihwo4Exn6Etjdj7K9FdZ0se/3NsJIzTeR0am+tDAgAHzTm2lxhRpxcMRygEcrWN4cI+9eiEYNnVjayY4ZXn2ebCrsfI22IUNY7jX66qv7zv8XMD7fsTDfYchv5zoEjiCTECw83NTd3f32+TdGISNzc3dXt7W6enp1s07xJnnrwidSXd9fAphSPct8RKvC8JUgpfrq6utrH2MAyTXAFBQ/VnfzvbIoDS+BOAS1g2wSQJQynXCYGMtuob5RS8DtSrZMwdUyNAcGzTeKmcBDTqQ8+JJMfWMeaq2ubOpJcEBzEJHXOIvDWgeIp4TM6BTIyD4krA751n0nHM6EvpT09PJ/T66upqCxibzaaur6/r9vZ2y0D00cBQ3Bjo4emR6KE8r1JVW5bj3roDS7bbY2NPpCnDr6QnE6D8j8L2sp5sq3JIao/PaHC2hck7Co2SYOHjqXJS2FVVO306x1AoyWDdqE9PTyfHz4GFM2E5CrJHH0eyOLWlqrZgIKDwMFn/HSpHAxRC1ZSg2hcwEtPg904hNCD39/dbD6StjJOhiLYCCoEFEZziQKE2MORRHsKVge1QPRxAXHkTyEixvF+lnJohIZvgR0DBMXKDIVNK16qaJo79P45VYoEdo1A5BBL2nbZz4YyL6+CSpLDKz58DCzoHAls3+0YAkQ6R2REojjmZ+ShxWpcYBb2Oo7WX1Uk38DQwggbp9c3NTY3jWGdnZ1tGwRBEH67BkKQ4fBiGSU7k+vp6CzA+G8OBVtzMtmqbpoz9mNQ/iWGcnZ1tAYOJTtJ8F18Dwr4mQDhwOPizPV5nBwsvXzmls7OzSUKSBuZ9kMIMXnefPIPnLLqyeQ2205OWKtPDCG0JFEyuEyDc8RwiRwEUVdnjVdWESrKjSaMpcwPaneMex2PHNAOhrQzj9PR0G5Z0jIIeWfvX6/WWGpK9aKB1jaUYMyWskmJ2wCHlZq7g9PR00h++KMuBXL9Jp6nUyUhZf9XH80x+jDsLgpzqQx3pcgBzfcl+Y/vmQMPZlWTOMbH/lGCWMEGprQMeFyA6A1nKzT1GjgIo1GAyCibdqqbTQz5g/l3i+5YGump3NaLHfUJuTqkyJuRgSVLCrqomjOLVq1dbReEUm5KqnpBj380pghupU1k3Ghnd6enpJF/hQMG+1HECl27cWFfuZ/28bmwbZwMIFgQJLsBiWfSuHVA4sOrc5Ixcl8iIebzqlXRKQKFxdwBM+qd2eH6Iqz99Bu055GiAgujqtEz7fU56HMeJ4aX4lcqUPCG3HSV1VsMpPx0nDyrq6wqeKDb3M071ZBQVxo07GV5H+/lfYhk8Xx8yDv3HGQUep3UEUlh5c9aTFDvVpQOJOWGd2A5niASMJF4PZ7G83pyz6Zhb10aGLJ7HYB6iA3pnEc6cliYC9pGjAIrNZrOdphOVJ6NQbM+sf9Uue6BhJGrK/9N5VbtLgjkzwWSmaDkNmwNG6RJ3BISLi4u6vb2t8/Pzuru7q7Ozs23uggu/Oi9J0JQXc6DhtRn/J++penHqlFOP+k32ofJkuN2KwjRO7GvSax9nL8eXjKuOGp9ksB0lJ9PrAJrtS7kNOhQ6Fp3LpDcXQhGgCapkIolRuA50MzuHylEAhTqtajrFd3JyMon3fRASI0hZdfcGHZ3kNRjb+rRcOleGJzDzstKAKS6lQinskJGqb3QNX+DkXtLr56AlJWIOxv/ncTJcNwqn/Xd3d5OcRmIO+u0LlKqm8biznCVJrLHqgQVR7u/vI4AlQGEuyhmGn8vx4DHSBzdijmPST9U1sZAEXs4mnluOAig2m01961vf2i72UZzLxT+ay1c8Ry+X4mbGfL4Ix4/tjJkg4dOj+o+GoRvbktFyS++jQT89PZ1Mz15eXtbt7e12/cbNzc3OFJjH3X7jWkqGiZnoOlxxSg/ssbfGSf8pzNJ3JWXZlwIQ3+cemWtKVG8fg07IXlLC1Wl3CmkcJOYWMdF4EyATiLVfTkflaAaNLJHlJbCmvlB/0myOs553KvRQg7mefhwfbvqqqp2Gu/GlTkmGqsHTNeYy1NzP41kn1qFD9A4oVB+2XR+GELq+WFaKWxmTO7ti6MHzuHKROQWKGxLjfZ8dcopMwGYfeBiUchOPoc10EAQkv5aPj7eNoJjYI4Ez9RHPFxshEHLLceC5qV4JNNKn65tD5SiA4v7+fssoFPOKUfBZCro92hkF5+6ZIEz3WCQW4crsilWVFwDRI6ZjEkjpfw7sZrPZYS7aat2GZkdSsnNu63kUsRJ5M07pOq1V3aikFFdiKb/CSPdqGluBJcM0zyfMhTpz48m1Hs5cUt352ym/GIWXw1DT6+1raHQ8wcFv1vLQcc7ovb86nUss6xA5CqAYx3FLsRmf39/fT+6lYDJTgMG1DAQOelNPeLmiqcMTWDiy+3dKR/E6oNB3Xc/pK728BtyN2mmxJ7yScipBLCNgCMJpNrKSRNmrpkDCMIjrKLTleOm7A1Lqt32F45fAxKVjE2R9ZHycuXL248lhdyg+buyrZPQOyu6MkuPyfvO+OESOAig2m029fPly53kJXEIslqEVg2Qd9FQ+Y5K8i9NT30/l6kAhGbukU0z+z+8MmwSG2i9w4GIbnyJLikdqy1ibN7Vp68zCwYfAo/aynbyHRf87BdesiO9345ib0Zpjhd6/zha9TB9HjiFDwWEYJncTd6EIxyBNwXK/z2D5LE8CzE4PU5s1JszvHSpHAxRXV1cTw/aPgEEJMyY+pYicTtXWk5odTWXI8hQU7hR27vj0ncyG9Jz5BAcIj189melPOxKDuL9/uCtWS559QRkToJytYL09dPNjSNUJaNpPI+B1UojIMlP/JpDYF7g9R9Alv50NESB8TMhWfPrX8zkJKLztc21w4Hb9P0SOAijGcdze60CjV2PX63Xd3NxMMuvptmgdn56twHMcDLg/zaJU5ftEnOLNKXZXBs/376T+ChUSIHgOITEMD1WcSXS309/d3U3CEjILid8BmtiAx/hzkozdz08ZfQKQ039nfZ24gafcQTL6tK6hC8s6oPdrzjmepFPsK9rCO5OjEKNIiShHRgIE0bNjIwkoCArp+AQmnTF7XJzARtIBC//Xby791YeP/XMvrGP120MUgoUrOIFD07C6jf7m5mYLInyiFa/nfZOo8xwLoLDfE8tT33hZNEYCLevoYOFA4vVmf3ougX3p+SGfulbZzhxSvqHL07hedM6G9qI8nmzlEDkKoKh68ILD8JC8I83Tb1JWGj/BxGdCCBRVtWPQvvrTk6M6h53tCp2Aopuic485x2D4ncpOhWKCTVuGLQkoeBv9OD7cAKbksW6jF2NjwtMV3pXQk3EJ/Enr/dgOpD308Gsyz6EtdSed4+fOMQAPLxJgeDLY+6sLL1Jfeug0l29xHfR83zsBFKLAVIpupqJbRpzor3dU8lYEGQeKzqulc/Rx8OpAgOwn1ckVYilGrVq+r6DqITvPOXxSZzEMsgmFhQIKTeslJkPDYZKwm6rz8KELFZPBsBw6EJXjLCOxEO8v9hmf4OXTyt3Wb87yECSNqeqVchPdFD/LSccyr/fOhB5VtTOYis/pQdmZzjC0JTCwA3VOYgAe13XGTkSfe8YkM/3JE2qflEjXlTI5SDhtToPuRkMj9XO5P/Uxr6/2a3pa2y43wuMERB46ujg7cjDg+PGYVI7XSe1QO9kHfp6HFjR6Z2VdaDeXf/DQx8eNY0BHlxxkAtu5z6FyVEAhSRTc0ZPfE+K6B0n/zeUknA14OEEPqW03dcvcCAedDCbN3iSgSmDhikBvWpUTdP4h02By8+7uri4vLydTpp7Aowem56U3d1bgcTvr3rHEpA/UH2c2CWycUbDtDgTM34hl+dPZfS1LF3pQtzum6Q6FfeFGP8cqqFMdOD9WjgYoKI729ACpg8hGUl6AA8H/fNVdov76LUOm4q3X68niMHloLg4bhof7UzxskSK5x/T6kz0RFPR/xxj4H42T53vyTPs1965y1FbmNfg8TAcEMia1gUAn49E4SDzkY/t93L3uqgvbm/qExxOsVNfuGQ++sC0BbqqT962PsYOY66P6RP95OSwvhcuJfT1WjgYoEtLrO7fdeR048Lt3NgeoYyH6uHfz6VhORYkVOKNIyVWdx6XpnA72h8ckJXBP7f2i/ktK4+GOAE7X3GwelpHTS3ZUXR/d+CTj8/F1+s6xchah/W4kvLaHG35djTUlzWKkupE9+H02zqq8X30syBb5CkWODZmuMwqXBBSeD3qnQg9X6K7TXTrD8HLlkat2l8P6QHIfvbkzCi6AkgdlnO4zNBpEsRDNLpCNqF68ptpIZR/H6Ruk5hSi6zudR7ahcliu/neA0Eee1+vhxut9znyT1ye1qRtjlqsyeWs82+P90gGE5yDIKDyn0bEyz/04i3CGyfr6x4Eu6X33eWeAQlSXgz330TGSRO+6wZtjHul3l7vw2ZFuoZeHM36OPpqG1Ju7Li4uJnkPPj7fy00x7z7t6+grFZae0rcdo9BKTwKFzvGlzmQEXT3dKCQCaNaL+YYkBGMCXWIHTNJ2rMPrJgfBfQQKjqMYhZhnCh0Ylrr+0/E5o5BepH57ihwNUKhxBIQ02F0MuA8DcYPgvrm60QjFDmTgzPJrzl4KzGPdoB1omOvgeg/lAjybTrAiE9gXMNRP7m01BmoHFd/FgUKi6yeQ1372lS/gSvXs9rON1BkHKInGhPUm0CUQ6EIu6l5iQf6/dMbXODD/VTVd6SqdYf1VJ+53dkKG8hxyNEChF+7wk5bDMglYlT1HCl/SMUvfVbeq3ak5KVZaeyHam2YsfDD9XDGIV69ebbenp6d1cXExeeAtY1CWkRhGlyRlu+aUqkug0ePquG4KVWOp/aTxibXouzsM/k9jSczCWaiPv4OvA4Vfk4DofaHrsN08n2N8fn6+HWtuqWMcw1QHJoBZD7IQ2sCcLewjRwEUVQ+vfquaPjSEdDKFH67gHhcudVAKZ9I5zBfQ60rh3LNX7YYtBAoHEf3WDIkzDK2S5LM53DM5YHTJT4IFZ4yqcjiibQpNvJ/JwGiM+u0rcGnsKbRhHXy/yhR743gSBNKY8rju+ZNJBygpjGP5FAG7Qg4HCiY0fbxYX+ZenAlW1Q4DUT8fKkcBFGIUDhRMFNITOQ3UOVXZ8NP1KKTFXkZ3Do+jwiWg8I8vK1bsKqBIgHF9fT25Y9bXa3DrwJGAogOxxD64j4CQvvuxDhDe3wSVxABYBkFLfZ76N4GC64Nfr2MeHPs0c8K+4m/2jfpF4yNGwYcxdXojIZvx6Vz2u1/fj3uqHAVQqPOcCnoMSWrHxFPV7gNU5piEG/3c1JYPuAuPTVSc53bGOQxD3dzcTMKG29vbyTQpWYTfSs+kWPJYiVl4IpRspwtbtE3GqGOV19Ax+jAxOo4PeReW5aGIruXHpWvSE3ehRhpj1xuOuV9rbusJRZ6nff5MFb9pKwGE6iPH4Wza9ZPnso2HylEABRWUHkSdQ6AgdSWYuGJWzQ84hR5N53VA4QbOpFFHP9Pgenn6Tiqu32IWvHlNVDYxCioiQYXG5EDhv8lKVJdOOV0hEyWm4bPdHr55+7sx87GQA+E+jW0yFPWt5xQ4Nuk6qW4OwPyf/eqhh9+L4eeqTkrAel1cz1zmwqjHytEAxfn5+Q6l0iASJDz0YFKsKnt4CtGexyQ20oUfndK411wqJ/WDhyA3Nzc74YQnLR0wpIgXFxdbBU235VNJU2LUy3X24Qq9xOS8DwnQBEUfDw89qB/a6jidr/UpupbXS4C02WwmK0x9vBLr0lizHT4mOs7HyBfXubNJ/an6K2TtwIzn00aeAyyOAiiqprdKOx11T+vH0ED1v/Z3RinhoqWkLB2165iEMx2J096kvFW73oO0vVNKMi8a3Gaz2YYjXEbOxJo+fNkPDdjn8R0o2I4UprnS7wMm7Ev2l+9Xf6jdagvHRyGqjwEZkjNZrwON2cMDfffVt2Q6rBsf69iFGqlPl8T7yPv7nQAKdbQ3jAtc9PGHt/giGTdIlb+PJKVOdfXvGtgOyf2JUIltOAvSOa5IDAdceZ1RaDo1PW+DDwDyxJoWfqUkafKAyXDI3FLfsM1zkkDHgULrTrTQi2slpC/s21Snrj5u/L6ISe105sCQJD0zJQFEqqcfM5eLY7+m/N4hchRAUVU7HtEVJCmYPJ4P9GM7ZY5Cz4Uffm4HAAmoluqYznem4l5L+9QvYkiaYmUII2Pi1C6pvGSOUbAOHctge+YU/CmK7KyAjMCn2NWGdM05Q1oCCh8Dzzek1bk+han6cOuyLzNIjupQkKg6ACiGYfhDVfXfVdUXqmpTVV8bx/FnhmH4bFX9fFV9qap+o6p+bBzHjxbKmjAK3vuQWAaP8TzGUxWuM+bkxXheQnHeZah2sH7JK7hxJfBzZeroL2dBPMyQ0nJVIFkIl5BrzUZKiPK6/LBs7y+2q5OU40njJZGOSH/0m6FsklS+101lkAEkoKh6YBQpR+EsIjmXYZjeRSs94Y1oSW/IiFgmj/fFWU+RQxjFXVX9R+M4/o1hGD5VVb86DMP/WlV/oqp+aRzHrw7D8JWq+kpV/dRSYYnSJ3GvUdUn0/YNOfYBCk+CJhZCb6r/6dncc6UBJAh4u6gYc9Sf9RVIeTLU8xkCXVJkgp8Wf1Xt3nXLzD6v5yGI+oP1TP3tQLHPOLpedGseWA8HivTcBjfwlJtiO/34xEbY3o59df3hwNDZQMfenipPBopxHL9RVd948/3bwzD8WlX9gar6kar6wTeH/VxV/XItAAVpo1NtKiv3UZH3ZRSdwvkgpuM6JsE6pcfKaSEW/9fHlzkn1jG3z4UJvGF4mDHxWRMatWZDfKbk6upqZ4FXN/XHHIlyIt3agCVmRpljAzw3MQx/lwVBvCu3Y0EdKKsuXfilMskodA5BwvXHmYS+k6Uqh0W76HJjR7OOYhiGL1XVP1VVv1JVn38DIjWO4zeGYfju5pwvV9WXq6o++9nP7s0i3pw7OwvSdYx7N+5Pymz13fnPUdzrQEDz/c4yeL6Dol+z8xQsgzkF5SR81sNpOkFE9eDCKPeIjMNFnQUsfASeA4YbUzf2S57Q2ZuPlb4/hVmmMufKSYClLduexiwxBoIHE5IpzKjaXay2dK/LY+VgoBiG4cOq+p+q6t8fx/H39h2UcRy/VlVfq6r60pe+NKZcAxGXRkTldlSu2p1qbeq9/Z7iR6fNS+EJB1PU3rdcB+LnqCwO+BJNn6OdAga2z7PxBAy+YGm9Xm9fPcisfWc8nPoTm/DFRKL1TJg6pe/GqPP0qS5u6N4HbjQ8fgm4OifRlZfOcWMm4yQD1ZYP7a2a3j2q3w4y/PDYQ+QgoBiG4bReg8RfGMfxL73Z/dvDMHzxDZv4YlV9c5+yFNtqW5UNXQPvzyFkZ5Fp6P+m/tsyqbBzCueia6reXO8gxsBr+HEdHdV/VAyyBLabdeHWZ5EcLPgf43oBm4ydU7Wp//yl0rqBjUlSv6M2LRVf8uJdniid4wDUAf1jGA5lib16GQnM3ag9zNDzPTqnsE95zyWHzHoMVfVfV9WvjeP4X+Cv/7mqfryqvvpm+4t7lLVVUio+va2DARNyfozKWRrwfRQnMQxex0HNgYAhhrOiuZkQv8tS4YOSioleeshSNQVUsQh9Z9m6lqZSZfA8x/usAwqu3dA9LN1jAVMM73X0mQQHdGck0g/WMQER+4XbfWUOpJ2pUF/04WseHSC0ZejB8v1u2XR/1HOEHJJDGMUfrap/u6r+1jAMf/PNvj9brwHiF4Zh+Imq+npV/eg+hWmQ2CEyOvec9IAeo/GYfWJcbhOzcNrLgadCOhsgYHh73MCTkUtZCDJSJu8Xb3uipQIEtUPG59eQ4srY3Qi9/4Zh2AIKn8bloYevRkxLxx2gE/vQdVkWQaGbldiHUXg7ExDMCQHaAYN66s/i5AuiufXH71G6JKbr13PJIbMe/1tVdS77hx5TlhTG77X33IRTef5PoPEO6jrMjXufc1g3bwPDJ/3PfIkf54Od2i1lUlkEG5/5IYAQLFI9uHqRBuWAtmRgHjIymSmgIPtzoOBMjDMIn6FxAPHr854Pb5uuz3FnOzq24YbPMUrjlkIRjYc7AX+jvM9sCChcf3wcHSQ8N7FPKLUkR7Eyk9SURkJkdIbQxWoEFpdk/J0yLSlO1w6yHL8pyRlAaoMGnEqv36ShVKi5ZBjr0fVL1cMKQoGDhwPeTjd6JkRvbm529nXLmLn1/5x1eJjia0P4qapJboRt6YBiTmcSUFT1j8nzMvTf/f3DO0EcKNJTv1inBGg6TmOe9OudA4qq3Zif8RYHeE7pq2qyQMhliS1wS2++1IZuH+m+A5tfMzEOJiMJFA5ArC8ZBc+j8LeAmb8JnGwTQzWfTeCiJTEez4E4QKSPxm8Ydt+LQgBhXymHw1BEx3pb2YaOVXShh4eNNE56dJ6Xcg98RSPH1a/hfe+sO9Wjc7BPlaMBCi0ddgrGAfDOIMV3hOf/yUh4Dr/73Zv+f4fOKeFHz6ayxzHfn9IpCJeDU5loHPpPTwnj26yYSXdPJcOtergJjTew0Zi8rQwZSP3Vf2mBF/MJKWHpjKFjIekp51ymLsejMMjzDw4UiTktOROn/v4kb9fVlJvQ29R4Tndthl0Uz0+4rj8HSFQdEVDwDkBtpcj0ko6S6RkCDgrqYPeYS2j7GFTeh3mQFRDkqh5AhbkD1lNl0yOrHF6X3tsVxsGJ331+ntf2mRP+p3qwfQzlFDaxn5yV6BoMP1T/BDrMiTBvwxBHgCyW4aBPZ+AzJN1YJzZL6r8EFB4a8rWLHXtxsHbddkbxtuQogGK1er10mN6T00We4PEYUdsOWf2pWBycpVyE0++5eI+hQMq+02A67+FJRwKIU0tXUnkqMRY9vEXvBE396KDL/xILY94hhT4dVe6mID0EIfvw3+nhOvxOoODTyjkOzgwZeqj+PGZp7HX8ElCQ+flqSzokXpN95zkdv77rRdKpQ+QogKLqIX5341IyypWS9NmV1TtShkNPS4axhMSs1z7ehtdIQi/cidojhuDrRthmeWQCJYFAovMJrpwp8WNTW9N+H5dkaH5ttpMGkUINsksCBJelc58f24F7CjHVHu2XkXXH6ni/n4cMLYWKieGxLxJo8Fhe24HpbchRAMVq9TAHX1Vb76dpNg5EQmJnGphedhUAACAASURBVGlweJ6OJZ3bByxSeENFcu/jwMXQw9eA8Dxdz0Me/67fPuuh+zPEJPymImbetSUYq3z1k9eN4qCcxiadm/Y7YHBbVXFNRkqKMlfh7Up174zQ2YjnMzwUUZ+xDzy3lEINlq+tL0jTGKU7XFmH1I7nAI+jAIqq6QIgKToz9hpsPo3YQwj9VscyMampMqfIAoyukyVpP6/nij8HPM6CEsNYMkpdYxzHbRKT4Mf1E7e3txPPzGMIeGItDLc6I9M5c/XqjKKqf/ZDAggCuvTB8xm+9dWcqpf35dz/6XZ6rxP1Tn3qzM4BIrWdOuH7OUa8hyf155LDe6ocBVDIS3gn+6yFjMLR01mEI7vHj85S3AuoXG4l6bezCzIFz7b7dt8BJuBReD7DFDIM3q4sdiFw0UI3gZVWVuo1AeofGqHu3/BcgvcJQyAaEuvdtXWOUXQAof7hb5XH67lBOhjqP7+Of/xcB4PENDjWaqcvLJPe8DtvMKQDTDri8hzgcTRA4fd6kKarw6hw9FROeXkO2YgMSQBBBXLPkMICSQILNxSWl5KYT5HkwVm2MxX2H0FCCinlZV95e7lojAbjqyVTmORAQZDu1rhQPEfAqU+GAXOxPetDfemMKrXVwxq/trdZ/eaA0eVnWJ8UklTtznrsE+o+pxwFUFRNVzXSyJzCOpPwD/93NOdWgEFP615gzhMy/FH9uaV0YYnH83Pn0LukvquaPnnKwxDGysxRcApRtPbu7m7CKDyH4rMQbJeDtcQNiPmPjlV5XzKsYJKSjEZ94EaZDNVFuqfy/O3xPsuia3sZyaHRsbHeDjieANd+htwOhg4Yb0OOBiiqpllfxu4JcbukWYqPSaHJKLjAi1OxHsKobgSxzsB1rKRjEHP159a9tTOUpDDu3VPcrKlTsY5hGLaLrZTTYOhHoSGxrWwT8zaeyON3zjyla7kk6k9xcFMdPDHrwuumRVu60c2f0ZGAguUlZ8CchIOT2kRWw9yMs+M5kHhO0DgaoOgUhApX9aBQDFU6oHDA0KAKIFS2Bt49L8HFAcS9RKeEKSRJrIiKxfLoaVwxXUlTHkPej9dxBkFvv1o93C6uJcZsJ8v3kEptoyJznLrFYgRklpcAiuESRftub2+3bfHtHGh7fwk41U/K5QgwkoF7/ZMeJn32c7wc1ZHjn0CiC7ueQ44CKKhEVbs03untPtuqXTR349eTmwUSYhceW9NYuudf6tqJYnpbfUtD6YBCipJot/eV312p/WwPVwcyBOGDZ/T2Nl/k5gqqOqtM9j0XjHHtioclzoJ8LFVG+t8BheOzBBSpXQov+PIkMYuzs7NtyCpWlcCCdXOg4GMIySy8Polp6rpd2JFYznPI0QCFnmlQ1a9NoDjNTkBC9OWgcEukpsL6ug0BiVM/xr7av0SfU/tZT/5OYOnAycQbxZN92pf6oOrhPg+BkUIyf8qSC8FRsTRzBZ5zcjBVvxEs05h2iWX2uV9nLg/EPpcQsGjgZGXcp3p1zILlMmwiG0mfx8rbzlMcBVBsNpu6urraWWTSxW36za1L8tyuNExg6jvXJJBZcF1+Wk5O5uErPjsDS96jU2zGtVJMTqvpWE94MVOv/hrHsc7OziZtuLm5qXEc6+rqqu7vX9/ZKM/J5zayXt53BAkaAik33/PJcgjGDIW0Zf9yPPWdMzesk4d0nX5wf5plo3jomCSFgfrNGZTEAB9j6G4TbjPPJUcBFFJUZpdFVTvUpceROKr6MWQB/F/X8nNpnFUPTydijE3FdNpHo3ajVx3m+iT95nWrHvIt+3gj1o9eOxmsFJhrLNh/HTOQ8OYs0mt956sh2bcOFPLcDsAeEqa+IWCwfs4a2D+qu0IEznTwGRtpjcWc0yJw+HmpnLnworMJn0F554Di7u6ufvd3f7fOzs62ySK/2cdRODGNjp6TwjJs8DBCClk19Zau2NxP4+JsSkd9VT7r1IFGyncQZJiUZdLQQy6dz+O0FJhgN45jXV5ebtmB2uTLvbn15eEKVZT30UI6X3jFNmrLerD96T4KbdOCLv6ndqg/2M8OMDRWTYuu1+vJI/60z29fTwbO67hBd4yiavftYizb99E2uI868xxyFEAhyuuGKYPzzL13lA9EYhYuZCYpE819KYbV1lfJKSEoBRdjcQpeNTX6uXpyv8pgvZy5EKRSYispH/uWnpmrBdUWtZOglCg6wY2swsee9WW7mBB1kBYwpYVNBAqxUwdcvw71ye8pcaCgcTtAdInE5Pk5xerMd588RmIS7hyXWOa+chRAcXt7W9/85jfr/Py8Tk5Ottuzs7Odt1X5S1/95p+l3IaLG66zD33kgV0hSYGlvMpnqGxXZu1Pj6rTNVR330chE/FymZdQWWyjPlw05UYkZsEnMmkx1t3dXV1fX2/bSzbBMCIlQlPYyGMcKBKz8NDDP75mhqEWr8n+cQ+fcmaub8nTU+aYr3v+tOXCLl7HF72lsIN1P1SOAijEKDabzQ5l5X0Jq9Xrl+gS9TX46jh6xjmWkcT3y7uyXE8K6js/rrBqI7fy1s4K6OFTsm5u0BOr4G9ey+vMKVe2gTMgVdPH23muR+eKQTnjcEN1o3WAYDvc+AlGCSj8nMRmfKwTrU9Akca8YxRzQOHAQKGeJUDqAItlOngcIkcBFHd3d/U7v/M72/deKlchRqHv6/V6+8g8vktCyOqIz06t2n1/Q+pAAgCPlch4Pa9BpfQFXXNrM3izD8/xhV+dsicW4IlKnaN+8LUYbgxepuqiNQSnp6db1kKGkRas0fgTHZ4bAzd6v53eE5zO9HTdOYCgLNH9pA/enpTU3AcY5oCM5zmDSPkLv+Y7wyg2m029evVqazCKLaWUWhXH/zx+JN32OJKemr+XBo/iitIZLlkGjyHb8BkST8QRjAgw6ZruMfg/8wnME6jMtBTcvZBYARdKOcNR+wQ0KYzw66S+d7bmICkgIhgx/+BAscQiunq5wfnW2aH3H6ULLyhpbD3UdCZBp5cAgt/9/pGnyFEAxf39ff3e7/3eNg8h49c7LMUoGHrot7Y61z/OOjo0dgWpmldqHkdWoIGmYSaD13eFWP6gVY+tE80moLDOEh5Hg6JBK+9DJXWj5zkCDC1j5upFb6PXh3Xtcks8hwbJPlFOxNkCQz0ClYc8qa9YB+9DiZfJMUhgl67jY6NyU5jkToYfD4F4Leoo7eAQOQqg2Gw29fLly8l7IRwo9N/19fWESfg7LnnHH+e9U3jCKS5XFP90CkAw4OD7loYnD8/vKkcP7fGFSem9DXNem307DNOnbcuwfcqX53fJMTGycXy9aEt1SGsdUr/pN5OoKVHo+qG6EyicMXjIwb5J4YC3NwGVG3TKl7Bdc2Et6+mglpxDYp/7hs/U6e6pWI+RowGKV69ebacTOcNBoydgzAGEfvtj3XVOYh68XmIdDGOq5qedaBTuXR1UBAzy0J7HoDf1PIcrmr5XTWcWXOmqHsISMRmuq0heS1v1p3IWAiJnLewjemnPhaQYu2MW3jf8T5LCM29DGiduqZfOHnw2h683YB/5tZw9cKw53eszaBqXBHSpvu7opNuHytEAxSeffLJtJMMJB4AEEL51w0/MwhOiCWiY9EuLYKr6GFbHkFl08W2KTT1vkTL96cldOtepsQyauR4BhEKIqmlCzym7h1MqX6GVg2IK7zpQ8P7tmIV7XEkKV9L4+D5eh/knv54bMI186Xo+zj7Nm8bRx9T1xEEihSjep4fIUQBF1fQOUr0Be7N5vfpPeYvVarVdUiz2sQ9bcKDQlnmODigcVLqFNq7YnSITSFIfEP1p6EziuScSK/AZB/WRyqIwFFGyuOrhDWsp7PK2OFOYi6k74PCwT33M67A/qnYfQtP1pf7zEMrP6Tyu32vCfidwe//6WLNOGmMChICWujEMD08SJ/ilvFRiub6i+Z3IUTiVlpD28+MAQZagDkpJTZ1DxsJ8h7OM1WqaPCVgzYUpLp1XS14z0VWyA6eum81m8pJbJUbp6Xm/Q1XtKKUUUuf4GHgbeAzZRWpbAggeQ3DgeKd+S2ws9ZvXY24M0pjNMQqCc5p18WulkMPLcmbBZG0348XQq+t/f3frIXIUQEHlcLoskWHIOKXc8n584KsrH2c/yEo49aqcBhkLjUdb1YHr/pMnTFTX/3OjSecmbyJFoyJqqzqQiQzDMGEXibaSUvtxS6CWFFX7PcSQJHDt+qEDjPSfS8f45hiTGzd1TecSeBNweXkaK+q0Ty9XPTy3w8/3BLbq7NOoql/KNR0iRwEUq9WqLi8vt293JsUTzeOxVdM5YoYEZB36fXZ2toOwAgqxCE+cOqPQsRcXF5NwRTMznifpjIBeVO1x6p0Uu0uGyRMxuXh9fb1lGrxJy2c3VN4wDFsvVlXb6Wh5K58dSoAzl8Sco/9dP6TjXbpy0/9+TOpnN/wUehBMU14oCZkEx03/qSwPbZlL8rzUHDi5bbwzOYpheHibuR5llhJlVdOXCOs4MQx2DjtLKO0Um/GhMsxUCCX6dAxXNnq2WufSQ/h9FEyWUUGSJ3WQYRKLMTrbobK4cpLi51J89oKshe1SfTh2aV8CyI5lcSznjKBjBF09urp0ZXC/5w5SOWkMlwzYczhpP/ua40z9VX/5NbvxOFSOAijW63V95jOf2TKK6+vrnaXBfqsxEVbCDpFRemgwDMOWYfhycJ9SJbNQaKNl5mIh8r7dojAChr/hSgzGp7WcXbBtHoqM43Tthbb39/d1dna2zVn4w2c8CcdkMheDESwZ6yaGof3JMF1ZE/BL6eeMk9+ToXsSk78T0KXE8j7e2g06iecW/FgCBEGJDkXXkKPSPs/nef84034nkpkyqqrpG674vyfzJN13xuiedCQSk3GofA2aQIoDVFVbg7y7u5s8KUpGWvWQK6DH1wB623VMl6yjcvIYeXspBff5sU6TJd30G/MWXEPiMXVXT/3uJIVA7Cse49dMOYV0TWdaBFtnlulceu5k8HOhRupP/u/A0QHJYyWB9qEgUfUMQDEMw7qq/npV/dY4jj88DMNnq+rnq+pLVfUbVfVj4zh+NFfGer2uz33uc1smcXV1tX1git6fqf8US3NKkNuq3XhRrEOddnNzU6vV61We8vSe8CSTEAvRb+U9EqPgf+kcJlF5VyxnHFyhE7Nwz+5hgs96KH/h7yJVP7pC8/WLntlP9e0M1b/PhReJMXC/5zPmgMn/S4nZLtEqIbAyJOVsBBPMDi5sOz+e+9D+7jv3JXFAIGNNrO8p8hyM4s9U1a9V1aff/P5KVf3SOI5fHYbhK29+/9RcAavVql68eFHD8JCd18N21eCqqZeu2l2m6ysD/X/RanUemYRCBC5x5n9p9kOJTOYoxD44Ry7morqTHVU9PHpNbdB1VWcpSgpD3DOqriqXayKcrXlyLTEQ0tw5ZWW9KIk58Bpz53eGnxKePN5BVNfjsQ6mfj03UI4Zma1vHXATQKZyu2PYbx3Asu5zSdpD5CCgGIbhD1bVv1ZV/2lV/Ydvdv9IVf3gm+8/V1W/XAtAcXJyUp/73Oe2TOLq6qpubm62H+27v7/frhlgPoMoz/ltGkLVdOZgtVpNQguBFGdPxDacYYgdeK5C35X7UK4ircXQ8fp+fn6+/e15E+Uy6AW7uFz9KYDz0EoARmAjsxDYEoxoUALCJAzp9Ns9q3vgjnUEXdvbGBwoOvaxxCgSE6Cu8ZjEALxNCYgdgPibZbPf0ph7W5+LSUgOZRT/ZVX9J1X1Kez7/DiO36iqGsfxG8MwfPdSIev1uj796U9vPaA8+9XVVV1fX29XZ+o/Kr720+jFRnxA6TXpGaXg9J4EDwGGmI4Yx8nJwzswZNw0TD47UmVzv2Yn+LCequmzLlhvX6fReWAqCRkR26x9NF5SYkryep0CdiFGAohkYF7GUvu6PuiOnQMKL4t1V105I9Qdsy+rcKBIwND1TZLUzueSJwPFMAw/XFXfHMfxV4dh+MEnnP/lqvpyVdV3fdd31Wc+85m6uLiou7u77ZqK6+vrLatQjkIMQkyDj2ETo2Ac7lsyDDcIik9ByeAIRv6qOXl/LdbimgvlRNKMidjHyclJvXr1qk5OTurFixdbNsKl5Fwn4l4jKYcbgnt8byPfr6LQJhlQ8pCpD90TJsrO/fyPIM92piXe7mEZr3P/EoAkYb1SQtiBLyXdeY6Di7MTL5/ndGzL2YTkORKZVYcxij9aVf/6MAz/alVdVNWnh2H476vqt4dh+OIbNvHFqvpmOnkcx69V1deqqr73e793/I7v+I66uLiozeb1naS3t7d1fX29BQuFHtrqQTcCCoUi+u3goTBlHMeYCK2q6A0cTMQOlDvhYi1/yI4Aw5+ZQXDRViCirV5fwOeH+v0oaQl5Vf8YNRm+tsmIdT6Zgyvfkpdzr+n96UChrd8cpSR0AgEHCRoW/1e907Jwb1fKcbA9vlDK9ULbDihcHCA6BjUnfswc2zpEngwU4zj+dFX9dFXVG0bxH4/j+MeHYfjPqurHq+qrb7a/uFQWjWmz2Wwp/fn5+RYozs/Pt9ORvkbAZ0MSs1BeYxwfXnbDx7e5AidKKSG7kEhBeHMVE5/KicjQBQTMbwggOqBQ8pT3mygPIfDiJymMU28dxwSoQMXXONj4T3IX/D+FE13oQZCQx2YeIBkcDVjbdIznNbz9Xh71kWWkkMDP0X5f75PqzT7s2NhcG/y/uRDsueRtrKP4alX9wjAMP1FVX6+qH106YRiGLZtQJ+vj06OcQhXDELMQYHBq1cGDiTsuQvIkqCutx/hkIQICGbnyGj4t6jenMaTQ1s/RAi8lO5nw9CSpJ0B9CW9SLCU11Zaq6YxA1S71p4FyFkn7EqPwmNvHmqDd3Q3LctK1OlbxVKNR36X8jLMRz2N4fTqD9vBPx/q4KV+VWFAXRnH8DpVnAYpxHH+5Xs9u1DiO/29V/dBjy1DHsDPUAVJ8TkPygS8+G+JAwf/GcdzeC8EnffOFvMxn8BF1KWGVBl55DCm6BpoeWO1TeOL5DeUKxELEMDhjojBFgHF5eTmZZeFKUN7/4vWVqN9V587r0midUTgT2zef4WV3uQq/hjM+6U/n+Xnt7ruHJ+l/D2USSCTgSqCVQivtc2NPoZMD13OHHVVHsjJzGB7u9ajaVQSPX0lVlZtgjoIfLQXfbKY3S/Ecz2UQKPw6vj+FJQScYRi2xq/B8ylYLoxx1pESpv5EcrGOFy9ebLcnJyd1cXExeXlNAgxXQP2vfW7Yaq+OIagkkOA1EmDw2Ln8hbMYBxKv55znZjt8/Fwv2S7mgjyU0TFeNy+LfZ/GQOVzVs+XYbMc6s3byE1IjgIoqqb01j0ZB0xbzuuTYZBR6LuvGeA5urvy7u7hMfT68Fhdj9OwDFWqptSd7fJ4X+1g7oLsieCiYwgUt7e3cZp4HB+eVKXtZrOZ3Ama6KvqxO3cGNEofbw69rAP/SU7SUafykgA0NWddSLIeIKyC1lkjAxB/BoEn6UEawcU3m7vI/3nyet03HPJUQCFPHS629KVxr2IDzjzCh56KJ/BN175VOtcXsN/Czg8THGq7B4xJeiYE2Db/fZ3fsQWLi8va71e1wcffLDdnpyc1OXlZZ2dndXl5eXkLWxzC7hSDsA97pJBpfCi86KclRCYidZz4RjHnH265D3nACItnpqrMxmF7unx6/M28MQounJ5nMpO5eiYxEZTec8lRwEUVdM597mFRWQUHHj95uBrn3IRnFVRvoPPkLy5udnJeShc8QVe9PbOOnir/BJYuDcjs1JfeNJTeQoxCrWRjEQ32ZHac0HX+fn55HhXeM8xsI7uKdNYUshkNIadeI6E8TqZJY/jMdQTioNKapcAw5O42mqMOW4podkZeCcsa+68jm1T3gZIVB0JUGw2r9dO+PtFtXW0dJbhBqgyfcbCPb8v2tKMCmdPEusQgPixnj8hs+H1FFqwfpxWS/EtY1GBhljChx9+uGUQylGcnp5umcT5+XldXFzU6enpNp/Bt7JpxiTlLghqbgBdbJ+y8B7fU8FlIE7PBYKeICRYc0l6p1vUBxq+9vP6qV38n/mZdB0K9XQf0HBWqa33H/MY/M3rPrccBVAo9KDnccVhh+l3V5Y+rmj6nfIOiuV9XYZWgmo6VvulrNfX1zu5Dt4/4Yqq726UqiNBRv+xvVydKQYkQ9lsNlvWoOdIcNpRa090Dmdg1KYOLBhSsU2J/vu0Ho2TnjmdR5bAOnSzGGpnMsSU7/D+95BgXxawJJ5T28d4E0ho684y5Sp4znPLUQDF/f19ffTRR3V5eTl56Y+WSSuZJ8Wr2r2TNHWQe0CunnOj1Fb79NunXfkmb60eVW7DZ1MYvpCVkJ3ofNVLBiFPSW9OiqrPyclJvXz5sk5OTuqTTz6pk5OT+uCDD+rs7KxevnxZZ2dndXZ2tl2Hwa1mRjiDwjtZu77sZgucDah+w/Bwr4yMJgGFhEm6FL5533TMhnX22SreD6RtCre8Xt4ur7OD6Jyk3IezRo2Hru0sYq4fn1OOAig2m9dvCqt6/Si8y8vLSUenxJvHvClecxG4CDDkjTwJSvA4Pz+fsA4u/NI9KL4YTMDgN4Qx+URFrXp4ZgbjbdJtX+lHhsUZEn+61fn5+RbETk9PtzMm2vLmNiZKdQ0JjXUOKJjs89wElZyAQWGfeB1YXpfjcZHRk60w4ai6OAClMlV/bVk/Mj+WS0khjW9TglLXTCzo90uOAiju7u7qo48+2q5svL6+3sbYjKGJsGQY9LBOJT35U7W7/p9K51syDP3milHdEk92IKDQ/Sh6C9rV1dUOC9GNcJ988knd3t5ub7WXwWumhvG4f/QgHoVVOv/6+nrLJvw29ouLiwmj8Ef8eV9W7c6CELzo4aTYBB7ddcv3o7gn5RilceOYsi4d9eZYMhktwPQQ0R2JgyG9u3vylIvwfXM5HekjP+o7ZxCeA/r9kKMAis1mU9/+9re3cbLu89BWqyZXq4flylIQKp4UomqXZczlNLTt6DW3pK6601W5CTGKV69e1f39fb18+XJ7fwq3BBeBh1gGl4ALGG5ubrb1JViQ5jL2Zz/6fSIKObQ4SwChRCjfcUIF9TCoaveGOXpcztBo3DhT5EDP8fJVj846aHDOTHycmZ/izJcMjnkMsUjpkocgNFJnXV0YxrHqmIrqzkVxBAoHiZTAnAtzngNMjgIo7u/v65NPPplQZz2LQqsL5REJGMPwsGgp3d/gCthllCVUjOS5x3HcPr5OW854XFxc1P39fb148WLy++LiYssebm9v69WrV9vb5wWIaptAUTfBnZ+fb0Mc5kGY8HRh2MYE68nJSV1dXdXp6el2STiXjDNXkYDCPaOHCDQieW6/k5ZL1hnrk167cXhCl9fkcekYjmUyahovZ56klzRArpDs2KpvyXoc4L3cpL/Ourx9rF8X2jyHHA1QfOtb36qrq6utMnMJssIQTe9xKpCUlkkgdrhn3l2x9qFwVChXKi77ZjLzgw8+2OZfbm9v6+XLl9tb5sUqxCjETBR6CGQEKq9evdp+1wxMSniqPwWiDpT+pC5fpOVLvlOyzPuCOQglLuUN/Unnfuere1EfQ3pvn5XhWPp/rKsDm/qF7EKsg/3pS8P3SSC6PnFs0lLzqofcBMGzA4w0DpIuJ/IcgHEUQDGO4/ZBt+pMeWwtrd5sHpZccyvA0G99qOyil0RuTtHNhSmJ/jKhRm+oa4nmaqt2nZycbJOKZ2dndXNzs92qTWIWTDwqbLm4uKjr6+tt7kOLwfz1dh4bc5/yQDIMUn32q/qwo7hkWe7Z1VZNw3o/kHUQ1MlEBDh+joco9NBe16r+3guOL8fLE8nUgRQWuZ44c1V/kI11SdKUtNwnF5J+U7rcyGPkKIBis3n9NnMl5dLTrMUsPBknoBDjUIiSEp/+Ri8aQgpTfNA5YD4NRkPlwi5NrSokUSJU4YQYxcuXL7d5DeY3xEb4EB/mN7jGQ0lUMRsuL2dClm199epVrdfrevny5c40KfspCWN6KrrGijkKf8kzQYEgIYDxceOWbEfnCbBYV2db9OpV09vpBfy+9oIGSH1wQPJwVmUykcq1QhS+iS3ppLcnif/XhSpPlaMACs4wMANcNZ2OkwFoWlNbKYryG0rcSeFIL7V12suwpIs3JRwAH/gujiftpZL7VCWTbozrZUCaERLLUH+IcREwdBu6Foklr8NZARqbgG8JKKpqkncQMLHNnkAchmEnuam+UBiXko+cahaoEARVb46Fe3JPVnPMnkM6nZnbR4a0ZNwaoxRmeL7inctRKLPPDlutVnV1dbWlpp49l+cj6xCj4HfFyfRU3dvMeW3+5pZA0wGGx7ii4Ofn51vAo9fnik8txtKUKvMafCwgZ1c4/So2oqSpzuGSdJ8xqaq6urraaS9pdoqRGbvzHN6PwmdjsM88HmeeqctnkGl07EN1drByME8GncKOpKtV0xkf7x9PrNJJibFoP0MqgvJSPmIpPyEweacYRVV/uzC9gtM+DQBX3cn46G21FbjwXLIReSgfaG7JPHwg9L+DBr018xr0nPSg+s0ZCRmfEr3KNfABN4rnq2r7mzkAtV19xH6W0Hu7Qc15KLXdDZK0nnSY/aE6ihWxD2lcnngUIHMsyES9fWm8OnpP9rEk1M+l3IHao+t7yKLvXRn75BtYj+diS0cBFKS7VdM3M1VNlyzL88mD+IIsf4MX424xCz0JSvkNj8uZ3+C26mGVKGmi5ze8XTKwbm2G2AfzHHoQMB+2o9yGZkyUx9DU8scff7z9TTai/IYSoVrzwUQoZwck7jWXRAbP1Z4CMve2Gk+OGxmJ9z0ZIGdV5laTuo5p61TfvfhjjczLIuP0/EjqT+bDOtDS733q5EnY5wCLowCKqumKu6rdNQ3yJkw2udeRVxYllWfmuTxWTELrIhgfp3M8zyFlpzfr43z7mAAAIABJREFUaG3V1Eg8plY5AopUX963wSXZYhbDMGy3CuWYaBTIKRciI+Y9Lol+P8azanxkFA6eKQelcfPFdCzLx9ynpdW2TsgkVSdPXKpu+7Y1MU5nFWnNCWUJgN0eHmv07xSjWK1W23UD9KrucRkSkFoPw1DX19dbei2G4Z4oPbBWW82o+ANstbDLZ0zIPmSIKZ+h9rlH41Rc1W587HkO3sLurygQS/j2t79dt7e39fHHH9f19XV98sknWxah2RXlNbQ0nAyDxud1YB2r8jSf2qgxoEdNoYAAjUzCt4zhlbNyZsEkaUr0OYugnqT8ibcnGTNZQ9X06eVqu48tyyUoMQybY5/a34nXU/V5Z6ZHGUpwdoANTdQtxa9SAq5GZKyveF4e1dc2+CPxmTRl8s0TaswHkGGkGN+3Lik8EeMRWMgDc6aEMyham3F1dbVlIrwlnv2y2TzcYs/nhhKkqvoX2KTxpNKyLVRaja/nalQvGTKN0NmjJ587/SK9J0BQh3iMt6MzfI6h51860JKOkJXw+z4zNG78SZdSyPNUOQqgWK1W9alPfWon85+8W4euabAJHPQiV1dXMT5mXsNZSHqMPm+JJ8PwTDynYVkv94SUlL1X28m8lL8QS7i7u6sPPvhgm7O4urraMgrmKJTfIMMgSyEoMSxJ7MAVOcXZvEfGx07Hcay70INJX+9PL69qynII5nPMxXNPLMfL9vCT+5OHZ31Uls51Fs08HR0o68A6+fVYxqEhyFEAxXr9egmxFF0DymlENZZz61W7NyYlj+AJSP0mQHTTbT4lpy2f68DpWLEQgow/Pl9KwjryexpwhiqeAOVqTjENbbUwSwlQ3aXKhCfDF06lOrPoVn5SeQkanpSml/Q2MgekMjXGTAiLGdLYU26A/asyHCg0lly3Qj1hwjoJAUEsR3X0hKKDitiQOwT2k4PGY8IIMjgxwkPkKIBCjEILiQQYpMLySElpO6ZBpGZeQ1uFIp53SPkG5Sp0DFce+kNhuEp0vX54+5dmXQhMMhAZg4MFv9MYuoQnp4TVn5eXl1vj1+saOWPiuQsBhrO67jb39GG4SNajredkkk5IOmNVWETn4OEQQ1T1H8GOSVfmVbhN7EDn8BpkPt0Yciy9jWzXU8RzSBy3Q+UogGK9fv02c8XcUnB/LqUWJjG5x49oqgOHdxSTjUxwOZ2l0Xr44MxC92KcnJxsw5IPPvhgu9V+5QsIOMxr0EPuwzTINrQGQU/W0p2oXNTlTyDnDWu87V19r/MIFCnZXLWbi+C5qpPfl+Ke0sduTlL44nXyGR+Bs44j+9QYJ6Dw0MZnSKR/yq9wzMh4Us4igUSXo+jEGQnt4p0JPVar1fYuRl9kJFZBLynFE2DQc8l7MInjCSCuzxC4OHhQgap2wUWMQrdtn52dbd9ELkbBKcyzs7Otoeg3p3AdMDrAmvNsqrP6SH3hS9tlwLwJjXer+sN1unwR42c3ei5+I6NIQJFAIhm9e3COqYc0nkdgnyYPPyfev9qXPmm8/P9ubNN1yYj2qe9TgWZJjgIoTk5O6ju/8zu3Sqp4Wh9O3/Hht0yS+Vu/RMUdUekRkywNhGJKJi7FDGT8mn7Vsys//PDDOjs7q0996lN1eXlZl5eXk+dscKkzZ1e4TYnPFKZUTZ812XkXPhl8s5m+RY0sTuyDxk+gcAbhMzN8SDGZhYeSLNPDy07RHUwceAj2adajm3VK0vV9Sp53Y8Tl6p5fIRi4rpHhsO1JEmCrrw+RowAKJfzk/bRPH7EMJZ38Zi+nfb4ewOPOqt3X3nkm3/e5V+MaDT69m0vGq2q78ElbGQ6NhDe36RqMqQUYpMKsyxzLqHpIBrJM9onnOWTQSorSqLllTohAoXMY4ohROVCkENKBgjeaeW6D4JBCFoKnMzWGm/zP81Ma6wQAzg7S2DhI+NoPJk29DSpP+Q/XTYkDR2Jmh8jRAMWHH364BYsPPvhgG09rS2ZBBuFMg0m4pJQyyOQhk7L6uT5AzDcoX6Echd4B+u1vf7tOT0/rww8/rIuLi3rx4kVdXl7WixcvtqxDj6bzm9183cYSdaUypWRZCsMYuhEUHNC8D7hNrIBM0MeN06W6hvc181FkDF5nZzlJv1Jo4P2n35wlc1Dh7IuHNSmEYJmcclf4yvUf0j2GVcylsA88R6L9nu94p3IU6siq3fxBQl6hK5Vd/+tcJTA9T+GMQvkQjwHdU7mXkyj29xyKpl65IEoAxWuICUlxqqZPC3fDJDNIeRWVScbg3o1JPNZD+1VXz0n4GHlfEmhl9EoeKrHqSU2/i5bg7UCRACKFMK5bbHsHEup36pxPp6f8g4+BX5u66yEH1224OGCoPLE+AmIXlu0TquwrRwEUq9V0CTcHn56H8TKXICvGJgvh8yU9XuaUK6cRVb7Hg/RavE2bhijF4i3Wn/rUp+rk5KQ+/elP1+npab18+bLOz8/r448/3r7F68WLF1tGIRayXj+87csf4pNuXOsUOykxlZzhjjMNj/1TcjElzujFOJYyes+REIwIEB2okOX5OLpxJFkK0wgUDhBkDe7EtPXrk1EwXKVjk7gz4oeJY5Xr4MxEPseFYeJT5SiAomr3hT7yvGQQGigyiaqH6U8ahrMPnkuP6lRN9Zjr2JQ0o7KrbpwCTYzCGZGets07VeWVSelVNpkF8xmqk/qC3smNwr+Tiait/Ghft3XjV5vZhqrauWuV5zAE8elZAoech74vjdu+kphEAgXmJNLCKfWnxogLvcgkuryYA7PGkeNDJrkPs3iqHAVQ0JBlrJ23UR5DLwniugDmMzi155l9vtXc8xzM0jNsECVXqKB6k+5ycKVAMnpNl+oZmVrRqXzG5eVlnZyc1Mcff1zr9Xr7W2szdDwfM+cxLx/I4ytOPVFHQFFbuKWkRFmSuUSjMxQP6ebyQ2QhVbWT19B2rt7puwO+9tP4PbRLMynubBJYcBWmz7bQYXjfEXydybKP/Prpc4gcBVBIfFpPDIC0Svv425dXa/2FAIPnKB9Aox+Gh9uzCQQECd6iXbX7KPeqKeJ7nM68ioBQgy5KzdkPHccbuPzmNSU+yVZ0fb+G9nnOgt5J+9L3qv3j3ER53RjdGNK0q+dIPNHKfMYSA/QtP37Tm4NBYhAdsPq1dIwYip/D48k4tXXgUFnMW3Tg/lxsoupAoBiG4TNV9bNV9QNVNVbVv1NVv15VP19VX6qq36iqHxvH8aO5crxBzOjS8NiZnFrcbB7es6F3Z3AGxPMa3CoWFpvw/Ia/Z0PnaM0H8yhqg+cFBDbKbwiolBfxuz6Vs2Fugjeg8TFz3PL2+dVqNXkEIPMbZCFLC82ohGkWxX+n+Jj7OeZV8zmRtMaC7JLgMmcQybN23jq1yROYCUy9HS5kI6qTAE6GLz2hkyR7YB+L4Xp/zbGSQ+RQRvEzVfW/jOP4bwzDcFZVL6rqz1bVL43j+NVhGL5SVV+pqp9aKohxtIRJN219PQDn/+n9laQUEPC3PDTvjdACIz28lrdhM/SQlxcL4crFzqtpEGmEVFBO23LGQQbNpd4OGOv1wxPJdU+JEqJqm1iILxlXmLLkJd1Aui3bq63GqesXN+A5o05Mw2dguvqkcplPolHpnNTOzoO7YaYyCCr6j3rtYCXdZh2kh95OB4XnZBNVBwDFMAyfrqp/sar+RFXVOI43VXUzDMOPVNUPvjns56rql2sBKGQsnlV+c53Jtmp3nT29oQyEwMBl3/f3D3dXJkYhFuK3Xeu3Zmf4cmK/D8UHyGcl6EFEe/mkbJ+i9ESYcg7KSeiBLnr0vn7r3a0EigQ82jIx6ok79vvSGDn9XgpnHCi4n9+ZaHaj6Mr3a/Djxr2P9+1A8TFGSiaQ6utld3UmWOrD/d5Ph8ghjOIfr6p/UFX/zTAM/2RV/WpV/Zmq+vw4jt9408BvDMPw3UsF0auSDnf0do5piF2wo/gUbAKGXhDMRKgAg/P+Cj2YPOUNa/ykUGSu3cx1eFvEWsSSOAUrtiFWcHJyMnlHBxOkFxcXk2Qo2Yg/mEfAoQ/Bgl6wahc49jGwlPzjtvuejMWP5XW8n70MZxbJI6fyU9kd2KRzCSxL5XZ11nU6cPCVtM5UniKHAMVJVf3TVfWnxnH8lWEYfqZehxl7yTAMX66qL1dVfeELX9g+kYmhhcd1lDQQVGomnxj3keZ56HF+fr4FAIYtm81muwRZMydkEz59lxYIqc5MmBEUVX8qgnIZ9Kj6zn7abB7WFChno3Ulp6ev3wXCW+CV1+CCMN8SmLqVoB2z8DHRtovv9/F2DlQet3fX73SGRuchQQL59JvjxHK4P13bzycz4D7pDo2eTFT/i80ywesA8g+TUfxmVf3mOI6/8ub3X6zXQPHbwzB88Q2b+GJVfTOdPI7j16rqa1VV3/d93zfKAGU8vkqu80RV07lk/ee/x/FhVoFMgx0vYOANUX6bNKdaNUB+ZyTvbO2Sb65IDKcYl0oBNWPDMEbfxTr03FB/MjkTmnwSuT87gwBCZsEb1KqmD4HZJ38xByqPMXRnMuqvjq0EndvRiaqcUKWOJQN3Q6eI1XbleMgwpyeex9L/CnOlpwyBmShP9XuKPBkoxnH8f4Zh+HvDMHzvOI6/XlU/VFV/+83nx6vqq2+2v7hHWdtYnQuuGJOmBUOpHEkXtpDm6zdzGzJIJUQ1IApfmOfQ4BAo9J8GUd6ejMNjxzmjSspPoGFZ+s0wTOEE28TpV7EQbpkL0WcYpq/yI1Awj9HVP63b6JhIBx5deOIyF350x3v4l6RjBPo9lzvpQgN+UsjgLIMs1deReDnPlZ+oOnzW409V1V8YXs94/N2q+pNVtaqqXxiG4Seq6utV9aNLhdzfv36+o79JO8XJj8nQ+39CeTELfTgYHIiE6L6knIu0yDqY3yBLkaJQQZIBLAEdQ5hubYgvFGLuQcxBU6rpHa7pUYECCF+85WDgod/S4qUuRJljCX6Mg26SxBJYHkNU3+o8TyhSPxgq6ngd6zcrcqq9yy04UPB6ZLjpuSEOGIfIQUAxjuPfrKp/Jvz1Q48sZ7IwqiqvpdD/yRsx4UbpDM7L4ncqh+rBJciqr1iEvnP6lUlVzWL4tCxnPRL6z7WHRudel3Ez+4bTx+P4kPMggCok4wIvhiFcfObGT3AgGBAo5oDfz0+An77vcyz7x787A+h+c4zo4ZODIaCTATAsSM/76HIVvvUchTsfP/dQOYqVmZvNZsIo5L24TDkl06p2b/d1b8ZjeE5V/6TrFMfyNzufA+PMQp5Cz6dMzwJVPoPKwTp1cXLV/FQgy+Hx7unTU8e5XsOnUIdh+lbx1PfMofC+ibTfQYXjpvFJK0oTKHGsJUsMw/uIfedhBA3Pmabfu8LzNL4yZj4JXVPvAooESA480rtxHCcgQ13sdOCpchRAoQ4no2Dj0vx+RzfFAjgr4IY1xzJSfOl1daXS4OhaYhZsjxKO2q7XD2/qSh5pH8Dw32RdrGtiKr4+grkL/dbTrv21A3MPXiFQ+PqPOTbiziCNOeteNQUQdwbd1sec0vUbjZXjRBZAoOjARQ5CIYdmzpTI74DGx98ZRZf/IpN4J4BC3ldZfSokGQYVwhVOSsXvrsgOKul7EiqnCwfUk1QffvjhRCHEKPSGcX2oaD6lyviyUxpXLtLhVB7bxTtW2XfsQzdqeXr33jRWhiliKtqfWAX3EVw6UElg0o2rs1Bnlj72iQ3QCD0PQBbpQE/mwTe7kUn4owsSk/X6pVxaYjKpfU+RowCKqodbrT07XvXwPAgqLr2nOkJsQt8JDomBVO0391/1QIMdVDgo8shSJHnmqtrSd76TQkpONuL0s6Oersjdsamu+s7ydGyalp5LVqZ+o/GmZ2eksCUtKPMQx7cp2d0xiRR+OoBQElDoOz9MWHsikmPCvITfc6QxT2PYiSdUO5B4LjkKoGCMJ+NarabrBlJSjPtogFJOlSMjYJIveSiCiVPzlGAjQCXDFcPQeg1PgPKx+KKfXXJM1/HrOWtgLMwH96g/OiBSGUuUPe1zpWS/yqj5DIYUcvC7sxGCDsGM4MJyWU9nlc5ivA4UgXxKRPoMF2fFHKwZeohh6rePcerPBGgptOF+jpOc3CFyNEBBauwzBq4cw/CQwddqQhkjkZWLXpZiVypZYhxJwV2xNFhiBwQlgqAvZNKMCBXOM9cs30MOJlN5narauf+E57HMzpM5SKX9yXu58fOxb3MgnRKeKSzqclYUZy4s28vlMd4f6mOuWfD7e1KOgv3DZCPX19DIve787kzbx83HRedJ9w6VowCKqilYECQ0qEwOOmWlISqRqAVGTvF5rNNtV1b9p8EUAKi+DjZLq/GYJNRUqaYkFaem9150IYP+46P3VT6fY5FWj/KcBCKJPmt8dDzBTP95f7Af+Z8rPg3eQcTZgBs/y/HQkmU4wBB0EvgnhujGnpiEGy7ZHs/xsXXAS8C6r3g7HnNukqMBCkc9goKHBUTYZPwCCO0j/WVOQArioERmwOuyPqynhyH8j0ZIxVQ4oJBA9dUKUAcKlk2D9hBH1FYsRaCkawqsBL4JFHQND2U8oy6AoDeVeOKzU9TE0DgtqmM89PC8Cfua19PYpOlZZy4+DZvCMwcKTxZ3zIu5K62dcUn5lqV+dGCZ6+tD5GiAgpIAQh9nFlV5XYQGl4lPHUuASIrnypfAQJ8uZOmUVqxE902w/gxXdFyi/u7pCIoOnGRVzFUQMMVoPBzxtqhepNv0pmQVEo+PnSqzjzy/oL5R37th+zksm6DDfmDeYwkoOM4e5jmT0H6OJ4XAr36i3jA3x3roO/slAQJ11Y99p3IU7o3cQPXRcxOlQMwBMP5nxzM8IXXtYtSUCGW9SF29rklc+Qh49PA0eippikFTaMAH9Chvw9voOUXXrR5V3VRvXlPKLsNgzE5jSUDgRpLa5m1k/6n/q3JyLuVYPGxhojtNvaenYzub4AIrz2Fo/FKdVZaEDomzOx6u0Xm4U+pCk87JHiJHARQuUtyqXU/q++kVHVHlQat2F1U5yBAkFL7Ii5Bt6Bx6ow4o6IndMDjAZBDODJIxsd78Lm9ZtRv704MKbD28YZZ/GB7uGaGCCgzUbj/HKThDGYGR94ezpjnWwbZRXzx0ckaS1mk4eFAHvGyxCJ+NcoDyfk/1VZ8KIJ4iHRgkAEkM57FyFECRKGQCCzd2ejp5BTdkMge/WapbZEQvI3SXIXSxblV+PyXr71unhqT+fq7EDUtKS3bj92xwWk/Zeq0O1GyRZl3SO179WRdkI2JxjN9psMxdpPApJU/nwIQyl3Bl6KGts8c07g74XgfWzXWY13EPr60nVeWYfBYn1SkxBg+Z+Z9PHx8iRwEUVdNklDzqnDgrcCOkwsiQJOo4UUVnFvRKZDHu/QlYDG0S3UvhQ2cAOjf9t9SH2noCkB+u1ZCn1LaqtoyDisjcUNXuO0l8jQYNit5Y7fa3sznbSKzCAcfBhgDF8eNvskeON0OvLu+REss+Zj4Ovs+P78CDvzumwHLIHBw8CFpPlaMCCjWGbCJJAgnG+W5onHbltRyt5zLiXTLMp9vce/B6c97KDWPOmzqt934U+JGRMet+f//w3FB/fqgYRXqH69IbvEjL/XrX19cTgFCuxI/188Zx+pJittnbNefpHcSTAXaMIvV5MlR6bgcoAoIeO+hPG2MIRGbi19NvZw+e5GUi950CiqopCDxWpDgyFM4AyOtV7c4yqGPFMBj6aEvPpUHl9ThwTLTyeq5cCSjmKHcKO9yLsj8dOAli9KJqJzP5aoe++zMv2LfsMw8nxFZ0DR2rshwcPDz0cIyML41fpzfJUDrG4uPnTIb9l/o2AZI7Fn8w0Ho9vbeF57FcfXedSjpIVnmoHAVQsOFzMamjfTf4nKZz2pW+E6V9BsTzGf60J89Z8BzGm6x/ooIOGB1wVGUKnvrIlViKypyE8g16JiiXJKcndPkaAhp7yjt0jwf0JzP5Mxo8X8JwhiGOT82mKVqNrSQBa8opSHyWhWPoMybUCw8rBAh6O9wHH3wwYRiep0i6QrByHXO983tmDpGjAIqqPA+8dHwnNBwPYzpqqv/kuTwp5XkIZxwcKJ9mozdkGR1YOGCk/9gW9luqs/cvvRRDBXrvFMcz8Uaj9+QjWYJPwxIoWIaSpnxhtK6rsEUA4qwvAeyczOUZ+H2OnaQp1dXq4S5Z7ncWoZAjPfk8JSVT3enckoPyWZy5UH4fORqg0OBX5bscq/r7NPw4ilPzFO8nD+yDReQm4+C8d1qf4ZQzlc26eOLM435nGuwbKowzHLaPgKHfMmhPcJIt+CIjX7bsQKFy9OmYBRkGWQbvp1BuxM/V9y4MUx+RcTigdMDhfZzCCBk73//qayHEJsQo/DGEAoq5xOMcuJE9dIziUDkKoPDQY+lYbveRTiG6TpfX53V8mtSRXPH8avXwcJrO66QYVPVxoOAdhsl7s4683jg+PBA4tVGsxvvDczUEK308eSnhTAgNmeFDerWB5ygIEB2YMFzRNdWGBBYEXJ/C9bUfPgaSNCvhTy33VzX6OUpeartarbZbOhDX3S7c9BxF+rwzQFE1TSDRQ8wBQsco0vfEUtxjsEyP9T3Jxu+J8nXxq8eTXIPB+kiZuUxaXtoV272K6LkMXttEZXVdiY5nHyUl5X6P//Wfz5CM4/TRbQ4UDip+rj/FyftibqwJej6rwm06x3M8zDeIUfiaheTdHVwEEAKXlI8gS/PxUN8v6aLKP0SOAijUMILDvvGmSwcS3DIWn4tDq3ZX1Tnq8ze9iMBFhkug6JKmrCOBogsFPPPOmJ35EdY9UVvV+bF97X1FhqL6+V2x6T4RDzkIJmIYiVUxFJM422J/sh/ZlwQRnpP6zkM7PmvUQ1MPXck6+DxYbXUcgZkhHOvkTsJzFHQc7wxQVNU2C8+BZkIpZaqrph3Gc5a8i//Xlev/e3KUysCwhMAgwBArSTMkzKxTeflSF1dsCdmYgEJTYgo/1Df+ZCkqkH/v2FX3P3MgqqPecZLWWHhS03MTPjOSQqE0y0EdImil8IXl+UNnqFvqZ65NkJGLNaZEJJ2IziWTUFk+/mwf+4n/se8ToyBQvBPJTCKxd4aHIG64VBQCi47tQo70/7519Wtq68ju+YzEIpxRSFyxZUgeClA5yCiYR9AybbEGB4yqfG+IK7sf40yKQMHwg8bobM7Zgdrq7MM9fzd2rkNMxM7lO7xcd0xO6bkAj+ElGRu3zFM4C+HtBxx7AioZmI6h3qXFg7zeO8Eo6FFT6NEpxRyAuDwGDOaO5TV5rPbJWGVUbAv/o1Kl6auk4G5s7CsqiG6G8+ceqA89L+JG7u1MHrJjJx5+CJw6oPAZCwcX/qaRJMaXgCr1I8GX3jr1L8eWgN+9usBZmoRJTT9X5coBEShUR9VHY+ZA4WDxzgKFUJZ00pWoYxk8VuVVPQ4cePxjWYa3JTEK98zOQLpEls8KyHh4PXokKTvXOuiBNvKAiTJXTW99Tm1yYOsWCLENKR+UQgNnAVXVGq6zRvfg/C+BTreWo5vNUbnpAcDpaVtqL+uamKXX2/uBTJI5PNpI0i9O179zOQp18mYzfcsVQUEdJJpdlVdapkGWsCw/h96Zxz9GPNRh/Z2Wkm04WCTv4vGphHkTlac+GoZhwixYvhJnydi831M/JMbh/0vBWb+ujwRuqhONw8fCx49bHk+j8wQmmQqBwkMP1Z15Bm55TKcHZCUJVDUmOkbnC4wcPJ1JJjD39TuHyFEAxTAM26QXB5UJztRBSXmZP1CHuxCUtPVyfdsBRtrvYKeBFMglZkFFYrlUanpYp/kMbQiiNA7R2/X64cE83lYyjKTsbrw+jglwun7iNfz/fVlduqb3nwMGE6PUrQ6IU9jVTTdL/Lpz9dUxXHHKMaY+eX6GIa7PyDiYHSJHAxQEBQkHW787j09D62ZIXJwmP0Z43XRu9z+9ANvig+lAsW/9qETOLlQe6yXAIMPQ/xIHVsbJBOO5OnbhYGfk+3jAOUZDQyKQysjZr2zfvkDhs0Z0KARf75/ULme13XWrameMvC5pRu3QGY+qIwGK1WpVFxcXE1rodFv/MfbmgJBCp1wHt67Y9JLdMa5Ac8aRjnNxhSDbSGURLNyLuDKoHZqOJSAo0UkjF5MQ0/Ab3shU0iyJDEKGyPZ1YUnXF96+rr+63w5Ic8lT79uOxbA+SwxGOuezOTzGr7XUNwIA12/VzcMZX8/zzjGKqtoqMxvOxE1VP+3p1JuLiIjuc+EL4+QleSoLSeLhRMdUJO5l5kBG4UaKuwkcKvfu7uFR/3yWgbYMA1lXj4Wp1J2BzbXLf89tE6Pgdw9D3VDnQp3k2dO1kh56KEnpnFC6frd1R+Gg9hz5iaojAgrdTKPcBOMxMg0xCbIGMoxu2q1qvxWaCYTITiRO8dNg78tAXAGTJ030V4rgysIyPAThVKwARIyGU6tiIHzcv85VFt4z/4kKU4GXWIG2zlb4/5w4w0z9TODSOZ0xd4Dk1/RPx7JcH8mWq2pWh71+TKK6Tvhs1DsTelTVhFrR44uSi2VU9aEE9ydm4B3eeQWnpU5neS2XJc/RnevG0dFu/U7epKPfFKfGOt5j6XEct+CgOqtfdQw9MetCdqT9zkCS0fn5btDpe/ovAfRcX3Zg5AbmAExx3Z2Tzsm4c5tzQksOJLGgQ+QogGIYHuZ81UlEVc6ApEU4ZB1kGanDq3bjfg9RiPIOSp7Q8+t115Ik9jIM05cOpVCChscpryV6ybpQyUmPpdxkC2kq0F+kxGXLqnsCMXk5tsnBLZWTvOUcE2F/d9sUOrgsGZY7EuqpT71ytkrHe/6Cd8A6o+A1U/1+P0Ci6siAQh0lBZYSMyvvybWq6XQkp/34vWr3dmKf6iOo0Hh4Lr2oBpNZdArLcUmgwjakBJQb4dwcvl+D9XEmRuORQfssQQIV/t+dOjI6AAAbgElEQVQZNt8V62EFr81MPfttTvnnjMDb6H3oZexrUB0T6Na9+FQsgcAf/OPHJunq6QzDvx8qBwHFMAz/QVX9u1U1VtXfqqo/WVUvqurnq+pLVfUbVfVj4zh+tFDOhKISIKiwUlB6ccXTziYYfjjlpxJx8PQf9zmDcLbihkM2RCE4pdCDdZnzGG6UOobX8PJc8QQENEaCiAzb703RG+MVBvoSbm7FRHgjnNrvoKL9HQvxYzvjnmNyEu+3rlwCancd6ki37Jy6WVUT8PD7OMg8HMTT9Pk+IND1w2PlyUAxDMMfqKo/XVV/ZBzHV8Mw/EJV/bGq+iNV9UvjOH51GIavVNVXquqnFivyZvVZxygcCJgwEjo7UDiDmAMKIr2Xo99VNbmeswvVlU+NTgPqYJFAhRlzB4tkZFW7z4tMgMc6JICgwfozOLg0mGU4k2DIonP87soOKPj/UrjibeXYer9633Vhjfe3X8/ZZxcCp/CX5wggtPU1RLwu2/2Yqc7nAomqw0OPk6q6HIbhtl4zib9fVT9dVT/45v+fq6pfrgWg4CDSSBzZdWxCXx3nZeh/Zxb87eDD7zIyZxS6nhuiWI57Bgekjlmojel7J24ciRXNhUUJMAga2qcylMdwIHMjZ14jhSkdcPC6LH/OcH2M3SOnOlbt5ka49Wsmx5IYRGISHIt0dyxZrYRhIO1jSTimzyVPBopxHH9rGIb/vKq+XlWvquqvjuP4V4dh+Pw4jt94c8w3hmH47n3KY5igRlK5HJVpyG4cyVgcGN7Ub/vbB9UZhSecOGWr43wRGEGtavrEJG/n3KAmJtF5OV7HE2c81o0uGVDyuDwv0Xid689t8OeJEigcNNLNZuwLb7cbY2pnAqKlNns/sewu5OB/HVPl3as8V2WTRfs7OZxVJDaaHOWhckjo8Y9V1Y9U1R+uqv+vqv7HYRj++CPO/3JVfbmq6nu+53v8v6p6oOgEhqr5zLaDjTMFxuY814/xcnljDpVQidaq2q7/8IfOUhILGoZhJ2yYU1qXjrWkRFoHFPpO9pDAws/l+VLMtPLTtzRWz0WoDgpxuvY7g3Bmx3M6UEqA0bUvOaJ9HAjPlUPhbIizY2dxnJLuxHXe9eJQOST0+Jeq6v8ax/EfVFUNw/CXqupfqKrfHobhi2/YxBer6pvp5HEcv1ZVX6uq+oEf+IFtS5zqOeXT/x2T4OAkg+8+XUzpU1lMonZxqv7zBTQEI2csBEI3XlJ2/cfB93rMZdH9pjleww1kDiCSuPERCG5ubiagkJiFt9Xj8QQUbpQJEMlS+D29zT71B6/HazJ88DHwMfJxur+/n7wRnW30sI5jxXYn56nruN0cKocAxder6p8fhuFFvQ49fqiq/npVfVJVP15VX32z/cVDKymhkbjRML5jorNqGsN7B7IjGRpoalXl+7lusKKJjKu9Xtyna5DSsjwd3yktj0+eLn08xNH11Wf7AkUnDBVT/+q7z4ZI6T3B2ik4+5zj5u1nW8iQmCOi91b7BFBzwCQQdvboYW0aq451uL491cCPilGM4/grwzD8xar6G1V1V1X/e71mCB9W1S8Mw/AT9RpMfnSf8jpjqepX09ETJwDwsp0GJqahwZPRu6cgK3Bm4KzDy/CynL1U7c5ceLKPxtQxmjRjM9e/7GMHBgcQP98NsgsrOKWachL06J2hStwYnVlwbFWmP9g4hRxL1019nZ4i5v3qDotl+LnOJvcxcra5A7dD5aBZj3Ec/1xV/TnbfV2v2cVjytmiuu+fkzkPl8AkZY3T4NCrsR6KF9070yO5V5DHIh0UmyDAecjl9WHZXf8lFkFQmutXb6+zJ/ZdBzoy+DQW+qR4PAG+zkvizKjLCXj9yHZSOEeG1YFiykXMietcx+7mZO4416HUbh//p8hRrMx0dN2X6qbj2SlunNyf2EcCKp8m9WdAcuDJRpYYhBuzKzrDoNRf2rpX5TYlVZMidX3J72Q6HegQMD0HwQQl8wRpdqPLD/A66Xdqp0+DOjPjMWx/uiaBgePN83x2JbXFw6N07X2lG1dew1nqU+QogKJqevPVnHTHJBbQKdsctXR2wXolhuFel0kmZw0CGno9xslUvm5qywfeFbsDpE6WGNbcOX6u+kozQcwJqK7c79dMsw4JlNLCMgcP1tHHzXWlYwUOyMlBSFiWgFD7O0e2pI+d7AOi7sAOlaMAinEct6/hq+oXvOjYrozHiCtLJx01duWcYwxzx2mfA0VXDoGFT2hOfeX5FG+vM619+jIpOq9ZtesxndUJPPzmNgfpVKdUN+9LiucHkjNI7fZ2pk+6DsMvtjGVS/E+4O/UF0mHqqZrgjpQe4ocBVBU7ca93lFPRd8kKRzxstN+gkTyVM4YmHRzJXQq74kuBw7Gz6xXAoiq6TJx77vOgClzgJw8YmfAXi/WexzHHbCYk7k6LbEC1mmOUXLrye/E0gjKnNZ0kJmT1D9L4nWlznBa/p1lFCm2W4pfk4F3no//+aDPeS33Qsn4XUnSIq1Uz6raQf+UNOOUnK/qI3tQuVRsr0c3A6R9jwHkOUNI7SdjVL95OHKoJPD0eswxiKraSQSnvtoX4BKYeh2p+564puELYNMYa8UnQeKdAgrdP8AEWKJfabtkhGn7GJCg7HNd/l66icfr4bcra3EOv6s/mKzdbB6eDJba42sbPD7vqPlc++ba5GXpO43Pw7gl46b3XpI5453Th6rpGCSgoCQGka6d9qdQcS78SnWgY5ET8btT35lkJj2oK5Z/0iIoCY23YxFV8970MZLife5PdUzCOtNTKJQhq7i9vZ201Q09JVM7gPRPMgyvJ6+Z+mPOg+s3AY59t0+eQgDZgUXnUFyS8Se9mAOKLp+QPkt9NZc38XowrPS7pbXiU7+lB4fK0QDF9fX15I1XXQiS9mk7p+BVy3T7EEmKuY/SJqXgdxmvBt8XMsnoNJtAgJhTfIKRA44f85g+c6ZFsHSQ4DYZTjfGNBSf2uz6POlGAsnU9+lcBwm/X4VrNDqw0v600MvZlsZXY0ZGyJBDHwLFOxV68HkPVQ9U2YHBFUT/UfFS+b59TpCQ7OMh5r4nz+8KkwxP/aRpybSEOs2icL+v5vRYOLGJpX7zurrRVu2+5YzHzC0w8/6jdIDs/dnpSteuBGTOdh2slpxDF45IxCo1RrqOAz3Bwe8xeqcYxc3NTXzgiWeTOTe/hNgsn9tu35wsXWMfJpEUqFMUsiB64fV6Xbe3t3V6erpNlurdoulu0W4a1v+fe/aol8nVlSlZyWtSCDRsv+cnHDj031zYudT3yZHMtSGVyd8Eh05fl8bYmQTbyrp4WO4zYwSK29vbLft0p3CIHA1QCC3ZId5p9AZpfcNc+en3czGJdL1Un6V6UtxwpBir1WryNDAyMfZPVU5UOlCkZ2joGFFdBwyVxf5n+R5OLfWRH9OFFV7PufHrjukM5ymOgyGG9u2TkNxXHHz9PwdzznY4wB8qRwMUNzc3O4qgNQn0Or6wJTGLOUV1hZjrxFTOPgNPg6Wysm3uRZbawPhUxsmbzlK+IbU1eaJxHCdPW2LyVArI8/xRf/ruU7lLfUyj7byqU3r9Vj+QjvOcdF22X+Lg5+PoY8k6MFfULd1+jJF2OuP/s65kgik30bG7x8pRAEXV1Bv5Ml8qRFW/wKcrt9u3VMZjGEBS0Ll6etmdcrDMdHObG09VptI0Em7JyOjJmUVXmTTIVKa3lVO5XZ9zfwJVXTuts/Bz9hnPufHYVxIgONA/l3idHew89HjnGYW/AYxeg8jtrCMNmKQbtMei/HMd28XPbAOft5naojwOvfE+huI0lsqk2FaMgjSWWXR/SAvPF7DonPv7++0rGKp2Fy+lpKiDbWKWOrczykT/eQ33sHP95/1PfdPt8v6ezznAWHI8HWiy3g7OnALtnpj1zgBF1XRprTqaysRYXQ3nnYmJer5NeUrnO8ixrswJ0At3xsCw6zHXJwg7s2CYRxbHsaAiKiPP512wPikE0vUeywo7w09MhPowV5e0TTIXJr4tRkHWkPrP7wx24Fhq02PlKIBCDayaDmh6L0daT8/v9CjJM1NSPiDVrfu9r7J5IirN/fvS3X08VLrbcm6rOjorcM/EaTY+35GMggxC5yq/oZcb8zhnFk6TkzjjSmPgAJUYi5+3xMJSv3Gcuje1eT/PffdQL9XZWaOPUwIItu05HeZRAEXVdD6dHeZv2qYn9Q5x7+zynB2XlC4NNLcSZ0qkrvLuTNJ1gNF5syWg4Naf3EWPLIotAOAb0QQM8m4CAtZFIOHA7eO2RP2X1lPo+xxLSaC+BFD+O43H0g1tc/WhvnZ1JbD7jFX3WWrPU+QogIJoSdot4UCRcbgByOCq8kIenjNnTEt1TZKy526MnVDppHinp6ctUMxNwXXHeZ+qz6msBAzOYOgeEmcUZ2dnkyXDfAy9Z+D5NizmQHxBF/uta+PceKQQKI3Jvga1FHYs1cf3dayjk44Bep6IdfRr7XNvzJIcBVBU7XrmrsFLYJJoKb2ljIHnzIFEKtfr7NllAoS2S9NUpLUEtfTaPrUp9YEfu6TQ3gbmKrRf/ef3ofCp40pe3t3d1Xq93nmptNgIl6KzjCWvmMSP6+i+tzf1/Zwk9jAHFnN1fKosAQbr6qHtOwUUVdP4kp6NCsrB4TQhwYV3bO6jBEvHOXvhvqQIZD1pcP2YBIScKhZgcAZIxrfEkFxpuj5IU66cEtVY0IPx+ZMyeoEEHz5LRkHAGMcxLh7rZkR8XzKSJHMg4WPAsuY+3tepjN8v4VgTGHz/IXI0QOGKkCgkO4GKRYPisSkMoewbdsyBRPrP40qGVu4FyHqoiL7i8vT0dLtUWKDhrCMpsb53bU3HqVyuCGVY4iEV32PiL7fpFnQpnPFbohO78L5K/dfJEkj4mHrfLLG0x4QSj9Uz7Zsrl6BAVsp9j3lfaSdHAxSUxCx8yq4qT4WlEEBlzXXYYyikDx4XO6WyllhHVU1Aj/SfszwEHr4dXM/yYN0YMiUv4233OvI4X6atj78GkvXnTX2+5bSqPum1kQRc9pn6oat76vMlg0ugQEPz/c5au7IdiNOWQl1mmJ1YtOug6uUzZu9c6FHVJ6ekMHwXIw3IDYqoyleydYbS7esMyM8RiM21a85b8lz3YPyQUegN4Up8cvEP15fMAQWVcK59PM5DKhmKZknc2MkYnFn4g3nIFJ2Bpbsi/Vo+Zr6Px3HsfNsBhMZg7iVBvt+N3a/bOSnO+PmLrxPT9psqfewPkaMDiiQ0fGcZVQ+IzjxFUhyeOxdX7gMcXThS9cAwSN95jhudC9vGelNZqMxVu3cickZBx3sdWA7buY/3TccyNNF+AYhPv1bVNpfh94j4u0t59yQBQ31FRudMbQko3FurL7twgwbIvuj6iNdwg01leCiT9JmhlzuTk5OTf7SAwtGTnpfMwj1a1ZRpuAElcPAkKSUBRAILn4WgoegaLEdtcaP0mNvpp24z51Z9IWbh785IAJVmUbh/CcxcATvQ1X9S7tPT063xk2kQLPyNZ93Uqn90reQkUiiT2ut9MscsOM4pV5KSit6HqR7sN4V4/jJknkcWsV6vJ3pQ1b+Y6TFytEBR1a+V0MCw431Kb45ZdKygu26SDjD4m9s0q+D73BMv9UVqSwJPKqC+k/H4OZ0H7PrK+yT1HdkNE7gMkQQYXh+2n2VwUVrKY3g/keGwzm7IDhDsj2TQcw6mA+l0LMNntWFOJ1hXhqN+H8o7xShSksaZhPYpH0AmIEXkOgQm1Eh9VY4/bIRKNEe9Je51uzhZHs8Tes4o5DUl9FRONYdhqNvb20k7PEfhCs86a5+/0XvOKDrWpfq5Z+68o3t33/qMCWdOEqNgv6m/3PPymj623j7vI//t7db1KKnvlhLKBAo6PTFoDzmqHm4SnAOKuXHbV44GKJK4F6nKMx06tqomMa57G6fSNN50LZ7zGOEgsh66tnu5lHdJSs7viQX4VsqqZdSsnwBC9fOVoLwOw5cuDNF/HaiwTH50jAxC5zmTIFNQmwgwDDerHv/OT44Rtx6acLaD5fmMGgE3sYvUfw7ozs5SvothpwBD3xP7eaocBVAk75O+V+1SSs5oVD0kEskg1GFSMm05leQ0bW5wE72mgndARKWiR+RzDlUGE3ae1Xfgc6+RPAnb5e1lfJsy50t025N+Swlcjtccu+BCO7EG9o3PrFTVDrPwqVYe6+LrD9if3oZUb7aZ/T7HyDgunljmNZyxOCMki6BO+1g9VY4CKKr66UhJtz890CataegMpqq2c/od4j+lHek7vYQU4+7ubserknImqptCNG7pmVM8zA89EsFCi6J4TAei/O6e3I/zfc5enD0SIPRbzoDsIBltty+xNhrqnKFzHKpqy9Y8pEmJzCXdSEDhTkHfCRSc9RBYPCebqDoioJDMMQunpDqe+9Xh7CgyCG3JOOhd/dFqjspp21FJV5ak1JwK5HYYHkIG1YvPfUheRrKUvKIxqC/0cOPEKNJ0oa7j3kxlOptL8bmDhY+zts4sPHzwfvU+rspvPE+zH2mMGY5JuLaDZXXnetvZB96fbFfnFHxMfDr0H1lGMfefx8Y0ws5oZZQOGA4qEo9fu/qnfZ1HcpBjPM6wRf9xZof9sRSeJQ9PoEgeisbvikfvRaNWbMww0Ns9F9p155Fx8Hdqm4OAg8PcuXPOgOXzQ31K5+zj2ed0y3XDP56QT6zonQIKiqO3S/ImOo+f5NUSw/D9/t2NRNs0cKrH3Fb11rUJEGw7VyiyXrxFe98FRh1YOHvRVms0khK6F1P9lEjWWgkBT/Kanlvy/vHf7N+kGw6gc/3gxtddL5XvH87i+Llz45/q74CSynWGtrTGY+m6+8rRAAVnJ/YBiY52e6c5MFQ9xPCckSDld6Dgf16GJw7T4HBf552kiIy/uepS5/rCG+1Tnb1cMZFumjVl+vmfKyf71jPufE4m2YWDi/cHgd3H0vty3//dyDqg2Oc6XRkM/zpGsQQOvu2Agro4t/UynksWgWIYhj9fVT9cVd8cx/EH3uz7bFX9fFV9qap+o6p+bBzHj97899NV9RNVdV9Vf3ocx7/y1Mq58jDmdPH4sWqX9rkHIZ1zCq7ymENwMNFxXoazkCXP5V5PRqj2Mjfh5yUazWP92hQqN4EkAWViUJym1pazOcxVsF9ZnyUFX/Ly3bkdUHhfpXI7ceBlOSnMWiq/a4+uRd1wHesA4m2ARNV+jOK/rar/qqr+//bOJdSqKozjvz9WRkakmXFTSQN72KAMCa0Goj3UQY4CBaGB0EAhiyC9OIgGgoOQGlQgvV+KmKQ4yMqKJmXdKOKa3jSMvGVqk0IHcYWvwV5b112sffY5597rXjfWDzbn7LVf/7POWt/61uM7520vbSNwwMy2SNro9jdImgusBO4AbgQ+lXSLmXX8n2axFgaq+6Sx7ohfEcpMhYveiF/Iw8oQmzb1K7/v+pV9+bJyxcK+Sx3++6pCUeJ7PuUy3vJZ4e9WlN5I2cqVr6GnEuaRv5DNxy/4VVv5OYeGhi54EuX4Rrm03B/zCL2P0GuJPa9KW6etaKxbEvteqq7xr/UHRsv8Cyt0VfcoRvgZwrLsH4+VVf8eMd2jQa2hMLMvJc0KklcAi9z7t4AvgA0ufYeZ/Qscl3QMuAf4qu45VQthOqXqHn5rDcNb0fJ4OABapsdaxJh34s9QhEYnVkBLHf57/96hyx/eJyyoVX/M43+OVl6ZT7hwyS+I/qBbWDHCBUm+ASuv9z2tcManlWGqa5ljx8PuRey8dg2FnydVA8uhltjzY3rC/XBswjcUoWH1z2nVcI6EbscobjCzk07ESUnTXPp04GvvvEGXVkvVh6nzLNrNGD9DY2sm/ApZ5UHEPI2wIvvThGGwVvkcf4tVwlgBLXX7Mw3luf5P0vn7ZYGO/ShMeX9/+jDMT98QxQq2v7oz9BLC2ZNwvUa4QMhvKas8Bn9au9QYy9cYsXLSysDE8qLM95gHWGUQwmfGzgl1h42RX97a6WrEPKCRMvJokeHEcjxqASQ9LqlPUt/Zs2dHTUArj6LuNbZYJ9bP9c+tu0d4XSujV0VVZajab9Uy+/frhLrP1GqLLXjy+/lVWysdnRJrGOryol3Po+5Y3XlVz4lp7OQ7bCc/26Vbj+KUpB7nTfQAp136IDDTO28G8EfsBma2DdgGIOnM2rVrzwF/dannUjOV8aMVxpferHVsmArc1O3F3RqKvcBjwBb3usdLf1/SVorBzDnAN3U3M7PrJfWZ2fwu9VxSxpNWGF96s9axwWmd1e317UyPbqcYuJwqaRB4lsJA7JS0BvgNeBTAzA5J2gn8BJwH1lkXMx6ZTCYt2pn1WFVxaEnF+ZuBzSMRlclk0mK0BzNHwramBXTAeNIK40tv1jo2jEirRmOONZPJ/L9JyaPIZDKJkg1FJpOpJQlDIWmppAFJx1TEjiSDpJmSPpd0WNIhSetd+hRJn0g66l4nN621RNIESd9L2uf2k9Qq6VpJuyQdcfm7MGGtT7nvv1/SdklXpqJV0uuSTkvq99IqtUnqdXVtQNLD7TyjcUMhaQLwErAMmAusUhFclgrngafN7HZgAbDO6SsD4+YAB9x+KqwHDnv7qWp9EfjIzG4D7qTQnJxWSdOBJ4D5VkRQT6AIfkxF65vA0iAtqk3DAzeXAi+7OtiaTpbijsUGLAT2e/u9QG/Tulro3QM8CAwAPS6tBxhoWpvTMsMVjMXAPpeWnFbgGuA4bkDdS09R63TgBDCFYknBPuChlLRS/ORDf10+hvUL2A8srLt/4x4FF7+EkrYDyS41KqJo5wEHCQLjgGnVV15SXgCeAfxIoBS13gycAd5w3aRXJU0iQa1m9jvwPMXiwpPA32b2MQlq9ajS1lV9S8FQtB1I1iSSrgY+AJ40s3+a1hNDUvkDQ981raUNLgPuBl4xs3nAORLoZsRw/fsVwGyK0IRJklY3q6pruqpvKRiKtgPJmkLS5RRG4j0z2+2ST7mAOILAuCa5D3hE0q/ADmCxpHdJU+sgMGhmB93+LgrDkaLWB4DjZnbGzIaA3cC9pKm1pEpbV/UtBUPxLTBH0mxJV1AMtOxtWNMFVMTzvgYcNrOt3qEyMA6GB8Y1hpn1mtkMK4J/VgKfmdlq0tT6J3BC0q0uaQlFjFByWim6HAskXeXKwxKKgdcUtZZUadsLrJQ0UdJs2gzcbHSQyBtQWQ78DPwCbGpaT6DtfgrX7EfgB7ctB66jGDQ86l6nNK010L2Ii4OZSWoF7gL6XN5+CExOWOtzwBGgH3gHmJiKVmA7xdjJEIXHsKaVNmCTq2sDwLJ2npGXcGcymVpS6HpkMpnEyYYik8nUkg1FJpOpJRuKTCZTSzYUmUymlmwoMplMLdlQZDKZWv4DivuTiSrOb28AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[3].squeeze(), cmap='Greys_r');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Architecture ######\n",
    "\n",
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 800, \"output_dim\": 512, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 512, \"output_dim\": 10, \"activation\": \"sigmoid\"} #Or relu again like the original example\n",
    "]#No Dropout...yet\n",
    "\n",
    "\n",
    "######  Init Layers  ######\n",
    "\n",
    "def init_layers(nn_architecture, seed = 99):\n",
    "    # random seed initiation\n",
    "    np.random.seed(seed)\n",
    "    # number of layers in our neural network\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    # parameters storage initiation\n",
    "    params_values = {}\n",
    "\n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "\n",
    "        # extracting the number of units in layers\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "\n",
    "        # initiating the values of the W matrix\n",
    "        # and vector b for subsequent layers\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Convolution Architecture - Downsampling/Upsampling  ##################\n",
    "\n",
    "def init_filters(layers, init_n_f ):\n",
    "    filters = []\n",
    "    bias = []\n",
    "    f_dc = []\n",
    "    trim = 0.1\n",
    "    n_f = init_n_f #initial number of filters/kernels\n",
    "    ch_in = 1      #input channels\n",
    "    for i in range(layers):\n",
    "        if(i != 0):\n",
    "            n_f = n_f*2 #16,32,64,128,256\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trim\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trim\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trim\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trim\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "    \n",
    "    #Deconvolution filters    \n",
    "    for i in range(1,layers):\n",
    "        n_f = n_f//2 #128,64,32,16\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        fdc = np.random.randn(n_f,ch_in,2,2)*trim #upsampling filter, its result will be conc with conv4 output so the channels will be doubled again\n",
    "        bdc = np.random.randn(f_fc.shape[0],1)* trim\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trim\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trim\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trim\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trim\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        dc = [fdc, bdc]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "        f_dc.append(dc)\n",
    "        \n",
    "    \n",
    "    return filters, bias, f_dc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation Functions ###\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UTILITIES #############\n",
    "\n",
    "\n",
    "def conv(image, params, s = 1, pad = 1 ): # s = 1 (conv stride)\n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    [f, b] = params\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((image.shape[0],image.shape[1]+2*pad ,image.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = image\n",
    "        image = tmp    \n",
    "        \n",
    "    f_num = f.shape[0]\n",
    "    f_depth = f.shape[1] #f1 = 1 , f2 = 8\n",
    "    f_size = f.shape[2] \n",
    "        \n",
    "    \n",
    "    h_range = int((image.shape[1] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    np_o = np.zeros((f_num, h_range, w_range))\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                np_o[z, _h, _w] = np.sum(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] * f[z, :, :, :]) + b[z]\n",
    "    \n",
    "    return np_o\n",
    "\n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s, pad = 1 ):\n",
    "    f_num, n_c, f, _ = filt.shape\n",
    "    \n",
    "    _ ,h , w = dconv_prev.shape\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((conv_in.shape[0],conv_in.shape[1]+2*pad ,conv_in.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = conv_in\n",
    "        conv_in = tmp\n",
    "    \n",
    "    dconv_in = np.zeros(conv_in.shape)\n",
    "    db = np.zeros((f_num,1))\n",
    "\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                #each entry of the dconv_prev will try to affect the idxs from which was made of.\n",
    "                dfilt[z] += dconv_prev[z, _h, _w] * conv_in[:, _h*s : _h*s + f, _w*s : _w*s + f]\n",
    "                dconv_in[:, _h*s : _h*s + f, _w*s : _w*s + f] += dconv_prev[z, _h, _w] * filt[z]  \n",
    "        db[z] = np.sum(dconv_prev[z])  #, axis =1) ## AXIS?\n",
    "    \n",
    "    if(pad >=1 ):\n",
    "        dconv_in = dconv_in[:, pad:-pad, pad:-pad]  # Cropping\n",
    "        \n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "\n",
    "def convTransp(image, params, s = 1, pad = 1):\n",
    "    #s is always 1, upsample kernel = 2\n",
    "    #zero insertions between pixels s_downsampled -1 = 2-1 = 1\n",
    "    #required padding in order to double my dimensions with the given data:\n",
    "    #(i-1)*2 + k -2p = output size, where our padding is k - p -1 = 2-0-1=1(we assume p=0)\n",
    "    i = image.shape[1]\n",
    "    target_dim = i*2\n",
    "    required_pad = 1 #always for filter 2x2 ,stride 1, zero insertion 1 and main target to double dim\n",
    "    #make our new custom input\n",
    "    size = i*2 +1\n",
    "    new_in = np.zeros((image.shape[0], size, size))\n",
    "    for i in range(target_dim):\n",
    "        for i in range(1, target_dim, 2):\n",
    "            for j in range(1, target_dim, 2):\n",
    "                if((i%2) == 1):\n",
    "                    new_in[:, i, j] = image[:, i//2, j//2]\n",
    "    #now we do a normal convolution\n",
    "                    \n",
    "def convTranspBackward(dconv_prev, conv_in, filt, s = 1, pad = 1):\n",
    "    \n",
    "\n",
    "def maxpool(image, f=2 , s=2):\n",
    "    \n",
    "    h_range = int((image.shape[1] - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f) / s) + 1     # (W - F + 2P) / S \n",
    "    out = np.zeros((image.shape[0], h_range, w_range))\n",
    "    \n",
    "    for z in range(image.shape[0]): # Number of channels\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                out[z, _h, _w] = np.max(image[z, _h*s : _h*s + f, _w*s : _w*s + f])\n",
    "    return out\n",
    "\n",
    "def nanargmax(arr):\n",
    "    #print(arr.shape)\n",
    "    try:\n",
    "        idx = np.nanargmax(arr)\n",
    "        #print (idx)\n",
    "    except:\n",
    "        idx = 0\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "def maxpoolBackward(dpool, conv, f=2 , s=2):\n",
    "    num_c, h, w = conv.shape\n",
    "    h = int((h - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w = int((w - f) / s) + 1     # (W - F + 2P) / S \n",
    "    \n",
    "    dout = np.zeros(conv.shape)\n",
    "    #print(conv.shape)\n",
    "    for z in range(num_c): # Number of channels\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                (a, b) = nanargmax(conv[z, _h*s : _h*s + f, _w*s : _w*s + f]) #Getting the indexes from the max value in this area\n",
    "                #put it on the new array\n",
    "                dout[z, _h + a, _w + b] = dpool[z, _h, _w]\n",
    "    \n",
    "    \n",
    "    return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-31-a88e69e3840e>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "<ipython-input-31-a88e69e3840e>:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n"
     ]
    }
   ],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    \n",
    "    # calculation of the input value for the activation function\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    # return of calculated activation A and the intermediate Z matrix\n",
    "    return activation_func(Z_curr), Z_curr\n",
    "\n",
    "def full_forward_propagation(X,filter_params ,params_values, nn_architecture, dropout):\n",
    "    \n",
    "    ######################## Forward Propagation Convolution Part  ##########################\n",
    "    [f1, f2, b1, b2] = filter_params\n",
    "    \n",
    "    params = [f1, b1]\n",
    "    conv1 = conv(X, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "    conv1[conv1<=0] = 0 #Relu\n",
    "\n",
    "    params = [f2, b2]\n",
    "    conv2 = conv(conv1, params, 1)\n",
    "    conv2[conv2<=0] = 0 #Relu\n",
    "    \n",
    "    pl = maxpool(conv2, 2, 2) #pool_f = 2 , pool_s = 2\n",
    "    \n",
    "    #packet\n",
    "    conv_mem = [X, conv1, conv2, pl]\n",
    "    \n",
    "    num_c, f_dim, _ = pl.shape\n",
    "    fc1 = pl.reshape(num_c*f_dim*f_dim, 1) #Flattening\n",
    "\n",
    "    \n",
    "    ######################## Forward Propagation FC Part  ##########################\n",
    "    \n",
    "    \n",
    "    # creating a temporary memory to store the information needed for a backward step\n",
    "    memory = {}\n",
    "    # X vector is the activation for layer 0 \n",
    "    A_curr = fc1\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        # extraction of W for the current layer\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        if ((layer_idx == 1)and(dropout)):\n",
    "                ## Dropout ##\n",
    "                d = (np.random.rand(W_curr.shape[0],W_curr.shape[1])<0.5)\n",
    "                d = d*1 #Bool --> int(0s and 1s)\n",
    "                W_curr = d*W_curr\n",
    "                #############\n",
    "            \n",
    "        # extraction of b for the current layer\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        # calculation of activation for the current layer\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        # saving calculated values in the memory\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    # return of prediction vector and a dictionary containing intermediate values\n",
    "    return A_curr, conv_mem, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cost Calculations ######\n",
    "def NLLLoss(logs, targets):\n",
    "    out = np.zeros(logs.shape)\n",
    "    mylen = logs.shape[0]*logs.shape[1]\n",
    "    #print(len(targets))\n",
    "    #logs.shape (dim x dim) like target\n",
    "    for i in range(logs.shape[0]):\n",
    "        for j in range(logs.shape[1]):\n",
    "            if(targets[i,j] == 1):\n",
    "                out[i,j] = logs[i,j] #in that case the propab. is correct with targen being the 1\n",
    "            else:\n",
    "                out[i,j] = 1 - logs[i,j] # e.g if logs[i,j]= 0.4 and we want(target) 0 --> 1-0.4=0.6 prob. for zero \n",
    "    #or\n",
    "    #out = targets*logs + ((-1*(targets-1)) - (-1*(targets-1))*logs)\n",
    "    return out.sum()/mylen\n",
    "\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    \n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost) ### ATTENTION!\n",
    "\n",
    "\n",
    "###### Accuracy Calculation ######\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:10: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:10: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-33-61d1904879c8>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "<ipython-input-33-61d1904879c8>:10: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n"
     ]
    }
   ],
   "source": [
    "#######  BACK PROPAGATION  #######\n",
    "\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    # number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "\n",
    "    # calculation of the activation function derivative\n",
    "    \n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    \n",
    "    # derivative of the matrix W\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    \n",
    "    # derivative of the vector b\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    # derivative of the matrix A_prev\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr \n",
    "\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, conv_mem, memory, filter_params, params_values, nn_architecture):\n",
    "    \n",
    "    \n",
    "    ################# Backwardpropagation for FC Part  #######################\n",
    "    grads_values = {}\n",
    "    \n",
    "    # number of examples\n",
    "    #print(Y.shape)\n",
    "    m = Y.shape[1]     # 1 sample each time\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # initiation of gradient descent algorithm\n",
    "    dA_prev = Y_hat - Y#- (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        \n",
    "        # we number network layers from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "\n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    ################# Backwardpropagation for Conv Part  #######################\n",
    "    [f1, f2, b1, b2] = filter_params\n",
    "    [image_in, conv1, conv2, pl]= conv_mem\n",
    "    #dA_prev\n",
    "    #Find dimensions of pooled image\n",
    "    #dim = int(np.sqrt(dA_prev.shape(0)/f2.shape(0))) #sqrt(800/8)=10 ==> 8*10*10\n",
    "    dpool = dA_prev.reshape(pl.shape) #, 1) \n",
    "    dconv2 = maxpoolBackward(dpool, conv2)  # , pool_f, pool_s)\n",
    "    dconv2[conv2<=0] = 0 # backpropagate through ReLU\n",
    "    \n",
    "    conv_s = 1\n",
    "    dconv1, df2, db2 = convolutionBackward(dconv2, conv1, f2, conv_s) #\n",
    "    dconv1[conv1<=0] = 0\n",
    "    \n",
    "    _, df1, db1 = convolutionBackward(dconv1, image_in, f1, conv_s)\n",
    "    \n",
    "    conv_grads = [df1, df2, db1, db2] \n",
    "    \n",
    "    return conv_grads, grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### UPDATE ######\n",
    "\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "\n",
    "    # iteration over network layers\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  TRAIN  ######\n",
    "#import torch\n",
    "import time\n",
    "time.time()\n",
    "\n",
    "\n",
    "def train(X, Y, nn_architecture, epochs, learning_rate, dropout, verbose=True, callback=None):\n",
    "    # initiation of neural net parameters\n",
    "    #params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    \n",
    "    #f1,f2,f3,f4,f5,fd1,fd2,fd3,fd4\n",
    "    filters,bias, f_dc = init_filters(5,16) #Double the channel-filter 4 times  (up to 256 and back again)\n",
    "    \n",
    "    #filter shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    \n",
    "    v_adam =  [] #filter1,filter2\n",
    "    s_adam =  [] \n",
    "    bv_adam = [] #bias1,bias2\n",
    "    bs_adam = []\n",
    "    fdc_v_adam=[] #filter,bias\n",
    "    fdc_b_adam=[]\n",
    "    #format: [v1,v2,v3,v4,v5,vd1,vd2,vd3,vd4] ,same for the rest, each of these include a tuple for the 2 conv filter\n",
    "    #[s1,s2,s3,s4,s5,sd1,sd2,vs3,sd4]\n",
    "    # upsampling filters : [v1_dc,v2_dc,v3_dc]\n",
    "    for i in filters:     \n",
    "        v1 = np.zeros(i[0].shape)\n",
    "        v2 = np.zeros(i[1].shape)\n",
    "        s1 = np.zeros(i[0].shape)\n",
    "        s2 = np.zeros(i[1].shape)\n",
    "        v_a = [v1, v2]\n",
    "        s_a = [s1, s2]\n",
    "        v_adam.append(v_a)\n",
    "        s_adam.append(s_a)\n",
    "            \n",
    "    for i in bias:\n",
    "        bv1 = np.zeros(i[0].shape)\n",
    "        bv2 = np.zeros(i[1].shape)\n",
    "        bs1 = np.zeros(i[0].shape)\n",
    "        bs2 = np.zeros(i[1].shape)    \n",
    "        bv_a = [bv1, bv2]\n",
    "        bs_a = [bs1, bs2]\n",
    "        bv_adam.append(bv_a)\n",
    "        bs_adam.append(bs_a)\n",
    "    \n",
    "    for i in f_dc:\n",
    "        fdc_v1 = np.zeros(i[0].shape)\n",
    "        bdc_v2 = np.zeros(i[1].shape)\n",
    "        fdc_s1 = np.zeros(i[0].shape)\n",
    "        bdc_s2 = np.zeros(i[1].shape)    \n",
    "        fdc_v_a = [fdc_v1, bdc_v2]\n",
    "        fdc_s_a = [fdc_s1, bdc_s2]\n",
    "        fdc_v_adam.append(fdc_v_a)\n",
    "        fdc_s_adam.append(fdc_s_a)\n",
    "    \n",
    "    #Final layer 1x1 filter setup\n",
    "    out_f = np.random.randn(1,16,1,1)*0.1\n",
    "    out_b = np.random.randn(out_f.shape[0],1)*0.1\n",
    "    \n",
    "    v_out_f = np.zeros(out_f.shape)\n",
    "    s_out_f = np.zeros(out_f.shape)\n",
    "    bv_out_b = np.zeros(out_b.shape)\n",
    "    bs_out_b = np.zeros(out_b.shape)\n",
    "    out_fb = [out_f, out_b]\n",
    "    # performing calculations for subsequent iterations\n",
    "    \n",
    "    \n",
    "    [f1,f2,f3,f4,f5,f6,f7,f8,f9] = filters\n",
    "    [b1,b2,b3,b4,b5,b6,b7,b8,b9]= bias \n",
    "    [fb6_dc, fb7_dc, fb8_dc, fb9_dc] = f_dc\n",
    "    \n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # step forward\n",
    "        running_loss = 0\n",
    "        accuracy = 0\n",
    "        batch = 12\n",
    "        print(\"Epoch: {%d}\" %(e+1))\n",
    "        for c in range(0, X.shape[0], batch):\n",
    "            \n",
    "            #### Reset Gradients (Every batch) ####\n",
    "            beta1= 0.95\n",
    "            beta2= 0.99\n",
    "            lr = learning_rate\n",
    "            batch_size= batch\n",
    "            \n",
    "            df =  []\n",
    "            db =  []\n",
    "            dfb=  []\n",
    "            for i in filters:\n",
    "                df1 = np.zeros(i[0].shape)\n",
    "                df2 = np.zeros(i[1].shape)\n",
    "                f_temp = [df1, df2]\n",
    "                df.append(f_temp)\n",
    "            for i in bias:\n",
    "                db1 = np.zeros(i[0].shape)\n",
    "                db2 = np.zeros(i[1].shape)\n",
    "                b_temp = [db1, db2]\n",
    "                db.append(b_temp)\n",
    "            \n",
    "            for i in f_dc:\n",
    "                df = np.zeros(i[0].shape)\n",
    "                db = np.zeros(i[1].shape)\n",
    "                fb_dc = [df_dc db_dc]\n",
    "                dfb.append(fb_dc)\n",
    "                \n",
    "            dout_f = np.zeros(out_f.shape)\n",
    "            dout_b = np.zeros(out_b.shape)\n",
    "            ######################################\n",
    "            \n",
    "            \n",
    "            #timestamp1 = time.time()\n",
    "            \n",
    "            if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "                batch = X.shape[0] - c\n",
    "            X_t = X[c:(c + batch)]  # shape:(m, ch, h, w)\n",
    "            Y_t = Y[c:(c + batch)]\n",
    "            cost = 0\n",
    "            accuracy = 0\n",
    "            \n",
    "            [df1,df2,df3,df4,df5,df6,df7,df8,df9] = df\n",
    "            [db1,db2,db3,db4,db5,db6,db7,db8,db9] = db \n",
    "            [dfb6_dc,dfb7_dc,dfb8_dc,dfb9_dc]     = dfb\n",
    "            \n",
    "            \n",
    "            for b in range(batch):\n",
    "                \n",
    "                \n",
    "                 #################### TODO: BLOCK IMPLEMENTATION - FUTURE UPDATE ######################\n",
    "                    \n",
    "                    \n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ######################## Forward Propagation Convolution Part  ##########################\n",
    "\n",
    "                \n",
    "                ########### 1st Big Layer ###########    (with zero padding ='same',so with stride =1 we get same dim as the input)\n",
    "                params = [f1[0], b1[0]]  \n",
    "                conv1_1 = conv(X_t[b], params, 1)   #conv1 shape = (num_channels, h, w), padding = 1 (same output dim)\n",
    "                conv1_1[conv1_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f1[1], b1[1]]\n",
    "                conv1_2 = conv(conv1_1, params, 1)\n",
    "                conv1_2[conv1_2<=0] = 0 #Relu\n",
    "                ##################################### conv1_2: 101x101x16\n",
    "                \n",
    "                pl1 = maxpool(conv1_2, 2, 2) #   pl1 : (101-2)/2+1  = 50 \n",
    "                ## ADD DROPOUT HERE(on pl1)\n",
    "                \n",
    "                ########### 2nd Big Layer ###########\n",
    "                params = [f2[0], b2[0]]  \n",
    "                conv2_1 = conv(pl1, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv2_1[conv2_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f2[1], b2[1]]\n",
    "                conv2_2 = conv(conv2_1, params, 1)\n",
    "                conv2_2[conv2_2<=0] = 0 #Relu             \n",
    "                #####################################  50x50x32\n",
    "\n",
    "                pl2 = maxpool(conv2_2, 2, 2) #pool_f = 2 , pool_s = 2    , (50 -2)/2 +1 = 25\n",
    "                ## ADD DROPOUT HERE\n",
    "\n",
    "                ########### 3rd Big Layer ###########\n",
    "                params = [f3[0], b3[0]]  \n",
    "                conv3_1 = conv(pl2, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv3_1[conv2_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f3[1], b3[1]]\n",
    "                conv3_2 = conv(conv3_1, params, 1)\n",
    "                conv3_2[conv3_2<=0] = 0 #Relu             \n",
    "                #####################################  25x25x64\n",
    "\n",
    "                pl3 = maxpool(conv3_2, 2, 2) #pool_f = 2 , pool_s = 2   ,  (25-2)/2 +1 = 12\n",
    "                ## ADD DROPOUT HERE\n",
    "                \n",
    "                ########### 4th Big Layer ###########\n",
    "                params = [f4[0], b4[0]]  \n",
    "                conv4_1 = conv(pl3, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv4_1[conv4_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f4[1], b4[1]]\n",
    "                conv4_2 = conv(conv4_1, params, 1)\n",
    "                conv4_2[conv4_2<=0] = 0 #Relu             \n",
    "                #####################################     12x12x128\n",
    "\n",
    "                pl4 = maxpool(conv4_2, 2, 2) #pool_f = 2 , pool_s = 2  , (12-2)/2 +1 =6  : 6x6x128\n",
    "                ## ADD DROPOUT HERE\n",
    "                \n",
    "                ########### 5th Big Layer ###########   6x6x128-->6x6x256\n",
    "                params = [f5[0], b5[0]]  \n",
    "                conv5_1 = conv(pl4, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv5_1[conv5_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f5[1], b5[1]]\n",
    "                conv5_2 = conv(conv5_1, params, 1)\n",
    "                conv5_2[conv5_2<=0] = 0 #Relu             \n",
    "                #####################################  6x6x256\n",
    "                \n",
    "                #####################################\n",
    "                #Because of ambigious size after the upsampling the concat func must take care possible crop of the conv#_2 \n",
    "                #####################################\n",
    "                \n",
    "                #Deconvolution/Upsampling\n",
    "                # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "                params = [fb6_dc[0], fb6_dc[1]] # deconv filter, deconv bias\n",
    "                dc6 = convTransp(conv5_2, params)   #result:   =  12x12x128 , # conv5_2 requires NO crop\n",
    "                #Concat dc6 with conv4_2 so we get 256 channels (12x12x256)\n",
    "                c6 = concat(conv4_2, dc6)\n",
    "                \n",
    "                ########### 1st Big dc Layer ###########          12x12x256     \n",
    "                params = [f6[0], b6[0]]  \n",
    "                conv6_1 = conv(c6, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv6_1[conv6_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f6[1], b6[1]]\n",
    "                conv6_2 = conv(conv6_1, params, 1)\n",
    "                conv6_2[conv6_2<=0] = 0 #Relu             \n",
    "                #####################################    12x12x128\n",
    "                #(12-1)*2 + 2 =24\n",
    "                params = [fb7_dc[0], fb7_dc[1]] # deconv filter, deconv bias\n",
    "                dc7 = convTransp(conv6_2, params)   #result:   =  24x24x64\n",
    "                #Concat dc7 with conv3_2 so we get  channels (24x24x128)\n",
    "                c7 = concat(conv3_2, dc7)   #crop is required\n",
    "                \n",
    "                ########### 2nd Big dc Layer ###########          24x24x128     \n",
    "                params = [f7[0], b7[0]]  \n",
    "                conv7_1 = conv(c7, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv7_1[conv7_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f7[1], b7[1]]\n",
    "                conv7_2 = conv(conv7_1, params, 1)\n",
    "                conv7_2[conv7_2<=0] = 0 #Relu             \n",
    "                #####################################    24x24x64\n",
    "                #(24-1)*2 + 2 = 48\n",
    "                params = [fb8_dc[0], fb8_dc[1]] # deconv filter, deconv bias\n",
    "                dc8 = convTransp(conv7_2, params)   #result:   =  48x48x32\n",
    "                #Concat dc8 with conv2_2 so we get  channels (48x48x64)\n",
    "                c8 = concat(conv2_2, dc8)   #crop is required\n",
    "                \n",
    "                ########### 3rd Big dc Layer ###########          48x48x64    \n",
    "                params = [f8[0], b8[0]]  \n",
    "                conv8_1 = conv(c8, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv8_1[conv8_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f8[1], b8[1]]\n",
    "                conv8_2 = conv(conv8_1, params, 1)\n",
    "                conv8_2[conv8_2<=0] = 0 #Relu             \n",
    "                #####################################    48x48x32                              \n",
    "                #(48-1)*2 + 2 = 96\n",
    "                params = [fb9_dc[0], fb9_dc[1]] # deconv filter, deconv bias\n",
    "                dc9 = convTransp(conv8_2, params)   #result:   =  96x96x16\n",
    "                #Concat dc9 with conv1_2 so we get  channels (96x96x32)\n",
    "                c9 = concat(conv1_2, dc9)   #crop is required                \n",
    "               \n",
    "                ########### 4th Big dc Layer ###########          96x96x32   \n",
    "                params = [f9[0], b9[0]]  \n",
    "                conv9_1 = conv(c9, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv9_1[conv9_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f9[1], b9[1]]\n",
    "                conv9_2 = conv(conv9_1, params, 1)\n",
    "                conv9_2[conv9_2<=0] = 0 #Relu             \n",
    "                #####################################    96x96x16\n",
    "                \n",
    "                ############################# Last Layer conv(1x1) --> 96x96x1 ##########################\n",
    "                params = [out_f, out_b]\n",
    "                output = conv(conv9_2, params, 1) #output.shape: 96x96x1\n",
    "                \n",
    "                ## Sigmoid ##\n",
    "                Y_hat = sigmoid(output)\n",
    "                \n",
    "                \n",
    "                cost += NLLLoss(Y_hat,Y_t[b]):\n",
    "\n",
    "                accuracy += get_accuracy_value(Y_hat, Y_t[b])\n",
    "                \n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ################################# Backward Propagation ##################################\n",
    "                \n",
    "                \n",
    "                #Pixel-wise sub, we we can get the diff(Y includes the 2 classes 0 and 1)\n",
    "                \n",
    "                dA_prev = Y_hat - Y_t[b]\n",
    "                dZ_prev = sigmoid_backward(dA_prev, output)\n",
    "                conv_s =1 \n",
    "                dconv9_2, dout_f_, dout_b_ = convolutionBackward(dZ_prev, conv9_2, out_f, conv_s) #\n",
    "                #pack data\n",
    "                \n",
    "                \n",
    "                dconv9_2[conv9_2<=0] = 0             \n",
    "                dconv9_1, df9_2, db9_2 = convolutionBackward(dconv9_2, conv9_1, f9[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv9_1[conv9_1<=0] = 0\n",
    "                conc_dconv9, df9_1, db9_1 = convolutionBackward(dconv9_1, c9, f9[0], conv_s) #\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv9, dconv1_2 = crop2half(conc_dconv9)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                \n",
    "                            \n",
    "                dconv8_2, df9_dc, db9_dc = convTranspBackward(dconv9, conv8_2, f9_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                dconv8_2[conv8_2<=0] = 0\n",
    "                dconv8_1, df8_2, db8_2 = convolutionBackward(dconv8_2, conv8_1, f8[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv8_1[conv8_1<=0] = 0\n",
    "                conc_dconv8, df8_1, db8_1 = convolutionBackward(dconv8_1, c8, f8[0], conv_s) #\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv8, dconv2_2 = crop2half(conc_dconv8)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                \n",
    "                dconv7_2, df8_dc, db8_dc = convTranspBackward(dconv8, conv7_2, f8_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                dconv7_2[conv7_2<=0] = 0\n",
    "                dconv7_1, df7_2, db7_2 = convolutionBackward(dconv7_2, conv7_1, f7[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv7_1[conv7_1<=0] = 0\n",
    "                conc_dconv7, df7_1, db7_1 = convolutionBackward(dconv7_1, c7, f7[0], conv_s) #\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv7, dconv3_2 = crop2half(conc_dconv7)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                \n",
    "                dconv6_2, df7_dc, db7_dc = convTranspBackward(dconv7, conv6_2, f7_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                dconv6_2[conv6_2<=0] = 0\n",
    "                dconv6_1, df6_2, db6_2 = convolutionBackward(dconv6_2, conv6_1, f6[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv7_1[conv7_1<=0] = 0\n",
    "                conc_dconv6, df6_1, db6_1 = convolutionBackward(dconv6_1, c6, f6[0], conv_s) #\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv6, dconv4_2 = crop2half(conc_dconv6)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                \n",
    "                dconv5_2, df6_dc, db6_dc = convTranspBackward(dconv6, conv5_2, f6_dc[0],conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                dconv5_2[conv5_2<=0] = 0\n",
    "                dconv5_1, df5_2, db5_2 = convolutionBackward(dconv5_2, conv5_1, f5[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv5_1[conv5_1<=0] = 0\n",
    "                dpl4, df5_1, db5_1 = convolutionBackward(dconv5_1, pl4, f5[0], conv_s) #\n",
    "                \n",
    "                dconv4_2 += maxpoolBackward(dpl4, conv4_2, f=2 , s=2) #Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                dconv4_2[conv4_2<=0] = 0\n",
    "                dconv4_1, df4_2, db4_2 = convolutionBackward(dconv4_2, conv4_1, f4[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv4_1[conv4_1<=0] = 0\n",
    "                dpl3, df4_1, db4_1 = convolutionBackward(dconv4_1, pl3, f4[0], conv_s) #\n",
    "                \n",
    "                dconv3_2 += maxpoolBackward(dpl3, conv3_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                dconv3_2[conv3_2<=0] = 0\n",
    "                dconv3_1, df3_2, db3_2 = convolutionBackward(dconv3_2, conv3_1, f3[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv3_1[conv3_1<=0] = 0\n",
    "                dpl2, df3_1, db3_1 = convolutionBackward(dconv3_1, pl2, f3[0], conv_s) #\n",
    "                \n",
    "                dconv2_2 += maxpoolBackward(dpl2, conv2_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                dconv2_2[conv2_2<=0] = 0\n",
    "                dconv2_1, df2_2, db2_2 = convolutionBackward(dconv2_2, conv2_1, f2[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv2_1[conv2_1<=0] = 0\n",
    "                dpl1, df2_1, db2_1 = convolutionBackward(dconv2_1, pl1, f2[0], conv_s) #\n",
    "                \n",
    "                dconv1_2 += maxpoolBackward(dpl1, conv1_2, f=2 , s=2)#Very important += merge with the gradients from concat backprop\n",
    "                \n",
    "                dconv1_2[conv1_2<=0] = 0\n",
    "                dconv1_1, df1_2, db1_2 = convolutionBackward(dconv1_2, conv1_1, f1[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv1_1[conv1_1<=0] = 0\n",
    "                _, df1_1, db1_1 = convolutionBackward(dconv1_1, X_t[b], f1[0], conv_s) #\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                [df1,df2,df3,df4,df5,df6,df7,df8,df9] = df\n",
    "                [db1,db2,db3,db4,db5,db6,db7,db8,db9] = db \n",
    "                [dfb6_dc,dfb7_dc,dfb8_dc,dfb9_dc]     = dfb\n",
    "\n",
    "                \n",
    "                df1[0] += df1_1\n",
    "                df1[1] += df1_2\n",
    "                df2[0] += df2_1\n",
    "                df2[1] += df2_2\n",
    "                df3[0] += df3_1\n",
    "                df3[1] += df3_2\n",
    "                df4[0] += df4_1\n",
    "                df4[1] += df4_2\n",
    "                df5[0] += df5_1\n",
    "                df5[1] += df5_2\n",
    "                df6[0] += df6_1\n",
    "                df6[1] += df6_2\n",
    "                df7[0] += df7_1\n",
    "                df7[1] += df7_2\n",
    "                df8[0] += df8_1\n",
    "                df8[1] += df8_2\n",
    "                df9[0] += df9_1\n",
    "                df9[1] += df9_2\n",
    "                \n",
    "                db1[0] += db1_1\n",
    "                db1[1] += db1_2\n",
    "                db2[0] += db2_1\n",
    "                db2[1] += db2_2\n",
    "                db3[0] += db3_1\n",
    "                db3[1] += db3_2\n",
    "                db4[0] += db4_1\n",
    "                db4[1] += db4_2\n",
    "                db5[0] += db5_1\n",
    "                db5[1] += db5_2\n",
    "                db6[0] += db6_1\n",
    "                db6[1] += db6_2\n",
    "                db7[0] += db7_1\n",
    "                db7[1] += db7_2\n",
    "                db8[0] += db8_1\n",
    "                db8[1] += db8_2\n",
    "                db9[0] += db9_1\n",
    "                db9[1] += db9_2\n",
    "\n",
    "                dfb6_dc[0] += df6_dc\n",
    "                dfb6_dc[1] += db6_dc\n",
    "                dfb7_dc[0] += df7_dc\n",
    "                dfb7_dc[1] += db7_dc\n",
    "                dfb8_dc[0] += df8_dc\n",
    "                dfb8_dc[1] += db8_dc\n",
    "                dfb9_dc[0] += df9_dc\n",
    "                dfb9_dc[1] += db9_dc\n",
    "\n",
    "                dout_f += dout_f_\n",
    "                dout_b += dout_b_\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ############## Adam Optimization ################\n",
    "            #changing the main structures(which are also updated)\n",
    "            #TODO: Future update - remove uneccessary memory loads/stores, v,s dont need to be saved \n",
    "            for i in filter:\n",
    "                v_adam[i,0] = beta1*v_adam[i,0] + (1-beta1)*df[i,0]/batch_size #f1\n",
    "                s_adam[i,0] = beta2*s_adam[i,0] + (1-beta2)*df[i,0]/batch_size #f1\n",
    "                filters[i,0] -= lr*v_adam[i,0]/np.sqrt(s_adam[i,0] + 1e-7)\n",
    "                \n",
    "                v_adam[i,1] = beta1*v_adam[i,1] + (1-beta1)*df[i,1]/batch_size #f2\n",
    "                s_adam[i,1] = beta2*s_adam[i,1] + (1-beta2)*df[i,1]/batch_size #f2\n",
    "                filters[i,1] -= lr*v_adam[i,1]/np.sqrt(s_adam[i,1] + 1e-7)\n",
    "                \n",
    "            for i in bias:\n",
    "                bv_adam[i,0] = beta1*bv_adam[i,0] + (1-beta1)*db[i,0]/batch_size #b1\n",
    "                bs_adam[i,0] = beta2*bs_adam[i,0] + (1-beta2)*db[i,0]/batch_size #b1\n",
    "                bias[i,0] -= lr*bv_adam[i,0]/np.sqrt(bs_adam[i,0] + 1e-7)\n",
    "                \n",
    "                bv_adam[i,1] = beta1*bv_adam[i,1] + (1-beta1)*db[i,1]/batch_size #b2\n",
    "                bs_adam[i,1] = beta2*bs_adam[i,1] + (1-beta2)*db[i,1]/batch_size #b2\n",
    "                bias[i,1] -= lr*bv_adam[i,1]/np.sqrt(bs_adam[i,1] + 1e-7)\n",
    "            \n",
    "            for i in f_dc:\n",
    "                fdc_v_adam[i,0] = beta1*fdc_v_adam[i,0] + (1-beta1)*dfb[i,0]/batch_size #f1\n",
    "                fdc_s_adam[i,0] = beta2*fdc_s_adam[i,0] + (1-beta2)*dfb[i,0]/batch_size #f1\n",
    "                f_dc[i,0] -= lr*bv_adam[i,0]/np.sqrt(bs_adam[i,0] + 1e-7)\n",
    "                \n",
    "                fdc_v_adam[i,1] = beta1*fdc_v_adam[i,1] + (1-beta1)*dfb[i,1]/batch_size #b2\n",
    "                fdc_s_adam[i,1] = beta2*fdc_s_adam[i,1] + (1-beta2)*dfb[i,1]/batch_size #b2\n",
    "                f_dc[i,1] -= lr*fdc_v_adam[i,1]/np.sqrt(fdc_s_adam[i,1] + 1e-7)    \n",
    "            \n",
    "            v_out = beta1*v_out + (1 - beta1)*dout_f/batch_size #f\n",
    "            s_out = beta2*s_out + (1 - beta2)*dout_f/batch_size #f\n",
    "            out_f -= lr*v_out/np.sqrt(s_out + 1e-7)\n",
    "            \n",
    "            bv_out = beta1*bv_out + (1 - beta1)*dout_b/batch_size #f\n",
    "            bs_out = beta2*bs_out + (1 - beta2)*dout_b/batch_size #f\n",
    "            out_b -= lr*bv_out/np.sqrt(bs_out + 1e-7)\n",
    "            \n",
    "            \n",
    "            #Updating Conv Part\n",
    "            \n",
    "\n",
    "            #print(Yh)\n",
    "            #print(t)\n",
    "            print(c)\n",
    "            \n",
    "            '''\n",
    "            f1 -= (learning_rate) * (df1/batch)\n",
    "            f2 -= (learning_rate) * (df2/batch)\n",
    "            b1 -= (learning_rate) * (db1/batch)\n",
    "            b2 -= (learning_rate) * (db2/batch)\n",
    "            filter_params = [f1, f2, b1, b2]\n",
    "                \n",
    "            # updating FC Part\n",
    "            #params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "            grads_values['dW1'] = dW1#/(batch)\n",
    "            grads_values['dW2'] = dW2#/(batch)\n",
    "            grads_values['db1'] = dB1#/(batch)\n",
    "            grads_values['db2'] = dB2#/(batch)\n",
    "                   \n",
    "            for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "                \n",
    "                params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "                params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "            \n",
    "            '''\n",
    "            print(\"Cost : {:.5f}\".format(cost/batch))\n",
    "            print(\"Accuracy : {:.5f}%\".format((accuracy*100)/batch))\n",
    "            \n",
    "        #END OF LOOP - EPOCH\n",
    "        #timestamp2 = time.time()\n",
    "        #print (\"This took %.2f seconds\" %(timestamp2 - timestamp1))\n",
    "        if(verbose):\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/batch, (accuracy*100)/batch))\n",
    "        #if(callback is not None):\n",
    "        #    callback(i, params_values)\n",
    "            \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1, 3, 3) (16, 16, 3, 3)\n",
      "(32, 16, 3, 3) (32, 32, 3, 3)\n",
      "(64, 32, 3, 3) (64, 64, 3, 3)\n",
      "(128, 64, 3, 3) (128, 128, 3, 3)\n",
      "(256, 128, 3, 3) (256, 256, 3, 3)\n",
      "(128, 256, 3, 3) (128, 128, 3, 3)\n",
      "(64, 128, 3, 3) (64, 64, 3, 3)\n",
      "(32, 64, 3, 3) (32, 32, 3, 3)\n",
      "(16, 32, 3, 3) (16, 16, 3, 3)\n",
      "(16, 1) (16, 1)\n",
      "(32, 1) (32, 1)\n",
      "(64, 1) (64, 1)\n",
      "(128, 1) (128, 1)\n",
      "(256, 1) (256, 1)\n",
      "(128, 1) (128, 1)\n",
      "(64, 1) (64, 1)\n",
      "(32, 1) (32, 1)\n",
      "(16, 1) (16, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fhfgh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-c143c70b2e30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mparams_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNN_ARCHITECTURE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#0.05 stable LR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-47-b0850e6920f5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, Y, nn_architecture, epochs, learning_rate, dropout, verbose, callback)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mfhfgh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;31m##filter params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mnum_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fhfgh' is not defined"
     ]
    }
   ],
   "source": [
    "###### Training ######\n",
    "#train_images, train_labels, test_images, test_labels\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "params_values = train(train_images, train_labels, NN_ARCHITECTURE, 2, 0.01, True) #0.05 stable LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Prediction ######\n",
    "Yt = test_labels.T\n",
    "temp1 = []\n",
    "for i in range(Yt.shape[1]):\n",
    "        for j in range(Yt.shape[0]):\n",
    "            if(Yt[j][i]==1):\n",
    "                temp1.append(j)\n",
    "Yt=np.array(temp1)\n",
    "Y_test_hat, _ = full_forward_propagation(np.transpose(test_images), params_values, NN_ARCHITECTURE)#multiple?!\n",
    "\n",
    "Yht = np.array(Y_test_hat.T)\n",
    "#x_log=np.log( np.exp(Yh) / np.sum(np.exp(Yh), axis = 1) )  #(60000,10) , we need to add along columns so we get sum of 1 on every example-row\n",
    "num = np.exp(Yht)\n",
    "den = np.sum(np.exp(Yht), axis = 1)\n",
    "for i in range(Yht.shape[0]): #60000\n",
    "                #for j in range(Yh.shape[1]): #10\n",
    "                Yht[i][:] = np.log(num[i][:] / den[i])  \n",
    "\n",
    "#cost = get_cost_value(Yht, Yt)\n",
    "\n",
    "#cost_history.append(cost)\n",
    "accuracy = get_accuracy_value(Y_test_hat, test_labels.T)\n",
    "#accuracy_history.append(accuracy)\n",
    "print(\"Accuracy: {:.5f}%\".format( accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
