{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "#import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load from /home/USER/data/mnist or elsewhere; download if missing.\"\"\"\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "from matplotlib import image\n",
    "\n",
    "\n",
    "\n",
    "def mnist(path=None):\n",
    "    r\"\"\"Return (train_images, train_labels, test_images, test_labels).\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory containing MNIST. Default is\n",
    "            /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist.\n",
    "            Create if nonexistant. Download any missing files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, train_labels, test_images, test_labels), each\n",
    "            a matrix. Rows are examples. Columns of images are pixel values.\n",
    "            Columns of labels are a onehot encoding of the correct class.\n",
    "    \"\"\"\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz',\n",
    "             'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz',\n",
    "             't10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "    if path is None:\n",
    "        # Set path to /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist\n",
    "        path = os.path.join(os.path.expanduser('~'), 'data', 'mnist')\n",
    "\n",
    "    # Create path if it doesn't exist\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Download any missing files\n",
    "    for file in files:\n",
    "        if file not in os.listdir(path):\n",
    "            urlretrieve(url + file, os.path.join(path, file))\n",
    "            print(\"Downloaded %s to %s\" % (file, path))\n",
    "    print(\"All files are ready to gzip!\")\n",
    "\n",
    "    def _images(path):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        with gzip.open(path) as f:\n",
    "            # First 16 bytes are magic_number, n_imgs, n_rows, n_cols\n",
    "            pixels = np.frombuffer(f.read(), 'B', offset=16)\n",
    "        return pixels.reshape(-1, 1, 28, 28).astype('float32') / 255\n",
    "\n",
    "    def _labels(path):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        with gzip.open(path) as f:\n",
    "            # First 8 bytes are magic_number, n_labels\n",
    "            integer_labels = np.frombuffer(f.read(), 'B', offset=8)\n",
    "        def _onehot(integer_labels):\n",
    "            \"\"\"Return matrix whose rows are onehot encodings of integers.\"\"\"\n",
    "            n_rows = len(integer_labels)\n",
    "            n_cols = integer_labels.max() + 1\n",
    "            onehot = np.zeros((n_rows, n_cols), dtype='uint8')\n",
    "            onehot[np.arange(n_rows), integer_labels] = 1\n",
    "            return onehot\n",
    "\n",
    "        return _onehot(integer_labels)\n",
    "    print(\"Train Images : Loading . . .\")\n",
    "    train_images = _images(os.path.join(path, files[0]))\n",
    "    print(\"Train Labels : Loading . . .\")\n",
    "    train_labels = _labels(os.path.join(path, files[1]))\n",
    "    print(\"Test Images  : Loading . . .\")\n",
    "    test_images = _images(os.path.join(path, files[2]))\n",
    "    print(\"Test Labels  : Loading . . .\")\n",
    "    test_labels = _labels(os.path.join(path, files[3]))\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return train_images[0,:,:,:], train_labels[0,:] #, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are ready to gzip!\n",
      "Train Images : Loading . . .\n",
      "Train Labels : Loading . . .\n",
      "Test Images  : Loading . . .\n",
      "Test Labels  : Loading . . .\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labelsa= mnist()  #, test_images, test_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 32, 32)\n",
      "(1, 1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "ch = 1 #Number of channels\n",
    "train_images=cv2.resize(train_images[0,:,:], (32,32)).reshape(1,1,32,32)\n",
    "print(train_images.shape)\n",
    "train_labels = train_images\n",
    "print(train_labels.shape) # ONE-HOT !!!\n",
    "#print(train_images.T.shape)\n",
    "#print(train_labels.reshape((train_images.shape[0], 10)).shape)\n",
    "\n",
    "##### Getting Ready for the Conv Layers #####\n",
    "#train_images = train_images.reshape(train_images.shape[0], ch, 28, 28).squeeze()\n",
    "#test_images = test_images.reshape(test_images.shape[0], ch, 28, 28).squeeze()\n",
    "\n",
    "#############################################\n",
    "\n",
    "#y = train_images[1].reshape(1,28,28)\n",
    "#print(y.squeeze().shape)\n",
    "'''\n",
    "temp = []\n",
    "#train_labels = train_labels.sum(1)\n",
    "for i in range(int(len(train_labels[:]))):\n",
    "    temp.append(list(train_labels[i][:]).index(1))\n",
    "    \n",
    "    \n",
    "train_labels = np.array(temp.copy())\n",
    "#print(train_labels[0:5])\n",
    "'''\n",
    "#plt.imshow(train_images[0].squeeze(), cmap='Greys_r');\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_labels[0].squeeze(), cmap='Greys_r');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Convolution Architecture - Downsampling/Upsampling  ##################\n",
    "\n",
    "def init_filters(layers, init_n_f ):\n",
    "    filters = []\n",
    "    bias = []\n",
    "    f_dc = []\n",
    "    trim = 0.1\n",
    "    n_f = init_n_f #initial number of filters/kernels\n",
    "    ch_in = 1      #input channels\n",
    "    for i in range(layers):\n",
    "        if(i != 0):\n",
    "            n_f = n_f*2 #16,32,64,128,256\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trim\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trim\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trim\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trim\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "    \n",
    "    #Deconvolution filters    \n",
    "    for i in range(1,layers):\n",
    "        n_f = n_f//2 #128,64,32,16\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        fdc = np.random.randn(n_f,ch_in,2,2)*trim #upsampling filter, its result will be conc with conv4 output so the channels will be doubled again\n",
    "        bdc = np.random.randn(fdc.shape[0],1)* trim\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trim\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trim\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trim\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trim\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        dc = [fdc, bdc]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "        f_dc.append(dc)\n",
    "        \n",
    "    \n",
    "    return filters, bias, f_dc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation Functions ###\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UTILITIES #############\n",
    "\n",
    "\n",
    "def conv(image, params, s = 1, pad = 1 ): # s = 1 (conv stride)\n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    [f, b] = params\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((image.shape[0],image.shape[1]+2*pad ,image.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = image\n",
    "        image = tmp    \n",
    "        \n",
    "    f_num = f.shape[0]\n",
    "    f_depth = f.shape[1] #f1 = 1 , f2 = 8\n",
    "    f_size = f.shape[2] \n",
    "        \n",
    "\n",
    "    h_range = int((image.shape[1] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    np_o = np.zeros((f_num, h_range, w_range))\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                np_o[z, _h, _w] = np.sum(np.multiply(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] , f[z, :, :, :])) + b[z]\n",
    "    \n",
    "    return np_o\n",
    "\n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s = 1, pad = 1 ):\n",
    "    f_num, n_c, f, _ = filt.shape\n",
    "    \n",
    "    _ ,h , w = dconv_prev.shape\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((conv_in.shape[0],conv_in.shape[1]+2*pad ,conv_in.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = conv_in\n",
    "        conv_in = tmp\n",
    "    \n",
    "    dconv_in = np.zeros(conv_in.shape)\n",
    "    db = np.zeros((f_num,1))\n",
    "\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                #each entry of the dconv_prev will try to affect the idxs from which was made of.\n",
    "                dfilt[z] += dconv_prev[z, _h, _w] * conv_in[:, _h*s : _h*s + f, _w*s : _w*s + f]\n",
    "                dconv_in[:, _h*s : _h*s + f, _w*s : _w*s + f] += dconv_prev[z, _h, _w] * filt[z]  \n",
    "        db[z] = np.sum(dconv_prev[z])  #, axis =1) ## AXIS?\n",
    "    \n",
    "    if(pad >=1 ):\n",
    "        dconv_in = dconv_in[:, pad:-pad, pad:-pad]  # Cropping\n",
    "        \n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "\n",
    "def convTransp(image, params, s = 1, pad = 1):\n",
    "    #s is always 1, upsample kernel = 2\n",
    "    #zero insertions between pixels s_downsampled -1 = 2-1 = 1\n",
    "    #required padding in order to double my dimensions with the given data:\n",
    "    #(i-1)*2 + k -2p = output size, where our padding is k - p -1 = 2-0-1=1(we assume p=0)\n",
    "    input_s = image.shape[1]\n",
    "    target_dim = input_s*2 #final dim, after conv\n",
    "    required_pad = 1 #always for filter 2x2 ,stride 1, zero insertion 1 and main target to double dim\n",
    "    #make our new custom input\n",
    "    size = input_s*2 +1\n",
    "    new_in = np.zeros((image.shape[0], size, size))\n",
    "    for i in range(1, target_dim, 2):\n",
    "        for j in range(1, target_dim, 2):\n",
    "                new_in[:, i, j] = image[:, i//2, j//2]\n",
    "    #now we do a normal convolution(pad = 0)\n",
    "    res = conv(new_in, params, 1, 0) #thats the final result with target_dim\n",
    "    return res, new_in # we will need new_in so we wont calc it again for the backprop\n",
    "    \n",
    "def convTranspBackward(dconv_prev, new_in, filt, s = 1):\n",
    "    #First, we do a backward convolution on new_in,d_conv_prev,\n",
    "    #then we will choose form the d_conv_new the entries that match the initial 'smaller' input image\n",
    "    #by selecting the odd matrix cells 1,3,5... because we had applied a standard pad=1,zero inser=1\n",
    "    dconv_in, dfilt, db = convolutionBackward(dconv_prev, new_in, filt,1,0)\n",
    "    #Now its time to choose the right entries to build the gradients of the initial input image\n",
    "    dim = dconv_in.shape[1]\n",
    "    final_dim = (new_in.shape[1] - 2)//2 + 1 #based on dimen of image before final conv that gives the result,..\n",
    "    #e.g. for new_in 7x7 that is going to convoluted with a 2x2 kernel and give a 6x6 upsampled from 3x3 init image\n",
    "    # now from this 7x7 --> apply the formula above,we get the 3x3 dimension number\n",
    "    res = np.zeros((dconv_in.shape[0], final_dim, final_dim))\n",
    "    for i in range(1, dim, 2):\n",
    "        for j in range(1, dim, 2):\n",
    "                res[:, i//2, j//2] = dconv_in[:, i, j]\n",
    "    return res, dfilt, db\n",
    "    \n",
    "    \n",
    "def maxpool(image, f=2 , s=2):\n",
    "    \n",
    "    h_range = int((image.shape[1] - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f) / s) + 1     # (W - F + 2P) / S \n",
    "    out = np.zeros((image.shape[0], h_range, w_range))\n",
    "    \n",
    "    for z in range(image.shape[0]): # Number of channels\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                out[z, _h, _w] = np.max(image[z, _h*s : _h*s + f, _w*s : _w*s + f])\n",
    "    return out\n",
    "\n",
    "def nanargmax(arr):\n",
    "    #print(arr.shape)\n",
    "    try:\n",
    "        idx = np.nanargmax(arr)\n",
    "        #print (idx)\n",
    "    except:\n",
    "        idx = 0\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "def maxpoolBackward(dpool, conv, f=2 , s=2):\n",
    "    num_c, h, w = conv.shape\n",
    "    h = int((h - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w = int((w - f) / s) + 1     # (W - F + 2P) / S \n",
    "    \n",
    "    dout = np.zeros(conv.shape)\n",
    "    #print(conv.shape)\n",
    "    for z in range(num_c): # Number of channels\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                (a, b) = nanargmax(conv[z, _h*s : _h*s + f, _w*s : _w*s + f]) #Getting the indexes from the max value in this area\n",
    "                #put it on the new array\n",
    "                dout[z, _h + a, _w + b] = dpool[z, _h, _w]\n",
    "    \n",
    "    \n",
    "    return dout\n",
    "\n",
    "\n",
    "def reshape(img, reshape_dim):\n",
    "    pad = reshape_dim - img.shape[1]\n",
    "    if(pad == 0):\n",
    "        return img\n",
    "    res = np.zeros((img.shape[0], reshape_dim, reshape_dim))\n",
    "    if(pad > 1):\n",
    "        res[:, pad//2:-(pad//2), pad//2:-(pad//2)] = img\n",
    "    else:\n",
    "        res[:, 0:-(pad), 0:-(pad)] = img\n",
    "    return res\n",
    "\n",
    "\n",
    "def crop(img, crop_dim): #TODO : look at it..future upgrade to downsample..maybe\n",
    "    start = img.shape[1]//2 - crop_dim//2\n",
    "    return img[:,(start):(start +crop_dim),(start):(start +crop_dim)]\n",
    "\n",
    "    \n",
    "    \n",
    "def crop2half(img):\n",
    "    #return gradients for decoder side and gradients for encoder side\n",
    "    n_ch = img.shape[0]//2\n",
    "    return img[n_ch:n_ch*2 ,:,:], img[0:n_ch,:,:]\n",
    "    \n",
    "def concat(img1_true, img2):\n",
    "    n_ch = img1_true.shape[0]\n",
    "    dim = img1_true.shape[1]\n",
    "    if(img2.shape[1] != dim):\n",
    "        img2 = crop(img2, dim)\n",
    "        print(\"Warning: Extra crop needed and handled!(%d --> %d)\" %(dim, img2.shape[1]))\n",
    "    res = np.zeros((n_ch*2, dim, dim))\n",
    "    res[0:n_ch,:,:] = img2\n",
    "    res[n_ch:n_ch*2 ,:,:] = img1_true\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cost Calculations ######\n",
    "def NLLLoss(logs, targets):\n",
    "    out = np.zeros(logs.shape)\n",
    "    mylen = logs.shape[0]*logs.shape[1]*logs.shape[1]\n",
    "    #print(len(targets))\n",
    "    #logs.shape (dim x dim) like target\n",
    "    for i in range(logs.shape[1]):\n",
    "        for j in range(logs.shape[2]):\n",
    "            if(targets[:,i,j] == 1):\n",
    "                out[:,i,j] = logs[:,i,j] #in that case the propab. is correct with targen being the 1\n",
    "            else:\n",
    "                out[:,i,j] = 1 - logs[:,i,j] # e.g if logs[i,j]= 0.4 and we want(target) 0 --> 1-0.4=0.6 prob. for zero \n",
    "    #or\n",
    "    #out = targets*logs + ((-1*(targets-1)) - (-1*(targets-1))*logs)\n",
    "    return -np.log(out.sum()/mylen)\n",
    "\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    \n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost) ### ATTENTION!\n",
    "\n",
    "def normalize(output):\n",
    "    output[output<-10]=-4\n",
    "    output[output>10] = 4\n",
    "    return output\n",
    "\n",
    "###### Accuracy Calculation ######\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  TRAIN  ######\n",
    "#import torch`\n",
    "import time\n",
    "time.time()\n",
    "\n",
    "\n",
    "def train(X, Y, epochs, learning_rate, dropout, verbose=True, callback=None):\n",
    "    # initiation of neural net parameters\n",
    "    #params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    #f1,f2,f3,f4,f5,fd1,fd2,fd3,fd4\n",
    "    filters,bias, f_dc = init_filters(2,16) #Double the channel-filter 4 times  (up to 256 and back again)\n",
    "    ##Final 1x1 filter\n",
    "    trim = 0.1\n",
    "    out_f = np.random.randn(1,16,1,1)*trim\n",
    "    out_b = np.random.randn(out_f.shape[0],1)*trim  \n",
    "    out_fb = [out_f, out_b]\n",
    "    #filter shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "\n",
    "    v_adam =  [] #filter1,filter2\n",
    "    s_adam =  [] \n",
    "    bv_adam = [] #bias1,bias2\n",
    "    bs_adam = []\n",
    "    fdc_v_adam=[] #filter,bias\n",
    "    fdc_s_adam=[]\n",
    "    #format: [v1,v2,v3,v4,v5,vd1,vd2,vd3,vd4] ,same for the rest, each of these include a tuple for the 2 conv filter\n",
    "    #[s1,s2,s3,s4,s5,sd1,sd2,vs3,sd4]\n",
    "    # upsampling filters : [v1_dc,v2_dc,v3_dc]  \n",
    "\n",
    "            \n",
    "\n",
    "    # performing calculations for subsequent iterations\n",
    "    \n",
    "    \n",
    "    [f1,f2,f3] = filters\n",
    "    [b1,b2,b3]= bias \n",
    "    \n",
    "    f1_dc = f_dc[0][0]\n",
    "    b1_dc = f_dc[0][1]\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # step forward\n",
    "        running_loss = 0\n",
    "        accuracy = 0\n",
    "        batch = 1\n",
    "        print(\"Epoch: {%d}\" %(e+1))\n",
    "        for c in range(0, X.shape[0], batch):\n",
    "            print('Batch: {}'.format(int(c/batch +1)))\n",
    "            #### Reset Gradients (Every batch) ####\n",
    "            beta1= 0.95\n",
    "            beta2= 0.99\n",
    "            lr = learning_rate\n",
    "            batch_size= batch\n",
    "            if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "                batch = X.shape[0] - c\n",
    "            X_t = X[c:(c + batch)]  # shape:(m, ch, h, w)\n",
    "            Y_t = Y[c:(c + batch)]\n",
    "            cost = 0\n",
    "            accuracy = 0\n",
    "            \n",
    "            df =  []\n",
    "            db =  []\n",
    "            dfb=  []\n",
    "            for i in filters:\n",
    "                v1 = np.zeros(i[0].shape)\n",
    "                v2 = np.zeros(i[1].shape)\n",
    "                s1 = np.zeros(i[0].shape)\n",
    "                s2 = np.zeros(i[1].shape)\n",
    "                v_a = [v1, v2]\n",
    "                s_a = [s1, s2]\n",
    "                v_adam.append(v_a)\n",
    "                s_adam.append(s_a)\n",
    "                \n",
    "                df1_t = np.zeros(i[0].shape)\n",
    "                df2_t = np.zeros(i[1].shape)\n",
    "                f_temp = [df1_t, df2_t]\n",
    "                df.append(f_temp)\n",
    "                \n",
    "            for i in bias:\n",
    "                bv1 = np.zeros(i[0].shape)\n",
    "                bv2 = np.zeros(i[1].shape)\n",
    "                bs1 = np.zeros(i[0].shape)\n",
    "                bs2 = np.zeros(i[1].shape)    \n",
    "                bv_a = [bv1, bv2]\n",
    "                bs_a = [bs1, bs2]\n",
    "                bv_adam.append(bv_a)\n",
    "                bs_adam.append(bs_a)\n",
    "                \n",
    "                \n",
    "                db1_t = np.zeros(i[0].shape)\n",
    "                db2_t = np.zeros(i[1].shape)\n",
    "                b_temp = [db1_t, db2_t]\n",
    "                db.append(b_temp)\n",
    "            \n",
    "            for i in f_dc:\n",
    "                fdc_v1 = np.zeros(i[0].shape)\n",
    "                bdc_v2 = np.zeros(i[1].shape)\n",
    "                fdc_s1 = np.zeros(i[0].shape)\n",
    "                bdc_s2 = np.zeros(i[1].shape)    \n",
    "                fdc_v_a = [fdc_v1, bdc_v2]\n",
    "                fdc_s_a = [fdc_s1, bdc_s2]\n",
    "                fdc_v_adam.append(fdc_v_a)\n",
    "                fdc_s_adam.append(fdc_s_a)\n",
    "                \n",
    "                \n",
    "                df1_t = np.zeros(i[0].shape)\n",
    "                db1_t = np.zeros(i[1].shape)\n",
    "                fb_temp = [df1_t, db1_t]\n",
    "                dfb.append(fb_temp)\n",
    "            \n",
    "            \n",
    "            #Final layer 1x1 filter setup\n",
    "\n",
    "            v_out_f = np.zeros(out_f.shape)\n",
    "            s_out_f = np.zeros(out_f.shape)\n",
    "            bv_out_b = np.zeros(out_b.shape)\n",
    "            bs_out_b = np.zeros(out_b.shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "            dout_f = np.zeros(out_f.shape)\n",
    "            dout_b = np.zeros(out_b.shape)\n",
    "            \n",
    "            ######################################\n",
    "            \n",
    "            \n",
    "            #timestamp1 = time.time()\n",
    "            \n",
    "            \n",
    "            [df1,df2,df3] = df\n",
    "            [db1,db2,db3] = db \n",
    "            dfb1_dc     = dfb\n",
    "            \n",
    "            \n",
    "            for b in range(batch):\n",
    "                \n",
    "                print('Image: {}/{}'.format((b+1),batch))\n",
    "                #################### TODO: BLOCK IMPLEMENTATION - FUTURE UPDATE ######################\n",
    "                    \n",
    "                    \n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ######################## Forward Propagation Convolution Part  ##########################\n",
    "\n",
    "                \n",
    "                ########### 1st Big Layer ###########    (with zero padding ='same',so with stride =1 we get same dim as the input)\n",
    "                params = [f1[0], b1[0]]  \n",
    "                conv1_1 = conv(X_t[b], params, 1)   #conv1 shape = (num_channels, h, w), padding = 1 (same output dim)\n",
    "                conv1_1[conv1_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f1[1], b1[1]]\n",
    "                conv1_2 = conv(conv1_1, params, 1)\n",
    "                conv1_2[conv1_2<=0] = 0 #Relu\n",
    "                ##################################### conv1_2: 32x32x16\n",
    "                \n",
    "                pl1 = maxpool(conv1_2, 2, 2) #   pl1 : (32-2)/2+1  = 16 \n",
    "                ## ADD DROPOUT HERE(on pl1)\n",
    "                \n",
    "                ########### 2nd Big Layer ###########\n",
    "                params = [f2[0], b2[0]]  \n",
    "                conv2_1 = conv(pl1, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv2_1[conv2_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f2[1], b2[1]]\n",
    "                conv2_2 = conv(conv2_1, params, 1)\n",
    "                conv2_2[conv2_2<=0] = 0 #Relu             \n",
    "                #####################################  16x16x32\n",
    "\n",
    "          \n",
    "                ##################################### \n",
    "                ##################################### \n",
    "                #####################################\n",
    "                #Deconvolution/Upsampling\n",
    "                # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "               \n",
    "                params = [f1_dc, b1_dc] # deconv filter, deconv bias\n",
    "                dc1, new_in1 = convTransp(conv2_2, params, 1, 0)   #result:   =  32x32x16 , # conv5_2 requires NO crop\n",
    "                #Concat dc6 with conv2_2 so we get 32 channels (32x32x32)\n",
    "                c1 = concat(dc1, conv1_2) # 1st one is the right one size  \n",
    "                \n",
    "                ########### 1st Big dc Layer ###########          32x32x32     \n",
    "                params = [f3[0], b3[0]]  \n",
    "                conv3_1 = conv(c1, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv3_1[conv3_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f3[1], b3[1]]\n",
    "                conv3_2 = conv(conv3_1, params, 1)\n",
    "                conv3_2[conv3_2<=0] = 0 #Relu   \n",
    "                #####################################    32x32x16\n",
    "                \n",
    "                ############################# Last Layer conv(1x1) --> 128x128x1 ##########################\n",
    "                params = [out_f, out_b]\n",
    "                output = conv(conv3_2, params, 1, 0) #output.shape: 32x32x1\n",
    "                \n",
    "                \n",
    "                output = normalize(output)\n",
    "                ## Sigmoid ##\n",
    "                Y_hat = sigmoid(output)\n",
    "                print(Y_hat[:,0:10,0:10])\n",
    "                #label crop is needed\n",
    "                #Y_t_b = crop(Y_t[b], Y_hat.shape[1])\n",
    "                plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "                cost += NLLLoss(Y_hat, Y_t[b])\n",
    "                print(cost/(b+1))\n",
    "                \n",
    "                accuracy += get_accuracy_value(Y_hat, Y_t[b])\n",
    "                print(accuracy/(b+1))\n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ################################# Backward Propagation ##################################\n",
    "                \n",
    "                \n",
    "                #Pixel-wise sub, we we can get the diff(Y includes the 2 classes 0 and 1)\n",
    "                \n",
    "                dA_prev = Y_hat - Y_t[b]\n",
    "                dZ_prev = sigmoid_backward(dA_prev, output)\n",
    "                conv_s =1 \n",
    "                dconv3_2, dout_f_, dout_b_ = convolutionBackward(dZ_prev, conv3_2, out_f, conv_s) #\n",
    "                #pack data\n",
    "                \n",
    "                \n",
    "                dconv3_2[conv3_2<=0] = 0             \n",
    "                dconv3_1, df3_2, db3_2 = convolutionBackward(dconv3_2, conv3_1, f3[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv3_1[conv3_1<=0] = 0\n",
    "                conc_dconv3, df3_1, db3_1 = convolutionBackward(dconv3_1, c1, f3[0], conv_s) #\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv3, dconv1_2 = crop2half(conc_dconv3)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #conv8_2 is not needed for input,we know how to select the right gradients            \n",
    "                #dconv1_2 = reshape(dconv1_2, conv1_2.shape[1])\n",
    "                dconv2_2, df1_dc, db1_dc = convTranspBackward(dconv3, new_in1, f1_dc, conv_s)\n",
    "                #pack data\n",
    "                print(df1_dc.shape)\n",
    "                dconv2_2[conv2_2<=0] = 0\n",
    "                dconv2_1, df2_2, db2_2 = convolutionBackward(dconv2_2, conv2_1, f2[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv2_1[conv2_1<=0] = 0\n",
    "                dpl1, df2_1, db2_1 = convolutionBackward(dconv2_1, pl1, f2[0], conv_s) #\n",
    "                \n",
    "                dconv1_2 += maxpoolBackward(dpl1, conv1_2, f=2 , s=2)\n",
    "                \n",
    "                dconv1_2[conv1_2<=0] = 0\n",
    "                dconv1_1, df1_2, db1_2 = convolutionBackward(dconv1_2, conv1_1, f1[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv1_1[conv1_1<=0] = 0\n",
    "                _, df1_1, db1_1 = convolutionBackward(dconv1_1, X_t[b], f1[0], conv_s) #\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                [df1,df2,df3] = df\n",
    "                [db1,db2,db3] = db \n",
    "                dfb1_dc     = dfb\n",
    "\n",
    "                \n",
    "                df1[0] += df1_1\n",
    "                df1[1] += df1_2\n",
    "                df2[0] += df2_1\n",
    "                df2[1] += df2_2\n",
    "                df3[0] += df3_1\n",
    "                df3[1] += df3_2\n",
    "                \n",
    "                \n",
    "                db1[0] += db1_1\n",
    "                db1[1] += db1_2\n",
    "                db2[0] += db2_1\n",
    "                db2[1] += db2_2\n",
    "                db3[0] += db3_1\n",
    "                db3[1] += db3_2\n",
    "                \n",
    "\n",
    "                #dfb1_dc[0] += df1_dc\n",
    "                #dfb1_dc[1] += db1_dc\n",
    "\n",
    "\n",
    "                dout_f += dout_f_\n",
    "                dout_b += dout_b_\n",
    "            \n",
    "            for i in range(len(filters)):\n",
    "                filters[i][0] -= lr*df[i][0]\n",
    "                bias[i][0] -= lr*db[i][0]\n",
    "            \n",
    "            \n",
    "            f1_dc -= lr*df1_dc\n",
    "            b1_dc -= lr*db1_dc\n",
    "            \n",
    "            out_fb[0] -= lr*dout_f\n",
    "            out_fb[1] -= lr*dout_b\n",
    "            \n",
    "            \n",
    "            ''''\n",
    "            ############## Adam Optimization ################\n",
    "            #changing the main structures(which are also updated)\n",
    "            #TODO: Future update - remove uneccessary memory loads/stores, v,s dont need to be saved \n",
    "            for i in range(len(filters)):\n",
    "                v_adam[i][0] = beta1*v_adam[i][0] + (1-beta1)*df[i][0]/batch_size #f1\n",
    "                s_adam[i][0] = beta2*s_adam[i][0] + (1-beta2)*(df[i][0]/batch_size)**2 #f1\n",
    "                filters[i][0] -= lr*v_adam[i][0]/np.sqrt(s_adam[i][0] + 1e-7)\n",
    "                \n",
    "                v_adam[i][1] = beta1*v_adam[i][1] + (1-beta1)*df[i][1]/batch_size #f2\n",
    "                s_adam[i][1] = beta2*s_adam[i][1] + (1-beta2)*(df[i][1]/batch_size)**2  #f2\n",
    "                filters[i][1] -= lr*v_adam[i][1]/np.sqrt(s_adam[i][1] + 1e-7)\n",
    "                \n",
    "            for i in range(len(bias)):\n",
    "                bv_adam[i][0] = beta1*bv_adam[i][0] + (1-beta1)*db[i][0]/batch_size #b1\n",
    "                bs_adam[i][0] = beta2*bs_adam[i][0] + (1-beta2)*(db[i][0]/batch_size)**2  #b1\n",
    "                bias[i][0] -= lr*bv_adam[i][0]/np.sqrt(bs_adam[i][0] + 1e-7)\n",
    "                \n",
    "                bv_adam[i][1] = beta1*bv_adam[i][1] + (1-beta1)*db[i][1]/batch_size #b2\n",
    "                bs_adam[i][1] = beta2*bs_adam[i][1] + (1-beta2)*(db[i][1]/batch_size)**2  #b2\n",
    "                bias[i][1] -= lr*bv_adam[i][1]/np.sqrt(bs_adam[i][1] + 1e-7)\n",
    "            \n",
    "            for i in range(len(f_dc)):\n",
    "                fdc_v_adam[i][0] = beta1*fdc_v_adam[i][0] + (1-beta1)*dfb[i][0]/batch_size #f1\n",
    "                fdc_s_adam[i][0] = beta2*fdc_s_adam[i][0] + (1-beta2)*(dfb[i][0]/batch_size)**2  #f1\n",
    "                f_dc[i][0] -= lr*fdc_v_adam[i][0]/np.sqrt(fdc_s_adam[i][0] + 1e-7)\n",
    "                \n",
    "                fdc_v_adam[i][1] = beta1*fdc_v_adam[i][1] + (1-beta1)*dfb[i][1]/batch_size #b2\n",
    "                fdc_s_adam[i][1] = beta2*fdc_s_adam[i][1] + (1-beta2)*(dfb[i][1]/batch_size)**2  #b2\n",
    "                f_dc[i][1] -= lr*fdc_v_adam[i][1]/np.sqrt(fdc_s_adam[i][1] + 1e-7)    \n",
    "            \n",
    "            v_out_f = beta1*v_out_f + (1 - beta1)*dout_f/batch_size #f\n",
    "            s_out_f = beta2*s_out_f + (1 - beta2)*(dout_f/batch_size)**2  #f\n",
    "            out_fb[0] -= lr*v_out_f/np.sqrt(s_out_f + 1e-7)\n",
    "            \n",
    "            bv_out_b = beta1*bv_out_b + (1 - beta1)*dout_b/batch_size #b\n",
    "            bs_out_b = beta2*bs_out_b + (1 - beta2)*(dout_b/batch_size)**2  #b\n",
    "            out_fb[1] -= lr*bv_out_b/np.sqrt(bs_out_b + 1e-7)\n",
    "            \n",
    "            \n",
    "            #Updating Conv Part\n",
    "            \n",
    "\n",
    "            #print(Yh)\n",
    "            #print(t)\n",
    "            #print(\"Batch:{}\".format(c+12))\n",
    "            \n",
    "           \n",
    "            f1 -= (learning_rate) * (df1/batch)\n",
    "            f2 -= (learning_rate) * (df2/batch)\n",
    "            b1 -= (learning_rate) * (db1/batch)\n",
    "            b2 -= (learning_rate) * (db2/batch)\n",
    "            filter_params = [f1, f2, b1, b2]\n",
    "                \n",
    "            # updating FC Part\n",
    "            #params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "            grads_values['dW1'] = dW1#/(batch)\n",
    "            grads_values['dW2'] = dW2#/(batch)\n",
    "            grads_values['db1'] = dB1#/(batch)\n",
    "            grads_values['db2'] = dB2#/(batch)\n",
    "                   \n",
    "            for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "                \n",
    "                params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "                params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "            \n",
    "            '''\n",
    "            print(\"Cost : {:.5f}\".format(cost/batch))\n",
    "            print(\"Accuracy : {:.5f}%\".format((accuracy*100)/batch))\n",
    "            \n",
    "        #END OF LOOP - EPOCH\n",
    "        #timestamp2 = time.time()\n",
    "        #print (\"This took %.2f seconds\" %(timestamp2 - timestamp1))\n",
    "        if(verbose):\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/batch, (accuracy*100)/batch))\n",
    "        #if(callback is not None):\n",
    "        #    callback(i, params_values)\n",
    "    #pack filters\n",
    "    params_values = [filters, bias, f_dc, out_fb]\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: {1}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.49953387 0.49495983 0.5005091  0.50166444 0.50128017 0.50172327\n",
      "   0.50143912 0.50138737 0.50131434 0.50132718]\n",
      "  [0.50803879 0.50788307 0.51163523 0.50809677 0.512329   0.51046247\n",
      "   0.51215757 0.50952654 0.51234597 0.50957294]\n",
      "  [0.50996138 0.50059131 0.51114077 0.50966753 0.51595264 0.51131928\n",
      "   0.51612995 0.51133199 0.51582035 0.51064578]\n",
      "  [0.50445889 0.50505241 0.50637439 0.50567709 0.50597611 0.50547193\n",
      "   0.50630609 0.50527235 0.50791506 0.50641564]\n",
      "  [0.50866351 0.49933314 0.50627186 0.50810495 0.51257677 0.5081566\n",
      "   0.51337103 0.51072129 0.51282624 0.50953569]\n",
      "  [0.50634877 0.50307122 0.50599867 0.50678904 0.5049176  0.50745335\n",
      "   0.5083668  0.50829781 0.5059042  0.50521478]\n",
      "  [0.50865478 0.49899908 0.50753704 0.50867509 0.51058189 0.50716082\n",
      "   0.5109032  0.50699062 0.50583833 0.50518215]\n",
      "  [0.50606084 0.50312359 0.50653522 0.50565359 0.50645641 0.50280178\n",
      "   0.50632385 0.50706876 0.50809497 0.50632143]\n",
      "  [0.50841759 0.49886593 0.50727875 0.50847495 0.50975218 0.50544622\n",
      "   0.50582101 0.50669557 0.50340426 0.50194583]\n",
      "  [0.50626    0.5030557  0.50763633 0.50588781 0.50761148 0.50344421\n",
      "   0.50686447 0.50747757 0.50408912 0.50682127]]]\n",
      "0.7076864376810436\n",
      "0.0458984375\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.70769\n",
      "Accuracy : 4.58984%\n",
      "Epoch:     1   -   cost: 0.71   -   Accuracy: 4.59%\n",
      "Epoch: {2}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.20871016 0.19604774 0.19301226 0.19473603 0.19386472 0.1943798\n",
      "   0.19415348 0.19423431 0.19431184 0.19431185]\n",
      "  [0.19990883 0.19619562 0.19301526 0.19457785 0.19352954 0.19700093\n",
      "   0.19408581 0.19755095 0.19409781 0.19760674]\n",
      "  [0.19009711 0.17907037 0.17568561 0.1788687  0.17775808 0.17990468\n",
      "   0.17895655 0.17915379 0.17894423 0.17934527]\n",
      "  [0.18803766 0.18666759 0.17676384 0.18280399 0.17845141 0.18510704\n",
      "   0.17720205 0.18573292 0.17703138 0.18527328]\n",
      "  [0.18984361 0.17623307 0.17294312 0.17610616 0.17354209 0.17672783\n",
      "   0.17457931 0.17641334 0.17331589 0.17612078]\n",
      "  [0.19038545 0.1852835  0.17858786 0.18350667 0.18085831 0.18582435\n",
      "   0.18140189 0.18739324 0.17951516 0.1865258 ]\n",
      "  [0.18945081 0.17817558 0.1740605  0.17735345 0.17540889 0.17874644\n",
      "   0.17691939 0.17913587 0.17654542 0.17445034]\n",
      "  [0.19027496 0.18482441 0.17877277 0.1832334  0.18114313 0.18477228\n",
      "   0.17996685 0.18819316 0.18537297 0.18820206]\n",
      "  [0.18934928 0.17803961 0.17461267 0.17765672 0.17611088 0.17808446\n",
      "   0.17771624 0.1816772  0.18198361 0.18197206]\n",
      "  [0.190113   0.1848043  0.17852832 0.18287588 0.17976083 0.18434232\n",
      "   0.18100319 0.18713133 0.18152691 0.18865011]]]\n",
      "0.19940192683462907\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.19940\n",
      "Accuracy : 70.60547%\n",
      "Epoch:     2   -   cost: 0.20   -   Accuracy: 70.61%\n",
      "Epoch: {3}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.18800284 0.17391507 0.17101483 0.17175361 0.17125479 0.17177739\n",
      "   0.17169695 0.17163912 0.17176184 0.17166941]\n",
      "  [0.1752774  0.16786622 0.16420913 0.16597666 0.16413644 0.16829985\n",
      "   0.16454304 0.16866681 0.1644454  0.16859472]\n",
      "  [0.16745987 0.15242499 0.14864225 0.15139496 0.15028442 0.15172005\n",
      "   0.1513406  0.15119132 0.15134302 0.15112073]\n",
      "  [0.1633355  0.15800063 0.1494411  0.15416634 0.14966921 0.15548764\n",
      "   0.14844701 0.15553245 0.14818362 0.15511875]\n",
      "  [0.16719082 0.14965801 0.14739407 0.14897642 0.14787765 0.149263\n",
      "   0.14852175 0.14924311 0.14795455 0.1492013 ]\n",
      "  [0.16565067 0.15712096 0.15163429 0.1551964  0.15231049 0.15619399\n",
      "   0.15267735 0.15777498 0.15173474 0.15717968]\n",
      "  [0.16692071 0.15134966 0.14812908 0.15026946 0.14844214 0.15127292\n",
      "   0.14918635 0.15286275 0.15105572 0.15197741]\n",
      "  [0.16537892 0.15675081 0.15148982 0.15468611 0.15234079 0.15521635\n",
      "   0.15173296 0.15897641 0.1570322  0.16303792]\n",
      "  [0.1668314  0.15122635 0.14843406 0.15013489 0.1489799  0.15124562\n",
      "   0.15130374 0.15571773 0.15706731 0.160894  ]\n",
      "  [0.16525535 0.15683137 0.15135527 0.15469408 0.15164692 0.15637765\n",
      "   0.15410717 0.1591353  0.15599233 0.16575054]]]\n",
      "0.1689813930309607\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.16898\n",
      "Accuracy : 70.60547%\n",
      "Epoch:     3   -   cost: 0.17   -   Accuracy: 70.61%\n",
      "Epoch: {4}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.18192387 0.16811778 0.16521083 0.16612643 0.16528645 0.16602116\n",
      "   0.16578673 0.1657744  0.16576018 0.16570434]\n",
      "  [0.16927277 0.16107961 0.15728646 0.15898104 0.15735202 0.16130628\n",
      "   0.15748829 0.16175656 0.15727482 0.16167332]\n",
      "  [0.16175955 0.14660072 0.14289586 0.14533839 0.143963   0.14535092\n",
      "   0.14491827 0.14481607 0.14486488 0.14458731]\n",
      "  [0.15750832 0.15156554 0.14295355 0.14764999 0.14299499 0.14885219\n",
      "   0.14201047 0.14913569 0.14181584 0.14901219]\n",
      "  [0.16168582 0.14396933 0.14153286 0.14294663 0.14167654 0.14311212\n",
      "   0.1423149  0.14312901 0.1418381  0.14288996]\n",
      "  [0.15972009 0.1505563  0.14527632 0.14866805 0.14571658 0.14939093\n",
      "   0.14613427 0.15123644 0.14547324 0.15038671]\n",
      "  [0.16143635 0.14514622 0.14223907 0.14425161 0.14220529 0.14481753\n",
      "   0.14268222 0.14612549 0.1447413  0.14882704]\n",
      "  [0.15946829 0.15038535 0.1452733  0.14827285 0.14593819 0.14862965\n",
      "   0.14712103 0.15290374 0.15235681 0.16022437]\n",
      "  [0.16142025 0.14506985 0.14246134 0.14419039 0.14299709 0.14592852\n",
      "   0.14551459 0.15116296 0.15311096 0.16146226]\n",
      "  [0.15934439 0.15049014 0.14523152 0.14851129 0.14548479 0.1503936\n",
      "   0.15006196 0.15326158 0.15421897 0.16434098]]]\n",
      "0.16335127565361748\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.16335\n",
      "Accuracy : 70.60547%\n",
      "Epoch:     4   -   cost: 0.16   -   Accuracy: 70.61%\n",
      "Epoch: {5}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.17885776 0.16555649 0.16263395 0.16372874 0.16288428 0.16351341\n",
      "   0.16329217 0.16324964 0.16322268 0.16317008]\n",
      "  [0.16669848 0.15794684 0.15410782 0.15587174 0.15428432 0.15804822\n",
      "   0.15404532 0.15845442 0.15374464 0.15849659]\n",
      "  [0.15880232 0.14379509 0.14010671 0.1423874  0.14085662 0.14229527\n",
      "   0.14196371 0.14170578 0.14186505 0.14132207]\n",
      "  [0.15478729 0.14866325 0.14010746 0.14475245 0.14007905 0.14604824\n",
      "   0.13928815 0.14662043 0.13941932 0.14684013]\n",
      "  [0.15921189 0.14126724 0.1388835  0.14024953 0.13893877 0.14033959\n",
      "   0.1396486  0.14031747 0.13933309 0.14038908]\n",
      "  [0.15694119 0.14762374 0.14235967 0.14581812 0.14274263 0.14649898\n",
      "   0.14328523 0.14851401 0.1436706  0.14761321]\n",
      "  [0.15876401 0.14224396 0.13976941 0.14146938 0.13939359 0.1417191\n",
      "   0.14011912 0.1436916  0.14326711 0.14845818]\n",
      "  [0.15677584 0.14760673 0.14254859 0.1454218  0.14345778 0.14579999\n",
      "   0.14645616 0.15019445 0.15305071 0.16293211]\n",
      "  [0.15872012 0.14221517 0.13990339 0.14161171 0.14039819 0.14410714\n",
      "   0.14442995 0.1506458  0.15385291 0.16984541]\n",
      "  [0.15663231 0.14767387 0.1425607  0.14556673 0.14380783 0.14786104\n",
      "   0.15021984 0.15242938 0.15914431 0.17056215]]]\n",
      "0.1623703703117755\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.16237\n",
      "Accuracy : 70.60547%\n",
      "Epoch:     5   -   cost: 0.16   -   Accuracy: 70.61%\n",
      "Epoch: {6}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.17683013 0.1635634  0.1606077  0.16164386 0.16061962 0.16139356\n",
      "   0.16103788 0.16112462 0.16082358 0.16117528]\n",
      "  [0.16477447 0.1546463  0.15030867 0.15196044 0.15030865 0.15425379\n",
      "   0.14977109 0.1549048  0.14958245 0.15530189]\n",
      "  [0.15660103 0.14075012 0.13728789 0.13916822 0.13773291 0.13913239\n",
      "   0.13891186 0.13852785 0.13905695 0.13818514]\n",
      "  [0.15274211 0.14546122 0.13719496 0.14109969 0.13704294 0.1425655\n",
      "   0.1364216  0.14351978 0.1366884  0.14402693]\n",
      "  [0.15746556 0.13841104 0.13642291 0.13742545 0.13626037 0.13716372\n",
      "   0.136945   0.13742989 0.13711916 0.13755906]\n",
      "  [0.15491249 0.14439008 0.13951626 0.14247038 0.1396602  0.14287669\n",
      "   0.14038762 0.14576124 0.14183443 0.14568812]\n",
      "  [0.15691591 0.13932841 0.13732181 0.13862394 0.13666186 0.13888402\n",
      "   0.13823015 0.14174385 0.14323972 0.15078726]\n",
      "  [0.15481399 0.14447741 0.13996884 0.14194388 0.14106387 0.14261706\n",
      "   0.14553798 0.14958148 0.15804604 0.17258101]\n",
      "  [0.15687833 0.13929205 0.13749795 0.13886567 0.13820686 0.14212708\n",
      "   0.14451306 0.15320573 0.16038208 0.18593652]\n",
      "  [0.15463091 0.14445009 0.13999041 0.14246341 0.1420953  0.14495253\n",
      "   0.15166551 0.15500317 0.16766551 0.18509116]]]\n",
      "0.16317989121075702\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.16318\n",
      "Accuracy : 70.60547%\n",
      "Epoch:     6   -   cost: 0.16   -   Accuracy: 70.61%\n",
      "Epoch: {7}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.17461427 0.16113556 0.15785877 0.15895111 0.1573728  0.15873186\n",
      "   0.15766388 0.15848353 0.15726673 0.15859806]\n",
      "  [0.16208719 0.14982001 0.1445997  0.14613655 0.14448104 0.14841933\n",
      "   0.14365829 0.14907033 0.14374589 0.14964661]\n",
      "  [0.15328662 0.13578296 0.13243729 0.13382339 0.13267063 0.13374421\n",
      "   0.13358675 0.13328279 0.13420775 0.13315343]\n",
      "  [0.149876   0.14055337 0.13239253 0.13539025 0.13210554 0.13681361\n",
      "   0.13170326 0.13852074 0.13245811 0.13976052]\n",
      "  [0.15427703 0.13371201 0.13224536 0.13250946 0.1316616  0.13208697\n",
      "   0.13276418 0.13293701 0.13348458 0.13389604]\n",
      "  [0.15191195 0.13955012 0.13459188 0.13725022 0.13466406 0.13726801\n",
      "   0.13579822 0.14121334 0.13923042 0.14537226]\n",
      "  [0.15383722 0.13449602 0.13310176 0.13398765 0.13230718 0.1344638\n",
      "   0.13508562 0.14073901 0.14644562 0.15911796]\n",
      "  [0.15186712 0.13979008 0.13537927 0.13671996 0.13716639 0.13716711\n",
      "   0.14468563 0.1520865  0.16838298 0.1932356 ]\n",
      "  [0.15376057 0.13456793 0.13358407 0.13417415 0.13434367 0.13866114\n",
      "   0.1451156  0.16023337 0.17463192 0.20848878]\n",
      "  [0.15165526 0.13979455 0.13544786 0.13738928 0.13873775 0.14055074\n",
      "   0.15367063 0.16037406 0.18319592 0.21129224]]]\n",
      "0.16560314781558708\n",
      "0.7060546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 32, 2, 2)\n",
      "Cost : 0.16560\n",
      "Accuracy : 70.60547%\n",
      "Epoch:     7   -   cost: 0.17   -   Accuracy: 70.61%\n",
      "Epoch: {8}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.16223267 0.14310757 0.14118071 0.14131784 0.13991451 0.14122083\n",
      "   0.14006324 0.14112856 0.13968364 0.14132427]\n",
      "  [0.14598491 0.12956339 0.12332324 0.12464617 0.12322758 0.12653929\n",
      "   0.12264474 0.1272377  0.12281329 0.1278657 ]\n",
      "  [0.13803807 0.11537888 0.11223518 0.11280276 0.11155692 0.11283759\n",
      "   0.11226355 0.11265907 0.11306916 0.11272927]\n",
      "  [0.13448606 0.12041461 0.11263801 0.11450885 0.11244338 0.11508508\n",
      "   0.11230835 0.11681234 0.11333069 0.1186324 ]\n",
      "  [0.13948744 0.11400342 0.11307679 0.11156717 0.11221469 0.11152466\n",
      "   0.11358868 0.11272812 0.11463901 0.11411887]\n",
      "  [0.13595904 0.11978067 0.11448269 0.11634804 0.11529983 0.11596249\n",
      "   0.11680086 0.12012012 0.12211201 0.12777274]\n",
      "  [0.13926955 0.11443755 0.11360221 0.11332822 0.11260299 0.11435239\n",
      "   0.11636192 0.12231996 0.13296835 0.14955507]\n",
      "  [0.13594313 0.1199798  0.11560015 0.11570633 0.1177749  0.11677444\n",
      "   0.12673833 0.13773056 0.16080758 0.19465678]\n",
      "  [0.13916653 0.11465585 0.11406276 0.11355929 0.11485865 0.11828423\n",
      "   0.12902729 0.14811561 0.17310012 0.21691261]\n",
      "  [0.13576363 0.11997291 0.11568734 0.11637939 0.11888359 0.11983715\n",
      "   0.13642898 0.14743502 0.18088692 0.21601827]]]\n",
      "0.15033291035712204\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.15033\n",
      "Accuracy : 70.60547%\n",
      "Epoch:     8   -   cost: 0.15   -   Accuracy: 70.61%\n",
      "Epoch: {9}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.15207788 0.12783118 0.12577902 0.1259529  0.12494854 0.12631565\n",
      "   0.12521019 0.12619033 0.12492779 0.12638227]\n",
      "  [0.13308733 0.11299656 0.10465612 0.10657905 0.10488102 0.10813044\n",
      "   0.10443398 0.10880234 0.10465461 0.10939186]\n",
      "  [0.12589489 0.09821064 0.09491385 0.09494127 0.09434883 0.09480814\n",
      "   0.09498546 0.09481813 0.09583688 0.09505913]\n",
      "  [0.12235737 0.10343492 0.09603316 0.09678937 0.09587507 0.09698065\n",
      "   0.09597465 0.09924352 0.09673349 0.10163577]\n",
      "  [0.12757032 0.09731515 0.09647194 0.09370843 0.09556742 0.09420531\n",
      "   0.09688154 0.09593645 0.09806131 0.09808561]\n",
      "  [0.1236037  0.10359003 0.09769709 0.0987118  0.09866506 0.09817657\n",
      "   0.1005504  0.10404838 0.10890924 0.11638389]\n",
      "  [0.12753756 0.09786968 0.09660766 0.09580021 0.09526998 0.0974345\n",
      "   0.10072027 0.10926209 0.12287397 0.14715833]\n",
      "  [0.12359982 0.10367953 0.09878273 0.09820091 0.101195   0.09977775\n",
      "   0.11318021 0.12709243 0.15817707 0.198352  ]\n",
      "  [0.12747164 0.09809111 0.09726221 0.09585241 0.09772538 0.10219209\n",
      "   0.11517621 0.14045938 0.17623182 0.23554084]\n",
      "  [0.12344966 0.10371105 0.09868129 0.0987542  0.10218083 0.10295835\n",
      "   0.12280949 0.13794141 0.18049373 0.22598669]]]\n",
      "0.13936141717467976\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.13936\n",
      "Accuracy : 70.60547%\n",
      "Epoch:     9   -   cost: 0.14   -   Accuracy: 70.61%\n",
      "Epoch: {10}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.14436782 0.11509757 0.11208656 0.11200308 0.11119389 0.11318051\n",
      "   0.11163959 0.11312643 0.11175855 0.11330059]\n",
      "  [0.12255545 0.09892012 0.08908237 0.09079936 0.08939073 0.09241087\n",
      "   0.08889397 0.09322082 0.08910616 0.09382898]\n",
      "  [0.11623455 0.08491069 0.08068111 0.08066492 0.08008937 0.08033666\n",
      "   0.08066073 0.0803101  0.08157942 0.08028912]\n",
      "  [0.112375   0.09018596 0.0821231  0.08251609 0.08208285 0.08255762\n",
      "   0.08202003 0.08499769 0.0825001  0.08708726]\n",
      "  [0.11788244 0.08364238 0.08251143 0.07922469 0.08149574 0.07977692\n",
      "   0.08301998 0.08199346 0.08434855 0.08487026]\n",
      "  [0.11322731 0.09026042 0.08359919 0.08419233 0.08446251 0.0840081\n",
      "   0.08688716 0.0911307  0.09667513 0.10625267]\n",
      "  [0.11799108 0.08406795 0.08276723 0.08109431 0.08114992 0.08343609\n",
      "   0.08703371 0.09869202 0.11270452 0.1461528 ]\n",
      "  [0.11312975 0.09030187 0.08470104 0.08392293 0.08771438 0.08593102\n",
      "   0.10167582 0.1188454  0.15582645 0.20211801]\n",
      "  [0.11797755 0.08426354 0.08342279 0.08123614 0.08347755 0.08865397\n",
      "   0.10300078 0.13531005 0.17850388 0.25796332]\n",
      "  [0.11297932 0.09035786 0.0843807  0.08409302 0.0880958  0.08881516\n",
      "   0.11091159 0.12959116 0.18214761 0.24110713]]]\n",
      "0.13138548146503587\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.13139\n",
      "Accuracy : 70.60547%\n",
      "Epoch:    10   -   cost: 0.13   -   Accuracy: 70.61%\n",
      "Epoch: {11}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.14122371 0.1082673  0.10405041 0.10327293 0.10314367 0.10475769\n",
      "   0.10344007 0.10485521 0.10358355 0.10507287]\n",
      "  [0.11714845 0.09037496 0.0798066  0.08148915 0.08014794 0.08308358\n",
      "   0.07983017 0.08395182 0.08004218 0.08459584]\n",
      "  [0.11142186 0.07675084 0.0716917  0.07199588 0.07142427 0.07196226\n",
      "   0.07200378 0.07207972 0.072936   0.07215448]\n",
      "  [0.10626983 0.08167606 0.07296852 0.0733839  0.07288093 0.07364504\n",
      "   0.07291638 0.07653158 0.073791   0.07856179]\n",
      "  [0.11252427 0.07530483 0.07324951 0.07030542 0.07201505 0.07118612\n",
      "   0.07392656 0.07426796 0.07577868 0.07833928]\n",
      "  [0.10705459 0.08177012 0.07432118 0.07487512 0.07558685 0.07536366\n",
      "   0.07893458 0.08430168 0.09087921 0.10368667]\n",
      "  [0.11260991 0.07578535 0.07357203 0.0722404  0.07184864 0.07534343\n",
      "   0.07955058 0.09482825 0.10913345 0.15113127]\n",
      "  [0.10706065 0.08184354 0.07564228 0.07469013 0.07945788 0.07809092\n",
      "   0.09673743 0.11755186 0.16545797 0.2195494 ]\n",
      "  [0.11252305 0.07600444 0.0743965  0.07251178 0.0744647  0.08213323\n",
      "   0.09702083 0.14051837 0.19250281 0.28693679]\n",
      "  [0.10688281 0.08186253 0.07522126 0.07485224 0.07991308 0.08078803\n",
      "   0.1070844  0.13216195 0.19801917 0.27088295]]]\n",
      "0.13117318128609023\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.13117\n",
      "Accuracy : 70.60547%\n",
      "Epoch:    11   -   cost: 0.13   -   Accuracy: 70.61%\n",
      "Epoch: {12}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.13011117 0.0915219  0.08720379 0.08586459 0.08613147 0.08746682\n",
      "   0.08680797 0.0875197  0.08677025 0.0878013 ]\n",
      "  [0.10434228 0.07406743 0.06285336 0.06357087 0.06343263 0.06481842\n",
      "   0.06316988 0.06582581 0.06325329 0.06634699]\n",
      "  [0.09824475 0.06177481 0.05654178 0.05675335 0.05573188 0.05762564\n",
      "   0.05634282 0.05732732 0.05692454 0.0575114 ]\n",
      "  [0.0929956  0.0657274  0.05655615 0.0564664  0.05691631 0.05689177\n",
      "   0.05684312 0.05958388 0.05752168 0.06037725]\n",
      "  [0.09905116 0.06010281 0.05745189 0.05473559 0.05577929 0.05584463\n",
      "   0.05774649 0.05781785 0.05897287 0.06042456]\n",
      "  [0.09339157 0.06574673 0.05734913 0.05774054 0.05885501 0.05799224\n",
      "   0.06081396 0.06406714 0.06932489 0.07913827]\n",
      "  [0.09926167 0.060473   0.05751031 0.05649984 0.05529919 0.05892502\n",
      "   0.06068837 0.07283682 0.08627872 0.12644716]\n",
      "  [0.09343276 0.06564595 0.05842584 0.05764759 0.06147026 0.06023791\n",
      "   0.07544469 0.09420804 0.1462501  0.22184355]\n",
      "  [0.09917777 0.06064794 0.05793616 0.05651535 0.05771555 0.06420282\n",
      "   0.07782998 0.12233689 0.18800033 0.3243484 ]\n",
      "  [0.09331716 0.06549006 0.0579804  0.0575934  0.06165239 0.06281576\n",
      "   0.08577212 0.11419596 0.19632848 0.2914814 ]]]\n",
      "0.12009265194587933\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.12009\n",
      "Accuracy : 70.60547%\n",
      "Epoch:    12   -   cost: 0.12   -   Accuracy: 70.61%\n",
      "Epoch: {13}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.13320795 0.09198102 0.08651881 0.08477259 0.08534841 0.08638057\n",
      "   0.08599086 0.08661782 0.0860361  0.08697638]\n",
      "  [0.10765215 0.07350923 0.06133807 0.06230725 0.06152753 0.06382379\n",
      "   0.06160201 0.06491159 0.061904   0.06559773]\n",
      "  [0.10146891 0.06134357 0.0553784  0.05561256 0.05451538 0.05651311\n",
      "   0.0551318  0.0565732  0.05603454 0.05696196]\n",
      "  [0.09545899 0.06545319 0.05499735 0.05527221 0.05522317 0.05613979\n",
      "   0.05528184 0.0595906  0.05672715 0.06062169]\n",
      "  [0.10243916 0.05962856 0.05609828 0.05380793 0.05439385 0.0556006\n",
      "   0.0572834  0.05873431 0.05947071 0.06439078]\n",
      "  [0.0964259  0.06527533 0.05587553 0.05679407 0.05762989 0.05746662\n",
      "   0.06103973 0.06723623 0.07555372 0.09083929]\n",
      "  [0.10254858 0.06015441 0.05618637 0.05591032 0.05423567 0.05969949\n",
      "   0.06352547 0.0812002  0.10208132 0.16571751]\n",
      "  [0.09633418 0.06515474 0.05722263 0.0565899  0.06144704 0.06084419\n",
      "   0.08321269 0.11330465 0.1960288  0.29152968]\n",
      "  [0.10252063 0.06030231 0.05661487 0.05627528 0.05757063 0.06813692\n",
      "   0.08673335 0.16049226 0.25635089 0.39360484]\n",
      "  [0.09615461 0.06496684 0.05665014 0.05629215 0.06161545 0.06393386\n",
      "   0.09494765 0.14544071 0.26809649 0.37301982]]]\n",
      "0.14319522253947944\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.14320\n",
      "Accuracy : 70.60547%\n",
      "Epoch:    13   -   cost: 0.14   -   Accuracy: 70.61%\n",
      "Epoch: {14}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.10624627 0.06389781 0.05823624 0.05661405 0.057016   0.05819633\n",
      "   0.05742825 0.05834281 0.05742389 0.05866036]\n",
      "  [0.08108965 0.04672526 0.03714779 0.03750922 0.03743367 0.03867663\n",
      "   0.03740449 0.03956138 0.03757435 0.04000617]\n",
      "  [0.07552878 0.03806058 0.03239688 0.03277939 0.03184853 0.03355341\n",
      "   0.03223396 0.0334773  0.03274309 0.03355046]\n",
      "  [0.07007819 0.04010074 0.03209342 0.03200975 0.03225768 0.03252645\n",
      "   0.03227828 0.03460683 0.03297315 0.03470419]\n",
      "  [0.07653904 0.03655045 0.03302456 0.03134639 0.03190227 0.03250143\n",
      "   0.03367157 0.03396483 0.03455744 0.03680099]\n",
      "  [0.07096932 0.04004349 0.03280347 0.03295794 0.03397143 0.03323858\n",
      "   0.03527503 0.03790175 0.04227657 0.05138458]\n",
      "  [0.07669636 0.03689709 0.03317772 0.03265373 0.03158115 0.03448674\n",
      "   0.03646271 0.04745342 0.06220746 0.1199424 ]\n",
      "  [0.07084018 0.03996831 0.03381468 0.03271944 0.0364487  0.03443056\n",
      "   0.04949884 0.07333232 0.15373138 0.31039314]\n",
      "  [0.07666339 0.03707818 0.03341024 0.03282067 0.03367205 0.03989193\n",
      "   0.05635462 0.12114357 0.25778438 0.5296523 ]\n",
      "  [0.07068914 0.03978113 0.03322019 0.03238969 0.03627667 0.0368467\n",
      "   0.06239834 0.11005563 0.28424634 0.45479831]]]\n",
      "0.13939365220542513\n",
      "0.7060546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 32, 2, 2)\n",
      "Cost : 0.13939\n",
      "Accuracy : 70.60547%\n",
      "Epoch:    14   -   cost: 0.14   -   Accuracy: 70.61%\n",
      "Epoch: {15}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.08911394 0.0489591  0.04394799 0.04254863 0.04268097 0.04407719\n",
      "   0.04306156 0.04425312 0.0431361  0.04458945]\n",
      "  [0.06525336 0.03323669 0.02715729 0.02617064 0.02752991 0.02741286\n",
      "   0.02755638 0.02831075 0.02781983 0.0284727 ]\n",
      "  [0.05988498 0.02632258 0.02265184 0.0226037  0.02258865 0.02354788\n",
      "   0.0228135  0.02367571 0.02327343 0.02368673]\n",
      "  [0.05494507 0.02740441 0.02210524 0.0213839  0.02238438 0.0222283\n",
      "   0.02214735 0.02406382 0.02306798 0.02369079]\n",
      "  [0.06096465 0.02497847 0.02288537 0.02159047 0.02246413 0.02296734\n",
      "   0.02380036 0.02394167 0.02393579 0.02479746]\n",
      "  [0.05562862 0.02733437 0.02265726 0.02238771 0.02350966 0.02298536\n",
      "   0.02407246 0.02511411 0.02671203 0.03322709]\n",
      "  [0.06108961 0.02530359 0.02309873 0.02294813 0.02177843 0.02417698\n",
      "   0.02518958 0.03265262 0.04850165 0.11928601]\n",
      "  [0.05553497 0.0273034  0.0238962  0.02188414 0.0259921  0.02346383\n",
      "   0.03604002 0.06074991 0.17785854 0.43629562]\n",
      "  [0.06104934 0.02544315 0.02346221 0.02320699 0.02390236 0.02928541\n",
      "   0.04434214 0.14109367 0.37980879 0.76965745]\n",
      "  [0.05539276 0.02713531 0.02326975 0.0213524  0.02584225 0.0253765\n",
      "   0.05530308 0.12267295 0.47172824 0.68403554]]]\n",
      "0.19391975284787072\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.19392\n",
      "Accuracy : 70.60547%\n",
      "Epoch:    15   -   cost: 0.19   -   Accuracy: 70.61%\n",
      "Epoch: {16}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.0422531  0.0148832  0.01185127 0.01112625 0.01101636 0.01149306\n",
      "   0.01111619 0.01148949 0.01108296 0.01155553]\n",
      "  [0.02230194 0.00623542 0.00429473 0.00409547 0.00433655 0.00415812\n",
      "   0.00433492 0.00429185 0.00436854 0.00434094]\n",
      "  [0.01821349 0.00427483 0.0030367  0.00284287 0.00288997 0.00299678\n",
      "   0.00299492 0.00299382 0.00305078 0.00301009]\n",
      "  [0.01632368 0.0043242  0.00292319 0.00273836 0.00294493 0.0028107\n",
      "   0.0029839  0.00293009 0.00309424 0.0029597 ]\n",
      "  [0.01858686 0.00402298 0.00311114 0.00273302 0.00298493 0.00292293\n",
      "   0.00315654 0.00294321 0.00317194 0.00299882]\n",
      "  [0.0166717  0.00441452 0.00305318 0.00286657 0.00314289 0.00281946\n",
      "   0.00309658 0.00297658 0.00304339 0.00301436]\n",
      "  [0.01882928 0.00404293 0.00315499 0.00285479 0.00289002 0.00293116\n",
      "   0.00284667 0.00283332 0.00313424 0.00445794]\n",
      "  [0.01666822 0.00438326 0.00313548 0.00285073 0.0033576  0.00266313\n",
      "   0.00318183 0.0033268  0.00718052 0.01963726]\n",
      "  [0.01881742 0.00408398 0.00318061 0.00289745 0.00299443 0.00305195\n",
      "   0.00390225 0.00761048 0.02972994 0.19573668]\n",
      "  [0.01662437 0.00439148 0.00307742 0.00280525 0.0032013  0.0028483\n",
      "   0.00452592 0.00907791 0.05627031 0.24402114]]]\n",
      "0.07355446243786132\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.07355\n",
      "Accuracy : 70.60547%\n",
      "Epoch:    16   -   cost: 0.07   -   Accuracy: 70.61%\n",
      "Epoch: {17}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.1289148  0.10483402 0.11586384 0.115585   0.1180541  0.12352408\n",
      "   0.11635399 0.12062287 0.1170425  0.12099345]\n",
      "  [0.11938214 0.11018308 0.1139931  0.11168635 0.11875624 0.11890183\n",
      "   0.11876577 0.12131487 0.11902434 0.118984  ]\n",
      "  [0.11848341 0.10253145 0.10881609 0.10998991 0.11421194 0.11540038\n",
      "   0.11283081 0.11224412 0.11325054 0.11322821]\n",
      "  [0.11011608 0.10338578 0.10305226 0.10216628 0.10481668 0.1070579\n",
      "   0.10132837 0.11260055 0.10825896 0.10406779]\n",
      "  [0.12042155 0.09692293 0.10723317 0.10361927 0.10668012 0.11219381\n",
      "   0.10707059 0.11867474 0.10904325 0.14742027]\n",
      "  [0.11169629 0.10272544 0.10706723 0.10579713 0.1084254  0.10398513\n",
      "   0.10840831 0.12816891 0.15561832 0.27291543]\n",
      "  [0.12024674 0.09705037 0.10748558 0.10969847 0.10001573 0.11834657\n",
      "   0.13006187 0.25476886 0.40521943 0.77092378]\n",
      "  [0.11136051 0.10127014 0.11295334 0.09826758 0.12619548 0.1108088\n",
      "   0.26421675 0.48842879 0.86466529 0.93266749]\n",
      "  [0.11979111 0.09773782 0.10943228 0.11000936 0.11635456 0.17636454\n",
      "   0.34680911 0.83889054 0.95094052 0.99189201]\n",
      "  [0.11082724 0.10048542 0.10888242 0.09313881 0.12989953 0.13418016\n",
      "   0.47345074 0.73685607 0.96199943 0.98542118]]]\n",
      "0.5426868590241096\n",
      "0.646484375\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.54269\n",
      "Accuracy : 64.64844%\n",
      "Epoch:    17   -   cost: 0.54   -   Accuracy: 64.65%\n",
      "Epoch: {18}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[1.14944156e-02 2.42693538e-03 1.62806572e-03 1.49177185e-03\n",
      "   1.47773403e-03 1.55722462e-03 1.47767136e-03 1.56630157e-03\n",
      "   1.47680082e-03 1.56367663e-03]\n",
      "  [2.66537469e-03 3.47638283e-04 1.36663989e-04 1.38558738e-04\n",
      "   1.40113257e-04 1.40731802e-04 1.44098442e-04 1.40575949e-04\n",
      "   1.45060568e-04 1.41126163e-04]\n",
      "  [2.04510340e-03 2.16620204e-04 1.06878794e-04 9.07631109e-05\n",
      "   8.63354394e-05 9.60707002e-05 8.62534857e-05 9.57243912e-05\n",
      "   8.58660327e-05 9.69918502e-05]\n",
      "  [1.57538786e-03 2.03113372e-04 8.65216416e-05 9.16588873e-05\n",
      "   8.24723178e-05 9.00268657e-05 8.48253549e-05 9.17829201e-05\n",
      "   8.72214059e-05 9.57506844e-05]\n",
      "  [1.88129324e-03 1.93566953e-04 1.02250853e-04 9.40827847e-05\n",
      "   8.94783616e-05 9.66413093e-05 8.85167859e-05 9.71847102e-05\n",
      "   9.15871064e-05 1.01209881e-04]\n",
      "  [1.63805619e-03 1.98688138e-04 9.09581427e-05 9.57912393e-05\n",
      "   8.74140491e-05 9.15101375e-05 8.97065800e-05 9.48612740e-05\n",
      "   9.22152209e-05 1.01251693e-04]\n",
      "  [1.96429982e-03 1.91426575e-04 1.05695802e-04 9.57196521e-05\n",
      "   9.19806721e-05 1.00410813e-04 1.00696244e-04 1.11651333e-04\n",
      "   1.35734181e-04 1.79297712e-04]\n",
      "  [1.64457234e-03 1.97742819e-04 9.17732379e-05 9.51540470e-05\n",
      "   9.03168392e-05 1.00111330e-04 1.05862951e-04 1.55132219e-04\n",
      "   2.88690000e-04 8.22864884e-04]\n",
      "  [1.96590059e-03 1.92026604e-04 1.07580198e-04 9.59037138e-05\n",
      "   9.85971007e-05 1.04269139e-04 1.23466980e-04 2.54081797e-04\n",
      "   8.33826959e-04 3.14179799e-03]\n",
      "  [1.64646779e-03 1.99128212e-04 9.09292809e-05 9.68829988e-05\n",
      "   9.08765061e-05 1.04889838e-04 1.20932697e-04 2.64312992e-04\n",
      "   8.27108546e-04 2.90783400e-03]]]\n",
      "0.0019114112319444571\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.00191\n",
      "Accuracy : 70.60547%\n",
      "Epoch:    18   -   cost: 0.00   -   Accuracy: 70.61%\n",
      "Epoch: {19}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.0160568  0.00400844 0.00290845 0.00267847 0.00267504 0.00279396\n",
      "   0.00266995 0.00281079 0.00265927 0.00281201]\n",
      "  [0.0042853  0.00073515 0.00030657 0.00032802 0.00031848 0.00033422\n",
      "   0.00032859 0.00033236 0.00033104 0.000333  ]\n",
      "  [0.00360852 0.00049826 0.00028045 0.00023614 0.00023244 0.00024886\n",
      "   0.00023084 0.00024776 0.00023    0.00024965]\n",
      "  [0.00278323 0.00047744 0.00022626 0.00024949 0.0002225  0.00024515\n",
      "   0.00022842 0.0002492  0.00023417 0.00025729]\n",
      "  [0.00331394 0.00045102 0.0002755  0.00025012 0.00024534 0.00025702\n",
      "   0.0002405  0.00026049 0.0002511  0.00027214]\n",
      "  [0.00286922 0.00046742 0.00023601 0.00026047 0.00023258 0.00024894\n",
      "   0.00024253 0.00026413 0.00025576 0.00028898]\n",
      "  [0.00346297 0.00044674 0.00028224 0.00025463 0.00025037 0.00026698\n",
      "   0.00028089 0.00030488 0.0004001  0.00056951]\n",
      "  [0.00289588 0.00046361 0.00023857 0.00025993 0.00024143 0.00027696\n",
      "   0.00028971 0.00045231 0.00090112 0.0030903 ]\n",
      "  [0.00345922 0.00044869 0.00028825 0.00025485 0.00026976 0.00027805\n",
      "   0.00034446 0.00074468 0.002694   0.01192444]\n",
      "  [0.00289396 0.00046782 0.00023705 0.00026527 0.00024556 0.0002871\n",
      "   0.00033297 0.00077191 0.00252442 0.01059405]]]\n",
      "0.0066712910894490265\n",
      "0.7060546875\n",
      "(16, 32, 2, 2)\n",
      "Cost : 0.00667\n",
      "Accuracy : 70.60547%\n",
      "Epoch:    19   -   cost: 0.01   -   Accuracy: 70.61%\n",
      "Epoch: {20}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.04016111 0.01613713 0.01396499 0.01389013 0.01319257 0.01463228\n",
      "   0.013111   0.01460644 0.01310538 0.01464415]\n",
      "  [0.01600604 0.0060678  0.00306727 0.00377565 0.00335954 0.00386836\n",
      "   0.00348589 0.00384155 0.00353099 0.00386414]\n",
      "  [0.01712707 0.00517912 0.00426657 0.00356924 0.00383637 0.00383649\n",
      "   0.00375805 0.003804   0.00375091 0.00383167]\n",
      "  [0.0135059  0.00527884 0.00342651 0.0040993  0.00364312 0.00410405\n",
      "   0.00374423 0.00406683 0.00382778 0.00423693]\n",
      "  [0.01603845 0.00491156 0.00436342 0.0038878  0.00396783 0.00401127\n",
      "   0.00397081 0.00412698 0.00443892 0.0045391 ]\n",
      "  [0.01362571 0.00514679 0.00350985 0.00423789 0.00371371 0.004132\n",
      "   0.00400163 0.00461658 0.00473699 0.00602378]\n",
      "  [0.01649157 0.00481028 0.00440525 0.00399576 0.00400049 0.00418984\n",
      "   0.00487663 0.00557614 0.00941102 0.01697671]\n",
      "  [0.01380637 0.00507515 0.00355981 0.00428574 0.00394634 0.00466805\n",
      "   0.00524651 0.00985378 0.02452945 0.11201087]\n",
      "  [0.01651259 0.00483633 0.00452092 0.00399527 0.00460743 0.00461168\n",
      "   0.00666987 0.01750073 0.07359981 0.22879359]\n",
      "  [0.01378173 0.00512904 0.00358523 0.00440092 0.00408969 0.00515897\n",
      "   0.00624274 0.01796015 0.06185519 0.21743845]]]\n",
      "0.04221081405647731\n",
      "0.7060546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 32, 2, 2)\n",
      "Cost : 0.04221\n",
      "Accuracy : 70.60547%\n",
      "Epoch:    20   -   cost: 0.04   -   Accuracy: 70.61%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### Training ######\n",
    "#train_images, train_labels, test_images, test_labels\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "params_values = train(train_images, train_labels, 20, 0.01, True) #0.05 stable LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-d1175c062e01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m###### Prediction ######\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mYt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtemp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_labels' is not defined"
     ]
    }
   ],
   "source": [
    "###### Prediction ######\n",
    "Yt = test_labels.T\n",
    "temp1 = []\n",
    "for i in range(Yt.shape[1]):\n",
    "        for j in range(Yt.shape[0]):\n",
    "            if(Yt[j][i]==1):\n",
    "                temp1.append(j)\n",
    "Yt=np.array(temp1)\n",
    "Y_test_hat, _ = full_forward_propagation(np.transpose(test_images), params_values, NN_ARCHITECTURE)#multiple?!\n",
    "\n",
    "Yht = np.array(Y_test_hat.T)\n",
    "#x_log=np.log( np.exp(Yh) / np.sum(np.exp(Yh), axis = 1) )  #(60000,10) , we need to add along columns so we get sum of 1 on every example-row\n",
    "num = np.exp(Yht)\n",
    "den = np.sum(np.exp(Yht), axis = 1)\n",
    "for i in range(Yht.shape[0]): #60000\n",
    "                #for j in range(Yh.shape[1]): #10\n",
    "                Yht[i][:] = np.log(num[i][:] / den[i])  \n",
    "\n",
    "#cost = get_cost_value(Yht, Yt)\n",
    "\n",
    "#cost_history.append(cost)\n",
    "accuracy = get_accuracy_value(Y_test_hat, test_labels.T)\n",
    "#accuracy_history.append(accuracy)\n",
    "print(\"Accuracy: {:.5f}%\".format( accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
