{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "#import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load from /home/USER/data/mnist or elsewhere; download if missing.\"\"\"\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "from matplotlib import image\n",
    "\n",
    "\n",
    "\n",
    "def mnist(path=None):\n",
    "    r\"\"\"Return (train_images, train_labels, test_images, test_labels).\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory containing MNIST. Default is\n",
    "            /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist.\n",
    "            Create if nonexistant. Download any missing files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, train_labels, test_images, test_labels), each\n",
    "            a matrix. Rows are examples. Columns of images are pixel values.\n",
    "            Columns of labels are a onehot encoding of the correct class.\n",
    "    \"\"\"\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz',\n",
    "             'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz',\n",
    "             't10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "    if path is None:\n",
    "        # Set path to /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist\n",
    "        path = os.path.join(os.path.expanduser('~'), 'data', 'mnist')\n",
    "\n",
    "    # Create path if it doesn't exist\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Download any missing files\n",
    "    for file in files:\n",
    "        if file not in os.listdir(path):\n",
    "            urlretrieve(url + file, os.path.join(path, file))\n",
    "            print(\"Downloaded %s to %s\" % (file, path))\n",
    "    print(\"All files are ready to gzip!\")\n",
    "\n",
    "    def _images(path):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        with gzip.open(path) as f:\n",
    "            # First 16 bytes are magic_number, n_imgs, n_rows, n_cols\n",
    "            pixels = np.frombuffer(f.read(), 'B', offset=16)\n",
    "        return pixels.reshape(-1, 1, 28, 28).astype('float32') / 255\n",
    "\n",
    "    def _labels(path):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        with gzip.open(path) as f:\n",
    "            # First 8 bytes are magic_number, n_labels\n",
    "            integer_labels = np.frombuffer(f.read(), 'B', offset=8)\n",
    "        def _onehot(integer_labels):\n",
    "            \"\"\"Return matrix whose rows are onehot encodings of integers.\"\"\"\n",
    "            n_rows = len(integer_labels)\n",
    "            n_cols = integer_labels.max() + 1\n",
    "            onehot = np.zeros((n_rows, n_cols), dtype='uint8')\n",
    "            onehot[np.arange(n_rows), integer_labels] = 1\n",
    "            return onehot\n",
    "\n",
    "        return _onehot(integer_labels)\n",
    "    print(\"Train Images : Loading . . .\")\n",
    "    train_images = _images(os.path.join(path, files[0]))\n",
    "    print(\"Train Labels : Loading . . .\")\n",
    "    train_labels = _labels(os.path.join(path, files[1]))\n",
    "    print(\"Test Images  : Loading . . .\")\n",
    "    test_images = _images(os.path.join(path, files[2]))\n",
    "    print(\"Test Labels  : Loading . . .\")\n",
    "    test_labels = _labels(os.path.join(path, files[3]))\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return train_images[0,:,:,:], train_labels[0,:] #, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are ready to gzip!\n",
      "Train Images : Loading . . .\n",
      "Train Labels : Loading . . .\n",
      "Test Images  : Loading . . .\n",
      "Test Labels  : Loading . . .\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labelsa= mnist()  #, test_images, test_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 32, 32)\n",
      "(1, 1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "ch = 1 #Number of channels\n",
    "train_images=cv2.resize(train_images[0,:,:], (32,32)).reshape(1,1,32,32)\n",
    "print(train_images.shape)\n",
    "train_labels = train_images\n",
    "print(train_labels.shape) # ONE-HOT !!!\n",
    "#print(train_images.T.shape)\n",
    "#print(train_labels.reshape((train_images.shape[0], 10)).shape)\n",
    "\n",
    "##### Getting Ready for the Conv Layers #####\n",
    "#train_images = train_images.reshape(train_images.shape[0], ch, 28, 28).squeeze()\n",
    "#test_images = test_images.reshape(test_images.shape[0], ch, 28, 28).squeeze()\n",
    "\n",
    "#############################################\n",
    "\n",
    "#y = train_images[1].reshape(1,28,28)\n",
    "#print(y.squeeze().shape)\n",
    "'''\n",
    "temp = []\n",
    "#train_labels = train_labels.sum(1)\n",
    "for i in range(int(len(train_labels[:]))):\n",
    "    temp.append(list(train_labels[i][:]).index(1))\n",
    "    \n",
    "    \n",
    "train_labels = np.array(temp.copy())\n",
    "#print(train_labels[0:5])\n",
    "'''\n",
    "#plt.imshow(train_images[0].squeeze(), cmap='Greys_r');\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_labels[0].squeeze(), cmap='Greys_r');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Convolution Architecture - Downsampling/Upsampling  ##################\n",
    "\n",
    "def init_filters(layers, init_n_f ):\n",
    "    filters = []\n",
    "    bias = []\n",
    "    f_dc = []\n",
    "    trimf = 0.01\n",
    "    trimb = 0.05\n",
    "    n_f = init_n_f #initial number of filters/kernels\n",
    "    ch_in = 1      #input channels\n",
    "    for i in range(layers):\n",
    "        if(i != 0):\n",
    "            n_f = n_f*2 #16,32,64,128,256\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trimb\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "    \n",
    "    #Deconvolution filters    \n",
    "    for i in range(1,layers):\n",
    "        n_f = n_f//2 #128,64,32,16\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        fdc = np.random.randn(n_f,ch_in,2,2)*trimf #upsampling filter, its result will be conc with conv4 output so the channels will be doubled again\n",
    "        bdc = np.random.randn(fdc.shape[0],1)* trimb\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trimb\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        dc = [fdc, bdc]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "        f_dc.append(dc)\n",
    "        \n",
    "    \n",
    "    return filters, bias, f_dc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation Functions ###\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UTILITIES #############\n",
    "\n",
    "\n",
    "def conv(image, params, s = 1, pad = 1 ): # s = 1 (conv stride)\n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    [f, b] = params\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((image.shape[0],image.shape[1]+2*pad ,image.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = image\n",
    "        image = tmp    \n",
    "        \n",
    "    f_num = f.shape[0]\n",
    "    f_depth = f.shape[1] #f1 = 1 , f2 = 8\n",
    "    f_size = f.shape[2] \n",
    "        \n",
    "\n",
    "    h_range = int((image.shape[1] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    np_o = np.zeros((f_num, h_range, w_range))\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                np_o[z, _h, _w] = np.sum(np.multiply(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] , f[z, :, :, :])) + b[z]\n",
    "    \n",
    "    return np_o\n",
    "\n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s = 1, pad = 1 ):\n",
    "    f_num, n_c, f, _ = filt.shape\n",
    "    \n",
    "    _ ,h , w = dconv_prev.shape\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((conv_in.shape[0],conv_in.shape[1]+2*pad ,conv_in.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = conv_in\n",
    "        conv_in = tmp\n",
    "    \n",
    "    dconv_in = np.zeros(conv_in.shape)\n",
    "    db = np.zeros((f_num,1))\n",
    "\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                #each entry of the dconv_prev will try to affect the idxs from which was made of.\n",
    "                dfilt[z] += dconv_prev[z, _h, _w] * conv_in[:, _h*s : _h*s + f, _w*s : _w*s + f]\n",
    "                dconv_in[:, _h*s : _h*s + f, _w*s : _w*s + f] += dconv_prev[z, _h, _w] * filt[z]  \n",
    "        db[z] = np.sum(dconv_prev[z])  #, axis =1) ## AXIS?\n",
    "    \n",
    "    if(pad >=1 ):\n",
    "        dconv_in = dconv_in[:, pad:-pad, pad:-pad]  # Cropping\n",
    "        \n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "\n",
    "def convTransp(image, params, s = 1, pad = 1):\n",
    "    #s is always 1, upsample kernel = 2\n",
    "    #zero insertions between pixels s_downsampled -1 = 2-1 = 1\n",
    "    #required padding in order to double my dimensions with the given data:\n",
    "    #(i-1)*2 + k -2p = output size, where our padding is k - p -1 = 2-0-1=1(we assume p=0)\n",
    "    input_s = image.shape[1]\n",
    "    target_dim = input_s*2 #final dim, after conv\n",
    "    required_pad = 1 #always for filter 2x2 ,stride 1, zero insertion 1 and main target to double dim\n",
    "    #make our new custom input\n",
    "    size = input_s*2 +1\n",
    "    new_in = np.zeros((image.shape[0], size, size))\n",
    "    for i in range(1, target_dim, 2):\n",
    "        for j in range(1, target_dim, 2):\n",
    "                new_in[:, i, j] = image[:, i//2, j//2]\n",
    "    #now we do a normal convolution(pad = 0)\n",
    "    res = conv(new_in, params, 1, 0) #thats the final result with target_dim\n",
    "    return res, new_in # we will need new_in so we wont calc it again for the backprop\n",
    "    \n",
    "def convTranspBackward(dconv_prev, new_in, filt, s = 1):\n",
    "    #First, we do a backward convolution on new_in,d_conv_prev,\n",
    "    #then we will choose form the d_conv_new the entries that match the initial 'smaller' input image\n",
    "    #by selecting the odd matrix cells 1,3,5... because we had applied a standard pad=1,zero inser=1\n",
    "    dconv_in, dfilt, db = convolutionBackward(dconv_prev, new_in, filt,1,0)\n",
    "    #Now its time to choose the right entries to build the gradients of the initial input image\n",
    "    dim = dconv_in.shape[1]\n",
    "    final_dim = (new_in.shape[1] - 2)//2 + 1 #based on dimen of image before final conv that gives the result,..\n",
    "    #e.g. for new_in 7x7 that is going to convoluted with a 2x2 kernel and give a 6x6 upsampled from 3x3 init image\n",
    "    # now from this 7x7 --> apply the formula above,we get the 3x3 dimension number\n",
    "    res = np.zeros((dconv_in.shape[0], final_dim, final_dim))\n",
    "    for i in range(1, dim, 2):\n",
    "        for j in range(1, dim, 2):\n",
    "                res[:, i//2, j//2] = dconv_in[:, i, j]\n",
    "    return res, dfilt, db\n",
    "    \n",
    "    \n",
    "def maxpool(image, f=2 , s=2):\n",
    "    \n",
    "    h_range = int((image.shape[1] - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f) / s) + 1     # (W - F + 2P) / S \n",
    "    out = np.zeros((image.shape[0], h_range, w_range))\n",
    "    \n",
    "    for z in range(image.shape[0]): # Number of channels\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                out[z, _h, _w] = np.max(image[z, _h*s : _h*s + f, _w*s : _w*s + f])\n",
    "    return out\n",
    "\n",
    "def nanargmax(arr):\n",
    "    #print(arr.shape)\n",
    "    try:\n",
    "        idx = np.nanargmax(arr)\n",
    "        #print (idx)\n",
    "    except:\n",
    "        idx = 0\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "def maxpoolBackward(dpool, conv, f=2 , s=2):\n",
    "    num_c, h, w = conv.shape\n",
    "    h = int((h - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w = int((w - f) / s) + 1     # (W - F + 2P) / S \n",
    "    \n",
    "    dout = np.zeros(conv.shape)\n",
    "    #print(conv.shape)\n",
    "    for z in range(num_c): # Number of channels\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                (a, b) = nanargmax(conv[z, _h*s : _h*s + f, _w*s : _w*s + f]) #Getting the indexes from the max value in this area\n",
    "                #put it on the new array\n",
    "                dout[z, _h + a, _w + b] = dpool[z, _h, _w]\n",
    "    \n",
    "    \n",
    "    return dout\n",
    "\n",
    "\n",
    "def reshape(img, reshape_dim):\n",
    "    pad = reshape_dim - img.shape[1]\n",
    "    if(pad == 0):\n",
    "        return img\n",
    "    res = np.zeros((img.shape[0], reshape_dim, reshape_dim))\n",
    "    if(pad > 1):\n",
    "        res[:, pad//2:-(pad//2), pad//2:-(pad//2)] = img\n",
    "    else:\n",
    "        res[:, 0:-(pad), 0:-(pad)] = img\n",
    "    return res\n",
    "\n",
    "\n",
    "def crop(img, crop_dim): #TODO : look at it..future upgrade to downsample..maybe\n",
    "    start = img.shape[1]//2 - crop_dim//2\n",
    "    return img[:,(start):(start +crop_dim),(start):(start +crop_dim)]\n",
    "\n",
    "    \n",
    "    \n",
    "def crop2half(img):\n",
    "    #return gradients for decoder side and gradients for encoder side\n",
    "    n_ch = img.shape[0]//2\n",
    "    return img[n_ch:n_ch*2 ,:,:], img[0:n_ch,:,:]\n",
    "    \n",
    "def concat(img1_true, img2):\n",
    "    n_ch = img1_true.shape[0]\n",
    "    dim = img1_true.shape[1]\n",
    "    if(img2.shape[1] != dim):\n",
    "        img2 = crop(img2, dim)\n",
    "        print(\"Warning: Extra crop needed and handled!(%d --> %d)\" %(dim, img2.shape[1]))\n",
    "    res = np.zeros((n_ch*2, dim, dim))\n",
    "    res[0:n_ch,:,:] = img2\n",
    "    res[n_ch:n_ch*2 ,:,:] = img1_true\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cost Calculations ######\n",
    "def NLLLoss(logs, targets):\n",
    "    out = np.zeros(logs.shape)\n",
    "    mylen = logs.shape[0]*logs.shape[1]*logs.shape[1]\n",
    "    #print(len(targets))\n",
    "    #logs.shape (dim x dim) like target\n",
    "    for i in range(logs.shape[1]):\n",
    "        for j in range(logs.shape[2]):\n",
    "            if(targets[:,i,j] == 1):\n",
    "                out[:,i,j] = logs[:,i,j] #in that case the propab. is correct with targen being the 1\n",
    "            else:\n",
    "                out[:,i,j] = 1 - logs[:,i,j] # e.g if logs[i,j]= 0.4 and we want(target) 0 --> 1-0.4=0.6 prob. for zero \n",
    "    #or\n",
    "    #out = targets*logs + ((-1*(targets-1)) - (-1*(targets-1))*logs)\n",
    "    return -np.log(out.sum()/mylen)\n",
    "\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    \n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost) ### ATTENTION!\n",
    "\n",
    "def normalize(output):\n",
    "    output[output<-10]=-4\n",
    "    output[output>10] = 4\n",
    "    return output\n",
    "\n",
    "###### Accuracy Calculation ######\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  TRAIN  ######\n",
    "#import torch`\n",
    "import time\n",
    "time.time()\n",
    "\n",
    "\n",
    "def train(X, Y, epochs, learning_rate, dropout, verbose=True, callback=None):\n",
    "    # initiation of neural net parameters\n",
    "    #params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    #f1,f2,f3,f4,f5,fd1,fd2,fd3,fd4\n",
    "    filters,bias, f_dc = init_filters(2,16) #Double the channel-filter 4 times  (up to 256 and back again)\n",
    "    ##Final 1x1 filter\n",
    "    trimf = 0.1\n",
    "    trimb = 0.1\n",
    "    out_f = np.random.randn(1,16,1,1)*trimf\n",
    "    out_b = np.random.randn(out_f.shape[0],1)*trimb  \n",
    "    out_fb = [out_f, out_b]\n",
    "    #filter shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "\n",
    "    v_adam =  [] #filter1,filter2\n",
    "    s_adam =  [] \n",
    "    bv_adam = [] #bias1,bias2\n",
    "    bs_adam = []\n",
    "    fdc_v_adam=[] #filter,bias\n",
    "    fdc_s_adam=[]\n",
    "    #format: [v1,v2,v3,v4,v5,vd1,vd2,vd3,vd4] ,same for the rest, each of these include a tuple for the 2 conv filter\n",
    "    #[s1,s2,s3,s4,s5,sd1,sd2,vs3,sd4]\n",
    "    # upsampling filters : [v1_dc,v2_dc,v3_dc]  \n",
    "\n",
    "            \n",
    "\n",
    "    # performing calculations for subsequent iterations\n",
    "    \n",
    "    \n",
    "    [f1,f2,f3] = filters\n",
    "    [b1,b2,b3]= bias \n",
    "    \n",
    "    f1_dc = f_dc[0][0]\n",
    "    b1_dc = f_dc[0][1]\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # step forward\n",
    "        running_loss = 0\n",
    "        accuracy = 0\n",
    "        batch = 1\n",
    "        print(\"Epoch: {%d}\" %(e+1))\n",
    "        for c in range(0, X.shape[0], batch):\n",
    "            print('Batch: {}'.format(int(c/batch +1)))\n",
    "            #### Reset Gradients (Every batch) ####\n",
    "            beta1= 0.95\n",
    "            beta2= 0.99\n",
    "            lr = learning_rate\n",
    "            batch_size= batch\n",
    "            if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "                batch = X.shape[0] - c\n",
    "            X_t = X[c:(c + batch)]  # shape:(m, ch, h, w)\n",
    "            Y_t = Y[c:(c + batch)]\n",
    "            cost = 0\n",
    "            accuracy = 0\n",
    "            \n",
    "            df =  []\n",
    "            db =  []\n",
    "            dfb=  []\n",
    "            for i in filters:\n",
    "                v1 = np.zeros(i[0].shape)\n",
    "                v2 = np.zeros(i[1].shape)\n",
    "                s1 = np.zeros(i[0].shape)\n",
    "                s2 = np.zeros(i[1].shape)\n",
    "                v_a = [v1, v2]\n",
    "                s_a = [s1, s2]\n",
    "                v_adam.append(v_a)\n",
    "                s_adam.append(s_a)\n",
    "\n",
    "                df1_t = np.zeros(i[0].shape)\n",
    "                df2_t = np.zeros(i[1].shape)\n",
    "                f_temp = [df1_t, df2_t]\n",
    "                df.append(f_temp)\n",
    "\n",
    "            for i in bias:\n",
    "                bv1 = np.zeros(i[0].shape)\n",
    "                bv2 = np.zeros(i[1].shape)\n",
    "                bs1 = np.zeros(i[0].shape)\n",
    "                bs2 = np.zeros(i[1].shape)    \n",
    "                bv_a = [bv1, bv2]\n",
    "                bs_a = [bs1, bs2]\n",
    "                bv_adam.append(bv_a)\n",
    "                bs_adam.append(bs_a)\n",
    "\n",
    "\n",
    "                db1_t = np.zeros(i[0].shape)\n",
    "                db2_t = np.zeros(i[1].shape)\n",
    "                b_temp = [db1_t, db2_t]\n",
    "                db.append(b_temp)\n",
    "\n",
    "            for i in f_dc:\n",
    "                fdc_v1 = np.zeros(i[0].shape)\n",
    "                bdc_v2 = np.zeros(i[1].shape)\n",
    "                fdc_s1 = np.zeros(i[0].shape)\n",
    "                bdc_s2 = np.zeros(i[1].shape)    \n",
    "                fdc_v_a = [fdc_v1, bdc_v2]\n",
    "                fdc_s_a = [fdc_s1, bdc_s2]\n",
    "                fdc_v_adam.append(fdc_v_a)\n",
    "                fdc_s_adam.append(fdc_s_a)\n",
    "\n",
    "\n",
    "                df1_t = np.zeros(i[0].shape)\n",
    "                db1_t = np.zeros(i[1].shape)\n",
    "                fb_temp = [df1_t, db1_t]\n",
    "                dfb.append(fb_temp)\n",
    "\n",
    "\n",
    "            #Final layer 1x1 filter setup\n",
    "\n",
    "            v_out_f = np.zeros(out_f.shape)\n",
    "            s_out_f = np.zeros(out_f.shape)\n",
    "            bv_out_b = np.zeros(out_b.shape)\n",
    "            bs_out_b = np.zeros(out_b.shape)\n",
    "\n",
    "\n",
    "\n",
    "            dout_f = np.zeros(out_f.shape)\n",
    "            dout_b = np.zeros(out_b.shape)\n",
    "\n",
    "            ######################################\n",
    "\n",
    "\n",
    "            #timestamp1 = time.time()\n",
    "\n",
    "\n",
    "            [df1,df2,df3] = df\n",
    "            [db1,db2,db3] = db \n",
    "            dfb1_dc     = dfb\n",
    "            \n",
    "            \n",
    "            for b in range(batch):\n",
    "                \n",
    "                print('Image: {}/{}'.format((b+1),batch))\n",
    "                #################### TODO: BLOCK IMPLEMENTATION - FUTURE UPDATE ######################\n",
    "                    \n",
    "                    \n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ######################## Forward Propagation Convolution Part  ##########################\n",
    "\n",
    "                \n",
    "                ########### 1st Big Layer ###########    (with zero padding ='same',so with stride =1 we get same dim as the input)\n",
    "                params = [f1[0], b1[0]]  \n",
    "                conv1_1 = conv(X_t[b], params, 1)   #conv1 shape = (num_channels, h, w), padding = 1 (same output dim)\n",
    "                conv1_1[conv1_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f1[1], b1[1]]\n",
    "                conv1_2 = conv(conv1_1, params, 1)\n",
    "                conv1_2[conv1_2<=0] = 0 #Relu\n",
    "                ##################################### conv1_2: 32x32x16\n",
    "                \n",
    "                pl1 = maxpool(conv1_2, 2, 2) #   pl1 : (32-2)/2+1  = 16 \n",
    "                ## ADD DROPOUT HERE(on pl1)\n",
    "                \n",
    "                ########### 2nd Big Layer ###########\n",
    "                params = [f2[0], b2[0]]  \n",
    "                conv2_1 = conv(pl1, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv2_1[conv2_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f2[1], b2[1]]\n",
    "                conv2_2 = conv(conv2_1, params, 1)\n",
    "                conv2_2[conv2_2<=0] = 0 #Relu             \n",
    "                #####################################  16x16x32\n",
    "\n",
    "          \n",
    "                ##################################### \n",
    "                ##################################### \n",
    "                #####################################\n",
    "                #Deconvolution/Upsampling\n",
    "                # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "               \n",
    "                params = [f_dc[0][0], f_dc[0][1]] # deconv filter, deconv bias\n",
    "                dc1, new_in1 = convTransp(conv2_2, params, 1, 0)   #result:   =  32x32x16 , # conv5_2 requires NO crop\n",
    "                #Concat dc6 with conv2_2 so we get 32 channels (32x32x32)\n",
    "                c1 = concat(dc1, conv1_2) # 1st one is the right one size  \n",
    "                \n",
    "                ########### 1st Big dc Layer ###########          32x32x32     \n",
    "                params = [f3[0], b3[0]]  \n",
    "                conv3_1 = conv(c1, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv3_1[conv3_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f3[1], b3[1]]\n",
    "                conv3_2 = conv(conv3_1, params, 1)\n",
    "                conv3_2[conv3_2<=0] = 0 #Relu   \n",
    "                #####################################    32x32x16\n",
    "                \n",
    "                ############################# Last Layer conv(1x1) --> 128x128x1 ##########################\n",
    "                params = [out_f, out_b]\n",
    "                output = conv(conv3_2, params, 1, 0) #output.shape: 32x32x1\n",
    "                \n",
    "                \n",
    "                output = normalize(output)\n",
    "                ## Sigmoid ##\n",
    "                Y_hat = sigmoid(output)\n",
    "                print(Y_hat[:,0:10,0:10])\n",
    "                #label crop is needed\n",
    "                #Y_t_b = crop(Y_t[b], Y_hat.shape[1])\n",
    "                plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "                cost += NLLLoss(Y_hat, Y_t[b])\n",
    "                print(cost/(b+1))\n",
    "                \n",
    "                accuracy += get_accuracy_value(Y_hat, Y_t[b])\n",
    "                print(accuracy/(b+1))\n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ################################# Backward Propagation ##################################\n",
    "                \n",
    "                \n",
    "                #Pixel-wise sub, we we can get the diff(Y includes the 2 classes 0 and 1)\n",
    "                \n",
    "                dA_prev = Y_hat - Y_t[b]\n",
    "                dZ_prev = sigmoid_backward(dA_prev, output)\n",
    "                conv_s =1 \n",
    "                dconv3_2, dout_f_, dout_b_ = convolutionBackward(dZ_prev, conv3_2, out_f, conv_s) #\n",
    "                #pack data\n",
    "                \n",
    "                \n",
    "                dconv3_2[conv3_2<=0] = 0             \n",
    "                dconv3_1, df3_2, db3_2 = convolutionBackward(dconv3_2, conv3_1, f3[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv3_1[conv3_1<=0] = 0\n",
    "                conc_dconv3, df3_1, db3_1 = convolutionBackward(dconv3_1, c1, f3[0], conv_s) #\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv3, dconv1_2 = crop2half(conc_dconv3)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #conv8_2 is not needed for input,we know how to select the right gradients            \n",
    "                #dconv1_2 = reshape(dconv1_2, conv1_2.shape[1])\n",
    "                dconv2_2, df1_dc, db1_dc = convTranspBackward(dconv3, new_in1, f_dc[0][0], conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                dconv2_2[conv2_2<=0] = 0\n",
    "                dconv2_1, df2_2, db2_2 = convolutionBackward(dconv2_2, conv2_1, f2[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv2_1[conv2_1<=0] = 0\n",
    "                dpl1, df2_1, db2_1 = convolutionBackward(dconv2_1, pl1, f2[0], conv_s) #\n",
    "                \n",
    "                dconv1_2 += maxpoolBackward(dpl1, conv1_2, f=2 , s=2)\n",
    "                \n",
    "                dconv1_2[conv1_2<=0] = 0\n",
    "                dconv1_1, df1_2, db1_2 = convolutionBackward(dconv1_2, conv1_1, f1[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv1_1[conv1_1<=0] = 0\n",
    "                _, df1_1, db1_1 = convolutionBackward(dconv1_1, X_t[b], f1[0], conv_s) #\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                [df1,df2,df3] = df\n",
    "                [db1,db2,db3] = db \n",
    "                dfb1_dc     = dfb\n",
    "\n",
    "                \n",
    "                df1[0] += df1_1\n",
    "                df1[1] += df1_2\n",
    "                df2[0] += df2_1\n",
    "                df2[1] += df2_2\n",
    "                df3[0] += df3_1\n",
    "                df3[1] += df3_2\n",
    "                \n",
    "                \n",
    "                db1[0] += db1_1\n",
    "                db1[1] += db1_2\n",
    "                db2[0] += db2_1\n",
    "                db2[1] += db2_2\n",
    "                db3[0] += db3_1\n",
    "                db3[1] += db3_2\n",
    "                \n",
    "\n",
    "                #dfb1_dc[0] += df1_dc\n",
    "                #dfb1_dc[1] += db1_dc\n",
    "\n",
    "\n",
    "                dout_f += dout_f_\n",
    "                dout_b += dout_b_\n",
    "\n",
    "\n",
    "      \n",
    "            \n",
    "            \n",
    "            ############## Adam Optimization ################\n",
    "            #changing the main structures(which are also updated)\n",
    "            #TODO: Future update - remove uneccessary memory loads/stores, v,s dont need to be saved \n",
    "            for i in range(len(filters)):\n",
    "                v_adam[i][0] = beta1*v_adam[i][0] + (1-beta1)*df[i][0]/batch_size #f1\n",
    "                s_adam[i][0] = beta2*s_adam[i][0] + (1-beta2)*(df[i][0]/batch_size)**2 #f1\n",
    "                filters[i][0] -= lr*v_adam[i][0]/np.sqrt(s_adam[i][0] + 1e-7)\n",
    "                \n",
    "                v_adam[i][1] = beta1*v_adam[i][1] + (1-beta1)*df[i][1]/batch_size #f2\n",
    "                s_adam[i][1] = beta2*s_adam[i][1] + (1-beta2)*(df[i][1]/batch_size)**2  #f2\n",
    "                filters[i][1] -= lr*v_adam[i][1]/np.sqrt(s_adam[i][1] + 1e-7)\n",
    "                \n",
    "            for i in range(len(bias)):\n",
    "                bv_adam[i][0] = beta1*bv_adam[i][0] + (1-beta1)*db[i][0]/batch_size #b1\n",
    "                bs_adam[i][0] = beta2*bs_adam[i][0] + (1-beta2)*(db[i][0]/batch_size)**2  #b1\n",
    "                bias[i][0] -= lr*bv_adam[i][0]/np.sqrt(bs_adam[i][0] + 1e-7)\n",
    "                \n",
    "                bv_adam[i][1] = beta1*bv_adam[i][1] + (1-beta1)*db[i][1]/batch_size #b2\n",
    "                bs_adam[i][1] = beta2*bs_adam[i][1] + (1-beta2)*(db[i][1]/batch_size)**2  #b2\n",
    "                bias[i][1] -= lr*bv_adam[i][1]/np.sqrt(bs_adam[i][1] + 1e-7)\n",
    "            \n",
    "            for i in range(len(f_dc)):\n",
    "                fdc_v_adam[i][0] = beta1*fdc_v_adam[i][0] + (1-beta1)*dfb[i][0]/batch_size #f1\n",
    "                fdc_s_adam[i][0] = beta2*fdc_s_adam[i][0] + (1-beta2)*(dfb[i][0]/batch_size)**2  #f1\n",
    "                f_dc[i][0] -= lr*fdc_v_adam[i][0]/np.sqrt(fdc_s_adam[i][0] + 1e-7)\n",
    "                \n",
    "                fdc_v_adam[i][1] = beta1*fdc_v_adam[i][1] + (1-beta1)*dfb[i][1]/batch_size #b2\n",
    "                fdc_s_adam[i][1] = beta2*fdc_s_adam[i][1] + (1-beta2)*(dfb[i][1]/batch_size)**2  #b2\n",
    "                f_dc[i][1] -= lr*fdc_v_adam[i][1]/np.sqrt(fdc_s_adam[i][1] + 1e-7)    \n",
    "            \n",
    "            v_out_f = beta1*v_out_f + (1 - beta1)*dout_f/batch_size #f\n",
    "            s_out_f = beta2*s_out_f + (1 - beta2)*(dout_f/batch_size)**2  #f\n",
    "            out_fb[0] -= lr*v_out_f/np.sqrt(s_out_f + 1e-7)\n",
    "            \n",
    "            bv_out_b = beta1*bv_out_b + (1 - beta1)*dout_b/batch_size #b\n",
    "            bs_out_b = beta2*bs_out_b + (1 - beta2)*(dout_b/batch_size)**2  #b\n",
    "            out_fb[1] -= lr*bv_out_b/np.sqrt(bs_out_b + 1e-7)\n",
    "            \n",
    "            '''\n",
    "                        for i in range(len(filters)):\n",
    "                filters[i][0] -= lr*df[i][0]\n",
    "                bias[i][0] -= lr*db[i][0]\n",
    "            \n",
    "            \n",
    "            f_dc[0][0] -= lr*df1_dc\n",
    "            f_dc[0][1] -= lr*db1_dc\n",
    "            \n",
    "            out_fb[0] -= lr*dout_f\n",
    "            out_fb[1] -= lr*dout_b\n",
    "            #Updating Conv Part\n",
    "            \n",
    "\n",
    "            #print(Yh)\n",
    "            #print(t)\n",
    "            #print(\"Batch:{}\".format(c+12))\n",
    "            \n",
    "           \n",
    "            f1 -= (learning_rate) * (df1/batch)\n",
    "            f2 -= (learning_rate) * (df2/batch)\n",
    "            b1 -= (learning_rate) * (db1/batch)\n",
    "            b2 -= (learning_rate) * (db2/batch)\n",
    "            filter_params = [f1, f2, b1, b2]\n",
    "                \n",
    "            # updating FC Part\n",
    "            #params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "            grads_values['dW1'] = dW1#/(batch)\n",
    "            grads_values['dW2'] = dW2#/(batch)\n",
    "            grads_values['db1'] = dB1#/(batch)\n",
    "            grads_values['db2'] = dB2#/(batch)\n",
    "                   \n",
    "            for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "                \n",
    "                params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "                params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "            '''\n",
    "            print(\"Cost : {:.5f}\".format(cost/batch))\n",
    "            print(\"Accuracy : {:.5f}%\".format((accuracy*100)/batch))\n",
    "            \n",
    "        #END OF LOOP - EPOCH\n",
    "        #timestamp2 = time.time()\n",
    "        #print (\"This took %.2f seconds\" %(timestamp2 - timestamp1))\n",
    "        if(verbose):\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/batch, (accuracy*100)/batch))\n",
    "        #if(callback is not None):\n",
    "        #    callback(i, params_values)\n",
    "    #pack filters\n",
    "    params_values = [filters, bias, f_dc, out_fb]\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: {1}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.51326518 0.51343363 0.5134375  0.51343674 0.51343704 0.51343676\n",
      "   0.51343704 0.51343676 0.51343704 0.51343676]\n",
      "  [0.51308054 0.51327282 0.51327391 0.51327158 0.51327335 0.51327164\n",
      "   0.51327335 0.51327164 0.51327335 0.51327164]\n",
      "  [0.51306612 0.5132632  0.51326487 0.5132641  0.51326427 0.51326414\n",
      "   0.51326427 0.51326414 0.51326428 0.51326414]\n",
      "  [0.5130666  0.51326289 0.51326571 0.51326389 0.51326508 0.51326394\n",
      "   0.51326508 0.51326395 0.51326509 0.51326397]\n",
      "  [0.51306686 0.51326412 0.51326604 0.51326534 0.51326539 0.51326537\n",
      "   0.51326541 0.51326546 0.51326552 0.51326558]\n",
      "  [0.51306658 0.5132629  0.51326565 0.51326391 0.51326501 0.51326398\n",
      "   0.51326508 0.51326417 0.51326536 0.51326447]\n",
      "  [0.51306686 0.51326412 0.51326604 0.51326534 0.5132654  0.51326541\n",
      "   0.51326551 0.51326573 0.51326615 0.51326634]\n",
      "  [0.51306658 0.5132629  0.51326565 0.51326391 0.51326502 0.51326404\n",
      "   0.51326515 0.5132644  0.51326601 0.51326505]\n",
      "  [0.51306686 0.51326412 0.51326604 0.51326534 0.5132654  0.51326539\n",
      "   0.51326533 0.51326554 0.51326614 0.51326593]\n",
      "  [0.51306658 0.5132629  0.51326565 0.51326391 0.513265   0.5132639\n",
      "   0.51326469 0.51326368 0.51326501 0.51326323]]]\n",
      "0.720026588074444\n",
      "0.0\n",
      "Cost : 0.72003\n",
      "Accuracy : 0.00000%\n",
      "Epoch:     1   -   cost: 0.72   -   Accuracy: 0.00%\n",
      "Epoch: {2}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.50898551 0.50815703 0.50801999 0.50801958 0.50801916 0.50801955\n",
      "   0.50801916 0.50801955 0.50801916 0.50801955]\n",
      "  [0.50783075 0.50669703 0.50648987 0.50648818 0.50648931 0.50648819\n",
      "   0.50648931 0.50648818 0.50648931 0.50648818]\n",
      "  [0.50769872 0.50652682 0.50630339 0.50630385 0.50630337 0.50630383\n",
      "   0.50630338 0.50630383 0.50630338 0.50630384]\n",
      "  [0.50769936 0.50652645 0.50630503 0.50630482 0.50630518 0.50630481\n",
      "   0.5063052  0.50630485 0.50630524 0.50630489]\n",
      "  [0.50770136 0.5065302  0.50630778 0.5063085  0.50630799 0.5063085\n",
      "   0.50630814 0.50630871 0.50630821 0.50630852]\n",
      "  [0.50769934 0.50652643 0.50630494 0.50630478 0.50630508 0.50630486\n",
      "   0.50630536 0.50630507 0.50630499 0.50630393]\n",
      "  [0.50770135 0.50653019 0.50630776 0.50630849 0.50630801 0.50630857\n",
      "   0.50630817 0.50630816 0.50630653 0.50630542]\n",
      "  [0.50769933 0.50652642 0.50630493 0.50630478 0.50630508 0.50630484\n",
      "   0.50630482 0.50630332 0.50630151 0.50629883]\n",
      "  [0.50770135 0.50653019 0.50630776 0.50630849 0.50630798 0.50630837\n",
      "   0.50630712 0.50630587 0.50630292 0.50630079]\n",
      "  [0.50769933 0.50652643 0.50630493 0.50630478 0.50630503 0.50630455\n",
      "   0.50630393 0.50630213 0.50630034 0.5062976 ]]]\n",
      "0.7062676003973503\n",
      "0.0\n",
      "Cost : 0.70627\n",
      "Accuracy : 0.00000%\n",
      "Epoch:     2   -   cost: 0.71   -   Accuracy: 0.00%\n",
      "Epoch: {3}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.50206313 0.49894517 0.49826881 0.49824271 0.49824408 0.49824256\n",
      "   0.49824409 0.49824256 0.49824409 0.49824256]\n",
      "  [0.49855583 0.49372131 0.49267227 0.49262992 0.49263368 0.49262969\n",
      "   0.49263368 0.49262969 0.49263368 0.49262969]\n",
      "  [0.49785811 0.49267864 0.4915116  0.49146605 0.49146866 0.49146576\n",
      "   0.49146865 0.49146576 0.49146862 0.49146569]\n",
      "  [0.4978373  0.49264547 0.49147652 0.49142943 0.49143286 0.49142911\n",
      "   0.49143282 0.49142904 0.49143256 0.49142848]\n",
      "  [0.49784084 0.49265164 0.49148149 0.49143517 0.49143788 0.49143481\n",
      "   0.49143782 0.49143428 0.49143606 0.49143106]\n",
      "  [0.49783705 0.49264496 0.49147589 0.4914287  0.49143217 0.49142848\n",
      "   0.49143213 0.49142692 0.49142735 0.4914189 ]\n",
      "  [0.49784082 0.49265159 0.49148144 0.4914351  0.49143789 0.49143504\n",
      "   0.49143778 0.49143225 0.49143041 0.49142163]\n",
      "  [0.49783704 0.49264495 0.49147588 0.49142869 0.49143225 0.49142901\n",
      "   0.49143282 0.49142758 0.49142853 0.49142268]\n",
      "  [0.49784082 0.49265159 0.49148144 0.4914351  0.49143798 0.49143585\n",
      "   0.4914403  0.49143958 0.49144624 0.49144895]\n",
      "  [0.49783704 0.49264495 0.49147588 0.49142869 0.49143232 0.49142993\n",
      "   0.49143725 0.49144089 0.49145569 0.49146458]]]\n",
      "0.6780620620194853\n",
      "0.7021484375\n",
      "Cost : 0.67806\n",
      "Accuracy : 70.21484%\n",
      "Epoch:     3   -   cost: 0.68   -   Accuracy: 70.21%\n",
      "Epoch: {4}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.48948331 0.48148122 0.47938348 0.47922521 0.47922989 0.47922323\n",
      "   0.47922985 0.47922323 0.47922985 0.47922323]\n",
      "  [0.48123398 0.46853162 0.46516075 0.46489561 0.46490581 0.46489187\n",
      "   0.46490569 0.46489184 0.46490569 0.46489184]\n",
      "  [0.47912289 0.46517819 0.46135532 0.46105    0.46105944 0.46104549\n",
      "   0.46105935 0.46104565 0.46105971 0.46104628]\n",
      "  [0.47897868 0.46493861 0.46107982 0.46076082 0.46077361 0.4607556\n",
      "   0.46077416 0.46075766 0.46077834 0.46076457]\n",
      "  [0.47898198 0.46494569 0.46108422 0.46076839 0.46077887 0.46076475\n",
      "   0.46078478 0.46077938 0.46081087 0.4608173 ]\n",
      "  [0.47897612 0.46493367 0.46107413 0.46075386 0.46076795 0.46075471\n",
      "   0.46079165 0.46081008 0.46088589 0.46093659]\n",
      "  [0.47898168 0.4649451  0.46108351 0.4607676  0.46077984 0.46077798\n",
      "   0.46083618 0.46090885 0.46105558 0.46119118]\n",
      "  [0.47897602 0.46493348 0.46107393 0.46075371 0.46077006 0.46077388\n",
      "   0.4608645  0.46099394 0.46122904 0.46144659]\n",
      "  [0.47898168 0.4649451  0.46108351 0.46076768 0.46078138 0.46079298\n",
      "   0.46089592 0.46106227 0.46133889 0.46159825]\n",
      "  [0.47897602 0.46493348 0.46107393 0.46075368 0.46076968 0.46077472\n",
      "   0.46087423 0.46102252 0.46127833 0.46150146]]]\n",
      "0.6230655678693466\n",
      "0.7060546875\n",
      "Cost : 0.62307\n",
      "Accuracy : 70.60547%\n",
      "Epoch:     4   -   cost: 0.62   -   Accuracy: 70.61%\n",
      "Epoch: {5}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.4679779  0.44838925 0.44256826 0.44193589 0.44194747 0.44191964\n",
      "   0.44194426 0.44191868 0.44194437 0.44191881]\n",
      "  [0.44807131 0.4169624  0.40767025 0.40661626 0.4066481  0.40658599\n",
      "   0.40664205 0.40658413 0.40664233 0.40658451]\n",
      "  [0.44216644 0.40780594 0.39742009 0.39623261 0.39625323 0.39619607\n",
      "   0.39624635 0.39619569 0.39625067 0.39620328]\n",
      "  [0.44157118 0.40685978 0.39632942 0.39508115 0.39511522 0.39503795\n",
      "   0.3951127  0.39505448 0.39515446 0.39512734]\n",
      "  [0.44156152 0.40685786 0.39631594 0.39507926 0.39510261 0.395048\n",
      "   0.39514626 0.39518181 0.39540569 0.39557444]\n",
      "  [0.44155449 0.40682835 0.39629208 0.39503415 0.3950762  0.39503595\n",
      "   0.39526167 0.39551738 0.39613092 0.39672768]\n",
      "  [0.4415571  0.40684975 0.39630616 0.39506771 0.39510321 0.39513751\n",
      "   0.39553526 0.39621315 0.39741639 0.39870368]\n",
      "  [0.44155322 0.40682576 0.3962891  0.39503085 0.39508725 0.39516033\n",
      "   0.39576683 0.39683091 0.39862678 0.40047116]\n",
      "  [0.4415571  0.40684975 0.39630616 0.3950681  0.39511018 0.39521472\n",
      "   0.39586191 0.39706226 0.39898222 0.40092191]\n",
      "  [0.44155322 0.40682576 0.39628906 0.39503042 0.39508024 0.39512618\n",
      "   0.39567166 0.39659046 0.39810989 0.39953057]]]\n",
      "0.5167143924682691\n",
      "0.7060546875\n",
      "Cost : 0.51671\n",
      "Accuracy : 70.60547%\n",
      "Epoch:     5   -   cost: 0.52   -   Accuracy: 70.61%\n",
      "Epoch: {6}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.4278301  0.38571402 0.37219342 0.37032468 0.37027875 0.37019632\n",
      "   0.37024198 0.37018536 0.3702442  0.37018796]\n",
      "  [0.38532919 0.32075344 0.30059433 0.29767686 0.29767666 0.29746076\n",
      "   0.29761666 0.29744255 0.29762186 0.29744929]\n",
      "  [0.37159589 0.30034265 0.27794762 0.27471294 0.27464371 0.27446176\n",
      "   0.27457836 0.2744525  0.27460648 0.27449598]\n",
      "  [0.36984888 0.29759971 0.27492022 0.27143301 0.2714656  0.27113856\n",
      "   0.27141543 0.27119945 0.27160225 0.27151826]\n",
      "  [0.36970139 0.29745409 0.27468508 0.27128174 0.27122321 0.27103772\n",
      "   0.27136264 0.27157773 0.27243878 0.27320486]\n",
      "  [0.36972804 0.29738384 0.27467606 0.27112677 0.27119856 0.2710021\n",
      "   0.27189779 0.27292895 0.27543385 0.27790022]\n",
      "  [0.36966123 0.29738529 0.27460653 0.27118898 0.271183   0.27135428\n",
      "   0.27290499 0.27575379 0.2807049  0.28624624]\n",
      "  [0.3697164  0.29736144 0.27465185 0.27109831 0.27123479 0.2715195\n",
      "   0.27404504 0.27856676 0.28632385 0.29458806]\n",
      "  [0.36966124 0.29738532 0.27460686 0.27119218 0.27122354 0.27175105\n",
      "   0.27453538 0.28001453 0.28876825 0.29821194]\n",
      "  [0.36971638 0.29736141 0.27465171 0.27109755 0.27121981 0.27149593\n",
      "   0.27408324 0.27868391 0.28634015 0.29412388]]]\n",
      "0.347922607093536\n",
      "0.7060546875\n",
      "Cost : 0.34792\n",
      "Accuracy : 70.60547%\n",
      "Epoch:     6   -   cost: 0.35   -   Accuracy: 70.61%\n",
      "Epoch: {7}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.35454172 0.2757313  0.25035503 0.24617784 0.24580812 0.24560051\n",
      "   0.24562227 0.24554852 0.24563822 0.24556717]\n",
      "  [0.27540382 0.17254351 0.14387829 0.13904939 0.1388444  0.13835074\n",
      "   0.13862725 0.1382891  0.13865372 0.13832304]\n",
      "  [0.24961431 0.14344905 0.11513266 0.11049464 0.11011947 0.10978628\n",
      "   0.10991546 0.10976195 0.11000556 0.10988691]\n",
      "  [0.24581277 0.13896867 0.11088537 0.10584599 0.10571526 0.10503486\n",
      "   0.10553799 0.1051436  0.10592684 0.10577093]\n",
      "  [0.2451199  0.13840782 0.11016189 0.10532648 0.1049732  0.10461111\n",
      "   0.10513971 0.1055607  0.10707077 0.10843894]\n",
      "  [0.24528391 0.13828537 0.11021207 0.10502428 0.10497832 0.10448681\n",
      "   0.10607955 0.10775162 0.11219421 0.11644018]\n",
      "  [0.2449282  0.13816919 0.10992569 0.1050551  0.10480788 0.1050506\n",
      "   0.10769359 0.11267472 0.12165334 0.13221023]\n",
      "  [0.24523023 0.13820937 0.110143   0.10494388 0.10503001 0.10538272\n",
      "   0.10988894 0.11802084 0.1332112  0.15062319]\n",
      "  [0.24492829 0.13816946 0.10992803 0.10506693 0.10490532 0.1058417\n",
      "   0.11091533 0.1215143  0.13995283 0.16230143]\n",
      "  [0.24523019 0.13820935 0.11014295 0.10494524 0.10502507 0.10548612\n",
      "   0.1105262  0.11997639 0.13737938 0.15752523]]]\n",
      "0.15671599797025107\n",
      "0.7060546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost : 0.15672\n",
      "Accuracy : 70.60547%\n",
      "Epoch:     7   -   cost: 0.16   -   Accuracy: 70.61%\n",
      "Epoch: {8}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.27386547 0.1693391  0.13925201 0.13404483 0.13348638 0.13322016\n",
      "   0.13321818 0.13314982 0.13324854 0.13318497]\n",
      "  [0.1693004  0.06891809 0.04808708 0.04456559 0.04436929 0.04399719\n",
      "   0.04419123 0.0439508  0.04421979 0.04398688]\n",
      "  [0.13881435 0.0478462  0.03105352 0.02839178 0.02814462 0.02794644\n",
      "   0.02802037 0.02794135 0.02809341 0.02803938]\n",
      "  [0.13415971 0.04466191 0.02865928 0.02589386 0.02579803 0.02541744\n",
      "   0.0257037  0.0255058  0.02597648 0.0259374 ]\n",
      "  [0.13316568 0.04420371 0.02820176 0.02557543 0.02536284 0.02517204\n",
      "   0.02550113 0.02581159 0.02679293 0.0277435 ]\n",
      "  [0.13344797 0.0441391  0.02825299 0.02542865 0.02539101 0.02514874\n",
      "   0.02613672 0.02729777 0.03027447 0.03337552]\n",
      "  [0.1329023  0.04401856 0.02805834 0.02542205 0.02528396 0.02549525\n",
      "   0.02723696 0.03071422 0.0374514  0.04631255]\n",
      "  [0.13337945 0.04408293 0.02821368 0.02538751 0.02544109 0.02577397\n",
      "   0.02881444 0.03493796 0.0478635  0.06550973]\n",
      "  [0.13290252 0.04401891 0.02806063 0.02543219 0.02535864 0.02607443\n",
      "   0.02968555 0.03816221 0.05570911 0.08213014]\n",
      "  [0.13337936 0.04408295 0.02821377 0.02538968 0.02544749 0.02591241\n",
      "   0.0295578  0.03749525 0.05471915 0.08004801]]]\n",
      "0.06469207252135827\n",
      "0.7060546875\n",
      "Cost : 0.06469\n",
      "Accuracy : 70.60547%\n",
      "Epoch:     8   -   cost: 0.06   -   Accuracy: 70.61%\n",
      "Epoch: {9}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.20526016 0.09683524 0.07130497 0.06714367 0.06687029 0.06668278\n",
      "   0.0667236  0.06664545 0.06674195 0.06666696]\n",
      "  [0.09730711 0.02479041 0.01444779 0.01292209 0.01287477 0.01274882\n",
      "   0.01282203 0.01273629 0.01283219 0.01274929]\n",
      "  [0.07165676 0.01441759 0.00762359 0.00669134 0.00663913 0.00658378\n",
      "   0.00661113 0.00658646 0.00663536 0.00662115]\n",
      "  [0.06789078 0.01302417 0.00677598 0.00586797 0.00585495 0.0057606\n",
      "   0.00584272 0.00580438 0.00595348 0.00598695]\n",
      "  [0.06734958 0.01288269 0.00666495 0.00579533 0.00575645 0.00571818\n",
      "   0.00584497 0.00600699 0.00641552 0.00687855]\n",
      "  [0.06751333 0.01287001 0.00668173 0.00576512 0.00577365 0.00575508\n",
      "   0.00614451 0.0067507  0.00810822 0.00978747]\n",
      "  [0.06721172 0.01282876 0.00663245 0.00576349 0.00575617 0.00590918\n",
      "   0.00669695 0.00846936 0.01214417 0.01781392]\n",
      "  [0.06748087 0.01285486 0.00667401 0.00575908 0.00580794 0.00606802\n",
      "   0.00748062 0.01093286 0.0192842  0.03390324]\n",
      "  [0.06721192 0.01282893 0.0066333  0.00576744 0.00579049 0.00619769\n",
      "   0.00798861 0.013051   0.0262037  0.05237995]\n",
      "  [0.06748084 0.01285488 0.00667412 0.00576004 0.00581285 0.00615269\n",
      "   0.00795153 0.01286897 0.02617192 0.05294933]]]\n",
      "0.035505913550039105\n",
      "0.7060546875\n",
      "Cost : 0.03551\n",
      "Accuracy : 70.60547%\n",
      "Epoch:     9   -   cost: 0.04   -   Accuracy: 70.61%\n",
      "Epoch: {10}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "[[[0.16936899 0.06855003 0.04755941 0.04439536 0.04430945 0.04423954\n",
      "   0.04426233 0.0442295  0.0442694  0.04423779]\n",
      "  [0.06923774 0.01370985 0.00729972 0.00646029 0.00645326 0.00641641\n",
      "   0.00644078 0.0064141  0.00644383 0.00641807]\n",
      "  [0.04831782 0.00734621 0.00351809 0.00305193 0.00304108 0.00302763\n",
      "   0.00303594 0.00303091 0.00304657 0.00304805]\n",
      "  [0.04539535 0.00656806 0.00308894 0.002653   0.00265351 0.00263035\n",
      "   0.00265838 0.00266118 0.00272837 0.00278253]\n",
      "  [0.04518153 0.00652851 0.00306013 0.00263589 0.00262986 0.00263236\n",
      "   0.00270328 0.00283276 0.00309201 0.00344041]\n",
      "  [0.04527722 0.00653103 0.00306912 0.0026314  0.00264326 0.00267989\n",
      "   0.00291901 0.00339475 0.00434764 0.00572098]\n",
      "  [0.04513788 0.00651566 0.00305338 0.00263018 0.00264436 0.0027875\n",
      "   0.00333833 0.00472445 0.0077295  0.01298129]\n",
      "  [0.04527    0.00652803 0.00306802 0.00263175 0.0026708  0.00291565\n",
      "   0.00393308 0.00687793 0.01473204 0.03089211]\n",
      "  [0.045138   0.00651573 0.00305366 0.00263211 0.00266775 0.0030046\n",
      "   0.00436197 0.00890543 0.02276838 0.05521919]\n",
      "  [0.04526998 0.00652804 0.00306806 0.00263217 0.00267445 0.00298507\n",
      "   0.00433919 0.00886043 0.02322574 0.05804592]]]\n",
      "0.033348853183633946\n",
      "0.7060546875\n",
      "Cost : 0.03335\n",
      "Accuracy : 70.60547%\n",
      "Epoch:    10   -   cost: 0.03   -   Accuracy: 70.61%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### Training ######\n",
    "#train_images, train_labels, test_images, test_labels\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "params_values = train(train_images, train_labels, 10, 0.01, True) #0.05 stable LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-d1175c062e01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m###### Prediction ######\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mYt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtemp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_labels' is not defined"
     ]
    }
   ],
   "source": [
    "###### Prediction ######\n",
    "Yt = test_labels.T\n",
    "temp1 = []\n",
    "for i in range(Yt.shape[1]):\n",
    "        for j in range(Yt.shape[0]):\n",
    "            if(Yt[j][i]==1):\n",
    "                temp1.append(j)\n",
    "Yt=np.array(temp1)\n",
    "Y_test_hat, _ = full_forward_propagation(np.transpose(test_images), params_values, NN_ARCHITECTURE)#multiple?!\n",
    "\n",
    "Yht = np.array(Y_test_hat.T)\n",
    "#x_log=np.log( np.exp(Yh) / np.sum(np.exp(Yh), axis = 1) )  #(60000,10) , we need to add along columns so we get sum of 1 on every example-row\n",
    "num = np.exp(Yht)\n",
    "den = np.sum(np.exp(Yht), axis = 1)\n",
    "for i in range(Yht.shape[0]): #60000\n",
    "                #for j in range(Yh.shape[1]): #10\n",
    "                Yht[i][:] = np.log(num[i][:] / den[i])  \n",
    "\n",
    "#cost = get_cost_value(Yht, Yt)\n",
    "\n",
    "#cost_history.append(cost)\n",
    "accuracy = get_accuracy_value(Y_test_hat, test_labels.T)\n",
    "#accuracy_history.append(accuracy)\n",
    "print(\"Accuracy: {:.5f}%\".format( accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
