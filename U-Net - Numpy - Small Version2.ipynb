{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "#import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "from matplotlib import image\n",
    "\n",
    "def mnist(path=None):\n",
    "    r\"\"\"Return (train_images, train_labels, test_images, test_labels).\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory containing MNIST. Default is\n",
    "            /home/USER/data/mnist or C:\\Users\\USER\\data\\mnist.\n",
    "            Create if nonexistant. Download any missing files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, train_labels, test_images, test_labels), each\n",
    "            a matrix. Rows are examples. Columns of images are pixel values.\n",
    "            Columns of labels are a onehot encoding of the correct class.\n",
    "\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    files = ['train-images-idx3-ubyte.gz',\n",
    "             'train-labels-idx1-ubyte.gz',\n",
    "             't10k-images-idx3-ubyte.gz',\n",
    "             't10k-labels-idx1-ubyte.gz']\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        # Set path to /home/USER/data/mnist or C:\\Users\\USER\\data\\salt\n",
    "        path = os.path.join(os.path.expanduser('~/'), 'data', 'salt')\n",
    "\n",
    "    # Create path if it doesn't exist\n",
    "    #os.makedirs(path, exist_ok=True)\n",
    "    \"\"\"\"\n",
    "    # Download any missing files\n",
    "    for file in files:\n",
    "        if file not in os.listdir(path):\n",
    "            urlretrieve(url + file, os.path.join(path, file))\n",
    "            print(\"Downloaded %s to %s\" % (file, path))\n",
    "    print(\"All files are ready to gzip!\")\n",
    "    \"\"\"\n",
    "    def _images(path):\n",
    "        \"\"\"Return images loaded locally.\"\"\"\n",
    "        folder = path + \"/images/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(32, 32)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,32,32).astype('float32')#/255\n",
    "        return pixels[:1,:,:,:]\n",
    "\n",
    "    def _labels(path):\n",
    "        \"\"\"Return labels loaded locally.\"\"\"\n",
    "        \n",
    "        folder = path + \"/labels/\"\n",
    "        onlyfiles = [cv2.resize(cv2.cvtColor(image.imread(folder+f), cv2.COLOR_RGB2GRAY),(32, 32)) for f in os.listdir(folder)]\n",
    "        #onlyfiles = [cv2.resize(image.imread(folder+f),(32, 32)) for f in os.listdir(folder)]\n",
    "        pixels = np.array(onlyfiles).reshape(-1,1,32,32).astype('float32') #/255\n",
    "        return pixels[:1,:,:,:]\n",
    "    print(\"Train Images : Loading . . .\")\n",
    "    train_images = _images(path)\n",
    "    print(\"Train Labels : Loading . . .\")\n",
    "    train_labels = _labels(path)\n",
    "    \"\"\"\"\n",
    "    print(\"Test Images  : Loading . . .\")\n",
    "    test_images = _images(os.path.join(path, files[2]))\n",
    "    print(\"Test Labels  : Loading . . .\")\n",
    "    test_labels = _labels(os.path.join(path, files[3])) \n",
    "    \"\"\"\n",
    "    print(\"Done!\")\n",
    "    return train_images, train_labels #, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images : Loading . . .\n",
      "Train Labels : Loading . . .\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels= mnist()  #, test_images, test_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 32, 32)\n",
      "(1, 1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "ch = 1 #Number of channels\n",
    "#train_images=cv2.resize(train_images[0,:,:], (32,32)).reshape(1,1,32,32)\n",
    "print(train_images.shape)\n",
    "#train_labels = train_images\n",
    "print(train_labels.shape) # ONE-HOT !!!\n",
    "#print(train_images.T.shape)\n",
    "#print(train_labels.reshape((train_images.shape[0], 10)).shape)\n",
    "\n",
    "##### Getting Ready for the Conv Layers #####\n",
    "#train_images = train_images.reshape(train_images.shape[0], ch, 28, 28).squeeze()\n",
    "#test_images = test_images.reshape(test_images.shape[0], ch, 28, 28).squeeze()\n",
    "\n",
    "#############################################\n",
    "\n",
    "#y = train_images[1].reshape(1,28,28)\n",
    "#print(y.squeeze().shape)\n",
    "'''\n",
    "temp = []\n",
    "#train_labels = train_labels.sum(1)\n",
    "for i in range(int(len(train_labels[:]))):\n",
    "    temp.append(list(train_labels[i][:]).index(1))\n",
    "    \n",
    "    \n",
    "train_labels = np.array(temp.copy())\n",
    "#print(train_labels[0:5])\n",
    "'''\n",
    "#plt.imshow(train_images[0].squeeze(), cmap='Greys_r');\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAANKElEQVR4nO3db4hddX7H8fe30RDxv2uUwX/ZFSmssk10kOLKYmm7pFLQXbCsj6IszRIq6IPCBgtdUyixpdr2USRbY0JpswhWFJHuimRx6wPraP0Tze7qSupmE41rKCootvrtgzmBSXbOzM2995w7k+/7BcM993fuvefLYT5z/vzm/n6RmUg6+f3WpAuQ1A/DLhVh2KUiDLtUhGGXijDsUhGnjPLmiFgP/COwAvinzLx3kde39vOdddZZre+77LLL5m1fuXLlQHUuR59++mnruoMHD7aue//997soR8tIZsZ87TFsP3tErAB+DvwhcAB4Hrg1M19f4D2tG1u/fn3rtrZt2zZv+5o1awYrdhnav39/67otW7a0rtu5c+f4i9Gy0hb2UU7jrwXezMy3MvNT4AfATSN8nqQOjRL2i4Bfznl+oGmTtASNcs0+36nCb5ymR8RGYOMI25E0BqOE/QBwyZznFwO/cecoM7cD22Hha3ZJ3RrlNP554IqI+GJErAS+BTw+nrIkjdvQd+MBIuJG4B+Y7XrbkZl/vdDrzz777LzuuuvmXbd79+7W951zzjlD13gy+uSTT1rXnXbaaT1WoqWo7W78SP3smfkk8OQonyGpH/4HnVSEYZeKMOxSEYZdKsKwS0WM1PV2otatW5d79uyZd53da+Nx++23z9vuF2Tq6OKLMJKWEcMuFWHYpSIMu1SEYZeKGOl/40/UihUrlvVd9wceeGDe9k2bNrW+5+OPP25dt2rVqpFrOt6OHTtO+D3eqa/BI7tUhGGXijDsUhGGXSrCsEtFGHapiF673vrU1k0GC3eVjdtCs7ds3bp17NuLmPc7EK2z6izGbrmTh0d2qQjDLhVh2KUiDLtUhGGXijDsUhGjTv+0H/gQ+Az4v8ycXuj109PTOTMzM/T2TkRbF9Ry0efYgE4ndXLpZPqnxu9l5q/H8DmSOuRpvFTEqGFP4EcR8UJEbBxHQZK6Mepp/Fcz82BEXAA8FRE/zcxn5r6g+SOwEeDSSy8dcXOShjXSkT0zDzaPh4FHgWvnec32zJzOzOnVq1ePsjlJIxg67BFxekSceXQZ+Dqwd1yFSRqvUU7jLwQebbq4TgH+NTP/fSxVqVddDHyppWfosGfmW8DvjLEWSR2y600qwrBLRRh2qQjDLhVh2KUilvWAkwsNKrkcrF+/ftIlqBCP7FIRhl0qwrBLRRh2qQjDLhWxrO/G9zmNUxceeuihSZcALP9eDQ3GI7tUhGGXijDsUhGGXSrCsEtFGHapiJGmfzpR457+ySmexmO570cdq236J4/sUhGGXSrCsEtFGHapCMMuFWHYpSIWDXtE7IiIwxGxd07beRHxVES80Tye222ZkkY1yJF9J3D8yIibgacz8wrg6ea5pCVs0bA3860fOa75JmBXs7wLuHnMdUkas2Gv2S/MzEMAzeMF4ytJUhc6v0EXERsjYiYiZt57772uNyepxbBhfzcipgCax8NtL8zM7Zk5nZnTq1evHnJzkkY1bNgfBzY0yxuAx8ZTjqSuLDrgZETsBm4Azo+IA8D3gHuBhyPi28DbwC1dFtnmtttua123c+fO3upYiFM8aalYNOyZeWvLqt8fcy2SOuR/0ElFGHapCMMuFWHYpSIMu1TEsp7rbaG50hZat3///tZ1W7ZsaV33zjvvzNu+bdu21vesWbOmdV2fnM9NHtmlIgy7VIRhl4ow7FIRhl0qwrBLRfQ619s111yTzz777LzrVq1a1VsdJ7O2LrZNmzb1XIkmxbnepOIMu1SEYZeKMOxSEYZdKqLXu/FTU1PZNm7c1q1be6tjuXv99ddb11155ZU9VqKlyLvxUnGGXSrCsEtFGHapCMMuFWHYpSIW7XqLiB3AHwOHM/Oqpu0e4E+Bo9Oy3p2ZTy66sYih+vk2b948b/vJ3F230JhxfqlFCxml620nMN+EZX+fmWubn0WDLmmyFg17Zj4DHOmhFkkdGuWa/Y6IeCUidkTEuWOrSFInhg37NuByYC1wCLiv7YURsTEiZiJiZshtSRqDocKeme9m5meZ+TnwfeDaBV67PTOnM3N62CIljW6osEfE1Jyn3wD2jqccSV0ZpOttN3ADcD7wLvC95vlaIIH9wHcy89CiGxuy603S4Nq63nr9iqthl7rnV1yl4gy7VIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUiDLtUhGGXijDsUhGGXSrCsEtFGHapCMMuFWHYpSIMu1SEYZeKMOxSEYZdKsKwS0UYdqkIwy4VYdilIhYNe0RcEhF7ImJfRLwWEXc27edFxFMR8Ubz6LTN0hI2yFxvU8BUZr4YEWcCLwA3A7cBRzLz3ojYDJybmd9d5LOc/knq2NDTP2Xmocx8sVn+ENgHXATcBOxqXraL2T8AkpaoE7pmj4g1wDrgOeDCozO3No8XjLs4SeNzyqAvjIgzgEeAuzLzg4h5zxTme99GYONw5Ukal4GmbI6IU4EngB9m5v1N28+AGzLzUHNd/+PM/O1FPsdrdqljQ1+zx+wh/EFg39GgNx4HNjTLG4DHRi1SUncGuRt/PfAT4FXg86b5bmav2x8GLgXeBm7JzCOLfJZHdqljbUf2gU7jx8WwS90b+jRe0snBsEtFGHapCMMuFWHYpSIMu1SEYZeKMOxSEYZdKsKwS0UYdqkIwy4VYdilIgy7VIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUiDLtUhGGXijDsUhGGXSpikLneLomIPRGxLyJei4g7m/Z7IuJXEfFS83Nj9+VKGtYgc71NAVOZ+WJEnAm8ANwM/AnwUWb+3cAbc/onqXNt0z8tOj97Zh4CDjXLH0bEPuCi8ZYnqWsndM0eEWuAdczO4ApwR0S8EhE7IuLcMdcmaYwGDntEnAE8AtyVmR8A24DLgbXMHvnva3nfxoiYiYiZMdQraUgDTdkcEacCTwA/zMz751m/BngiM69a5HO8Zpc6NvSUzRERwIPAvrlBb27cHfUNYO+oRUrqziB3468HfgK8CnzeNN8N3MrsKXwC+4HvNDfzFvosj+xSx9qO7AOdxo+LYZe6N/RpvKSTg2GXijDsUhGGXSrCsEtFGHapCMMuFWHYpSIMu1SEYZeKMOxSEYZdKsKwS0UYdqkIwy4VYdilIgy7VIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUiDLtUxCBzva2KiP+MiJcj4rWI2NK0nxcRT0XEG82jUzZLS9ggc70FcHpmftTM5vofwJ3AN4EjmXlvRGwGzs3M7y7yWU7/JHVs6OmfctZHzdNTm58EbgJ2Ne27gJvHUKekjgx0zR4RKyLiJeAw8FRmPgdceHTW1ubxgu7KlDSqgcKemZ9l5lrgYuDaiLhq0A1ExMaImImImWGLlDS6E7obn5n/A/wYWA+8GxFTAM3j4Zb3bM/M6cycHrFWSSMY5G786og4p1k+DfgD4KfA48CG5mUbgMe6KlLS6Aa5G/8VZm/ArWD2j8PDmflXEfEF4GHgUuBt4JbMPLLIZ3k3XupY2934RcM+ToZd6t7QXW+STg6GXSrCsEtFGHapCMMuFXFKz9v7NfDfzfL5zfNJs45jWcexllsdl7Wt6LXr7ZgNR8wshf+qsw7rqFKHp/FSEYZdKmKSYd8+wW3PZR3Hso5jnTR1TOyaXVK/PI2XiphI2CNifUT8LCLebMavm4iI2B8Rr0bES30OrhEROyLicETsndPW+wCeLXXcExG/avbJSxFxYw91XBIReyJiXzOo6Z1Ne6/7ZIE6et0nnQ3ympm9/jD7VdlfAF8CVgIvA1/uu46mlv3A+RPY7teAq4G9c9r+FtjcLG8G/mZCddwD/HnP+2MKuLpZPhP4OfDlvvfJAnX0uk+AAM5olk8FngN+d9T9MYkj+7XAm5n5VmZ+CvyA2cEry8jMZ4Djv/vf+wCeLXX0LjMPZeaLzfKHwD7gInreJwvU0aucNfZBXicR9ouAX855foAJ7NBGAj+KiBciYuOEajhqKQ3geUdEvNKc5vc6H0BErAHWMXs0m9g+Oa4O6HmfdDHI6yTCPt8X6yfVJfDVzLwa+CPgzyLiaxOqYynZBlwOrAUOAff1teGIOAN4BLgrMz/oa7sD1NH7PskRBnltM4mwHwAumfP8YuDgBOogMw82j4eBR5m9xJiUgQbw7Fpmvtv8on0OfJ+e9kkzAckjwL9k5r81zb3vk/nqmNQ+abZ9woO8tplE2J8HroiIL0bESuBbzA5e2auIOD0izjy6DHwd2Lvwuzq1JAbwPPrL1PgGPeyTZtahB4F9mXn/nFW97pO2OvreJ50N8trXHcbj7jbeyOydzl8AfzGhGr7EbE/Ay8BrfdYB7Gb2dPB/mT3T+TbwBeBp4I3m8bwJ1fHPwKvAK80v11QPdVzP7KXcK8BLzc+Nfe+TBerodZ8AXwH+q9neXuAvm/aR9of/QScV4X/QSUUYdqkIwy4VYdilIgy7VIRhl4ow7FIRhl0q4v8B7dPE9oDmeJAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_labels[0].squeeze(), cmap=plt.get_cmap('gray'), vmin=0, vmax=1);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Convolution Architecture - Downsampling/Upsampling  ##################\n",
    "\n",
    "def init_filters(layers, init_n_f ,trim):\n",
    "    filters = []\n",
    "    bias = []\n",
    "    f_dc = []\n",
    "    \n",
    "    trimf = trim\n",
    "    trimb = trim*5\n",
    "    n_f = init_n_f #initial number of filters/kernels\n",
    "    ch_in = 1      #input channels\n",
    "    \"\"\"\n",
    "    for i in range(layers):\n",
    "        if(i != 0):\n",
    "            n_f = n_f*2 #16,32,64,128,256\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trimb\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "    \n",
    "    #Deconvolution filters    \n",
    "    for i in range(1,layers):\n",
    "        n_f = n_f//2 #128,64,32,16\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        fdc = np.random.randn(n_f,ch_in,2,2)*trimf #upsampling filter, its result will be conc with conv4 output so the channels will be doubled again\n",
    "        bdc = np.random.randn(fdc.shape[0],1)* trimb\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b2 = np.random.randn(f2.shape[0],1)* trimb\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        dc = [fdc, bdc]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "        f_dc.append(dc)\n",
    "        \n",
    "    \"\"\"\n",
    "    trimbr = trim\n",
    "    locbr = 0\n",
    "    for i in range(layers):\n",
    "        if(i != 0):\n",
    "            n_f = n_f*2 #16,32,64,128,256\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.normal(loc = locbr, scale = trimbr , size = f1) #np.random.randn(n_f, ch_in, 3, 3) *trimf\n",
    "        b1 = np.random.normal(loc = locbr, scale = trimbr , size = (f1.shape[0],1)) #np.random.randn(f1.shape[0],1)* trimb\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.normal(loc = locbr, scale = trimbr , size = f2)\n",
    "        b2 = np.random.normal(loc = locbr, scale = trimbr , size = (f2.shape[0],1))\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "    \n",
    "    #Deconvolution filters    \n",
    "    for i in range(1,layers):\n",
    "        n_f = n_f//2 #128,64,32,16\n",
    "        #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "        fdc =  np.random.normal(loc = locbr, scale = trimbr , size = (n_f,ch_in,2,2))#upsampling filter, its result will be conc with conv4 output so the channels will be doubled again\n",
    "        bdc = np.random.normal(loc = locbr, scale = trimbr , size = (fdc.shape[0],1))\n",
    "        f1 = (n_f, ch_in, 3, 3)\n",
    "        f1 = np.random.normal(loc = locbr, scale = trimbr , size = (n_f, ch_in, 3, 3))\n",
    "        b1 = np.random.normal(loc = locbr, scale = trimbr , size = (f1.shape[0],1))\n",
    "        ch_in = n_f\n",
    "        f2 = (n_f, ch_in, 3, 3)\n",
    "        f2 = np.random.normal(loc = locbr, scale = trimbr , size = f2)\n",
    "        b2 = np.random.normal(loc = locbr, scale = trimbr , size = (f2.shape[0],1))\n",
    "        f = [f1, f2]\n",
    "        b = [b1, b2]\n",
    "        dc = [fdc, bdc]\n",
    "        filters.append(f)\n",
    "        bias.append(b)\n",
    "        f_dc.append(dc)\n",
    "    return filters, bias, f_dc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation Functions ###\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UTILITIES #############\n",
    "\n",
    "def conv(image, params, s = 1, pad = 1 ): # s = 1 (conv stride)\n",
    "    #f1 shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    [f, b] = params\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((image.shape[0],image.shape[1]+2*pad ,image.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = image\n",
    "        image = tmp    \n",
    "        \n",
    "    f_num = f.shape[0]\n",
    "    f_depth = f.shape[1] #f1 = 1 , f2 = 8\n",
    "    f_size = f.shape[2] \n",
    "        \n",
    "\n",
    "    h_range = int((image.shape[1] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f_size) / s) + 1     # (W - F + 2P) / S  \n",
    "    np_o = np.zeros((f_num, h_range, w_range))\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                np_o[z, _h, _w] = np.sum(np.multiply(image[:, _h*s : _h*s + f_size, _w*s : _w*s + f_size] , f[z, :, :, :])) + b[z]\n",
    "    \n",
    "    return np_o\n",
    "\n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s = 1, pad = 1 ):\n",
    "    #filt =np.rot90(filt, 2)  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!! A T T E N T I O N !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    f_num, n_c, f, _ = filt.shape\n",
    "    \n",
    "    _ ,h , w = dconv_prev.shape\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    if(pad >= 1):\n",
    "        tmp = np.zeros((conv_in.shape[0],conv_in.shape[1]+2*pad ,conv_in.shape[2]+2*pad))\n",
    "        tmp[:,pad:-pad,pad:-pad] = conv_in\n",
    "        conv_in = tmp\n",
    "    \n",
    "    dconv_in = np.zeros(conv_in.shape)\n",
    "    db = np.zeros((f_num,1))\n",
    "\n",
    "    for z in range(f_num): # Number of filters\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                #each entry of the dconv_prev will try to affect the idxs from which was made of.\n",
    "                dfilt[z] += dconv_prev[z, _h, _w] * conv_in[:, _h*s : _h*s + f, _w*s : _w*s + f]\n",
    "                dconv_in[:, _h*s : _h*s + f, _w*s : _w*s + f] += dconv_prev[z, _h, _w] * filt[z]  \n",
    "        db[z] = np.sum(dconv_prev[z])  #, axis =1) ## AXIS?\n",
    "    \n",
    "    if(pad >=1 ):\n",
    "        dconv_in = dconv_in[:, pad:-pad, pad:-pad]  # Cropping\n",
    "        \n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "def convTransp1(image, params, s = 2, pad = 1):\n",
    "    [f, b] = params\n",
    "    n_f, n_c, f_s, _ = f.shape\n",
    "    input_s = image.shape[1]\n",
    "    target_dim = input_s*2\n",
    "    res = np.zeros((n_f, target_dim, target_dim))\n",
    "    temp =np.zeros((n_c, target_dim, target_dim))\n",
    "    for z in range(n_f):\n",
    "        for _h in range(input_s):      \n",
    "            for _w in range(input_s):\n",
    "                temp[:, _h*s:_h*s+f_s, _w*s:_w*s+f_s] += image[:, _h, _w].reshape(n_c,1,1)*f[z,:,:,:] #bias will be added at the end\n",
    "        res[z] = np.sum(temp , axis = 0) + b[z]\n",
    "    return res, image\n",
    "\n",
    "def convTranspBackward1(dconv_prev, new_in, filt, s = 2):\n",
    "    n_f, n_c, f_s, _ = filt.shape\n",
    "    _, input_s, _ = new_in.shape\n",
    "    #final_dim = (new_in.shape[1] - 2)//2 + 1 \n",
    "    dc_s=dconv_prev.shape[1]\n",
    "    temp = np.zeros((n_c,dc_s,dc_s))\n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dconv_in = np.zeros(new_in.shape)\n",
    "    db = np.zeros((n_f,1))\n",
    "    for z in range(n_f):\n",
    "        for _h in range(input_s):      \n",
    "            for _w in range(input_s): \n",
    "                dfilt[z] += dconv_prev[z, _h*s:_h*s+f_s, _w*s:_w*s+f_s]*new_in[:,_h,_w].reshape(n_c,1,1)\n",
    "                temp[:, _h*s:_h*s+f_s, _w*s:_w*s+f_s] += dconv_prev[z, _h*s:_h*s+f_s, _w*s:_w*s+f_s] * filt[z]\n",
    "                for ch in range(n_c):\n",
    "                    dconv_in[ch, _h, _w] += np.sum(temp[ch, _h*s:_h*s+f_s, _w*s:_w*s+f_s])\n",
    "        db[z] = np.sum(dconv_prev[z])        \n",
    "    return dconv_in, dfilt, db\n",
    "\n",
    "    \n",
    "def convTransp(image, params, s = 1, pad = 1):\n",
    "    #s is always 1, upsample kernel = 2\n",
    "    #zero insertions between pixels s_downsampled -1 = 2-1 = 1\n",
    "    #required padding in order to double my dimensions with the given data:\n",
    "    #(i-1)*2 + k -2p = output size, where our padding is k - p -1 = 2-0-1=1(we assume p=0)\n",
    "    [f, b]=params\n",
    "    f = np.rot90(f, 1, (2,3))\n",
    "    params = [f, b]\n",
    "    input_s = image.shape[1]\n",
    "    target_dim = input_s*2 #final dim, after conv\n",
    "    required_pad = 1 #always for filter 2x2 ,stride 1, zero insertion 1 and main target to double dim\n",
    "    #make our new custom input\n",
    "    size = input_s*2 +1\n",
    "    new_in = np.zeros((image.shape[0], size, size))\n",
    "    \n",
    "    ### OR just: np.pad(image[:,:,:],2,'constant') # Important, we must loop with respect to the 1st dim\n",
    "    for i in range(1, target_dim, 2):\n",
    "        for j in range(1, target_dim, 2):\n",
    "                new_in[:, i, j] = image[:, i//2, j//2]\n",
    "    #now we do a normal convolution(pad = 0)\n",
    "    res = conv(new_in, params, 1, 0) #thats the final result with target_dim\n",
    "    return res, new_in # we will need new_in so we wont calc it again for the backprop\n",
    "    \n",
    "def convTranspBackward(dconv_prev, new_in, filt, s = 1):\n",
    "    #First, we do a backward convolution on new_in,d_conv_prev,\n",
    "    #then we will choose form the d_conv_new the entries that match the initial 'smaller' input image\n",
    "    #by selecting the odd matrix cells 1,3,5... because we had applied a standard pad=1,zero inser=1\n",
    "    dconv_in, dfilt, db = convolutionBackward(dconv_prev, new_in, filt,1,0)\n",
    "    #Now its time to choose the right entries to build the gradients of the initial input image\n",
    "    dim = dconv_in.shape[1]\n",
    "    final_dim = (new_in.shape[1] - 2)//2 + 1 #based on dimen of image before final conv that gives the result,..\n",
    "    #e.g. for new_in 7x7 that is going to convoluted with a 2x2 kernel and give a 6x6 upsampled from 3x3 init image\n",
    "    # now from this 7x7 --> apply the formula above,we get the 3x3 dimension number\n",
    "    res = np.zeros((dconv_in.shape[0], final_dim, final_dim))\n",
    "    for i in range(1, dim, 2):\n",
    "        for j in range(1, dim, 2):\n",
    "                res[:, i//2, j//2] = dconv_in[:, i, j]\n",
    "    return res, dfilt, db\n",
    "    \n",
    "    \n",
    "def maxpool(image, f=2 , s=2):\n",
    "    \n",
    "    h_range = int((image.shape[1] - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w_range = int((image.shape[2] - f) / s) + 1     # (W - F + 2P) / S \n",
    "    out = np.zeros((image.shape[0], h_range, w_range))\n",
    "    \n",
    "    for z in range(image.shape[0]): # Number of channels\n",
    "        for _h in range(h_range):      \n",
    "            for _w in range(w_range):\n",
    "                out[z, _h, _w] = np.max(image[z, _h*s : _h*s + f, _w*s : _w*s + f])\n",
    "    return out\n",
    "\n",
    "def nanargmax(arr):\n",
    "    #print(arr.shape)\n",
    "    try:\n",
    "        idx = np.nanargmax(arr)\n",
    "        #print (idx)\n",
    "    except:\n",
    "        idx = 0\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "def maxpoolBackward(dpool, conv, f=2 , s=2):\n",
    "    num_c, h, w = conv.shape\n",
    "    h = int((h - f) / s) + 1     # (W - F + 2P) / S  \n",
    "    w = int((w - f) / s) + 1     # (W - F + 2P) / S \n",
    "    \n",
    "    dout = np.zeros(conv.shape)\n",
    "    #print(conv.shape)\n",
    "    for z in range(num_c): # Number of channels\n",
    "        for _h in range(h):      \n",
    "            for _w in range(w):\n",
    "                (a, b) = nanargmax(conv[z, _h*s : _h*s + f, _w*s : _w*s + f]) #Getting the indexes from the max value in this area\n",
    "                #put it on the new array\n",
    "                dout[z, _h + a, _w + b] = dpool[z, _h, _w]\n",
    "    \n",
    "    \n",
    "    return dout\n",
    "\n",
    "\n",
    "def reshape(img, reshape_dim):\n",
    "    pad = reshape_dim - img.shape[1]\n",
    "    if(pad == 0):\n",
    "        return img\n",
    "    res = np.zeros((img.shape[0], reshape_dim, reshape_dim))\n",
    "    if(pad > 1):\n",
    "        res[:, pad//2:-(pad//2), pad//2:-(pad//2)] = img\n",
    "    else:\n",
    "        res[:, 0:-(pad), 0:-(pad)] = img\n",
    "    return res\n",
    "\n",
    "\n",
    "def crop(img, crop_dim): #TODO : look at it..future upgrade to downsample..maybe\n",
    "    start = img.shape[1]//2 - crop_dim//2\n",
    "    return img[:,(start):(start +crop_dim),(start):(start +crop_dim)]\n",
    "\n",
    "    \n",
    "    \n",
    "def crop2half(img):\n",
    "    #return gradients for decoder side and gradients for encoder side\n",
    "    n_ch = img.shape[0]//2\n",
    "    return img[n_ch:n_ch*2 ,:,:], img[0:n_ch,:,:]\n",
    "    \n",
    "def concat(img1_true, img2):\n",
    "    n_ch = img1_true.shape[0]\n",
    "    dim = img1_true.shape[1]\n",
    "    if(img2.shape[1] != dim):\n",
    "        img2 = crop(img2, dim)\n",
    "        print(\"Warning: Extra crop needed and handled!(%d --> %d)\" %(dim, img2.shape[1]))\n",
    "    res = np.zeros((n_ch*2, dim, dim))\n",
    "    res[0:n_ch,:,:] = img2\n",
    "    res[n_ch:n_ch*2 ,:,:] = img1_true\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cost Calculations ######\n",
    "def Cross_Entropy(logs, targets):  # Pixel-Wise Cross entropy --> average accuracy\n",
    "    out = np.zeros(logs.shape)\n",
    "    mylen = logs.shape[0]*logs.shape[1]*logs.shape[2]\n",
    "    #print(len(targets))\n",
    "    #logs.shape (dim x dim) like target\n",
    "    for i in range(logs.shape[1]):\n",
    "        for j in range(logs.shape[2]):\n",
    "            if(targets[:,i,j] >= logs[:,i,j]):#Gray and above\n",
    "                out[:,i,j] = logs[:,i,j]/targets[:,i,j] \n",
    "            else:\n",
    "                out[:,i,j] = (1 - logs[:,i,j])/(1 - targets[:,i,j]) # e.g if logs[i,j]= 0.4 and we want(target) 0 --> 1-0.4=0.6 prob. for zero \n",
    "    #or\n",
    "    #out = targets*logs + ((-1*(targets-1)) - (-1*(targets-1))*logs)\n",
    "    res =out.sum()/mylen\n",
    "    return -np.log(res),res\n",
    "\n",
    "\n",
    "def Dice_Coef(logs, targets):\n",
    "    out = np.zeros(logs.shape)\n",
    "    mylen = logs.shape[0]*logs.shape[1]*logs.shape[2]\n",
    "    #Apply Dice coefficient\n",
    "    numerator = (logs*targets)\n",
    "    denominator = logs + targets\n",
    "    loss = 1 - (2*np.sum(numerator))/(np.sum(denominator))\n",
    "    return loss, np.exp(-loss)\n",
    "                \n",
    "    \n",
    "    \n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    \n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost) ### ATTENTION!\n",
    "\n",
    "def normalize(output):\n",
    "    output[output<-4]=-4\n",
    "    output[output>4] = 4\n",
    "    return output\n",
    "\n",
    "###### Accuracy Calculation ######\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validate(X, Y, params):\n",
    "    ### Unpacking ###\n",
    "    [filters, bias, f_dc, out_fb] = params\n",
    "    [f1,f2,f3,f4,f5] = filters\n",
    "    [b1,b2,b3,b4,b5]= bias \n",
    "    f1_dc = f_dc[0][0]\n",
    "    b1_dc = f_dc[0][1]\n",
    "    f2_dc = f_dc[1][0]\n",
    "    b2_dc = f_dc[1][1]\n",
    "    [out_f, out_b] = out_fb\n",
    "    #################\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Calculating Forward step . . .')\n",
    "    \n",
    "    batch = 1\n",
    "    for c in range(0, X.shape[0], batch):\n",
    "        if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "            batch = X.shape[0] - c\n",
    "        X_t = X[c:(c + batch)]\n",
    "        Y_t = Y[c:(c + batch)]\n",
    "        for b in range(batch):\n",
    "            #################### TODO: BLOCK IMPLEMENTATION - FUTURE UPDATE ######################\n",
    "\n",
    "\n",
    "            #########################################################################################\n",
    "            #########################################################################################\n",
    "            ######################## Forward Propagation Convolution Part  ##########################\n",
    "\n",
    "\n",
    "            ########### 1st Big Layer ###########    (with zero padding ='same',so with stride =1 we get same dim as the input)\n",
    "            params = [f1[0], b1[0]]  \n",
    "            conv1_1 = conv(X_t[b], params, 1)   #conv1 shape = (num_channels, h, w), padding = 1 (same output dim)\n",
    "            conv1_1[conv1_1<=0] = 0 #Relu\n",
    "\n",
    "            params = [f1[1], b1[1]]\n",
    "            conv1_2 = conv(conv1_1, params, 1)\n",
    "            conv1_2[conv1_2<=0] = 0 #Relu\n",
    "            ##################################### conv1_2: 32x32x16\n",
    "\n",
    "            pl1 = maxpool(conv1_2, 2, 2) #   pl1 : (32-2)/2+1  = 16 \n",
    "\n",
    "            ########### 2nd Big Layer ###########\n",
    "            params = [f2[0], b2[0]]  \n",
    "            conv2_1 = conv(pl1, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "            conv2_1[conv2_1<=0] = 0 #Relu\n",
    "\n",
    "            params = [f2[1], b2[1]]\n",
    "            conv2_2 = conv(conv2_1, params, 1)\n",
    "            conv2_2[conv2_2<=0] = 0 #Relu             \n",
    "            #####################################  16x16x32\n",
    "\n",
    "            pl2 = maxpool(conv2_2, 2, 2) #   pl1 : (16-2)/2+1  = 8 \n",
    "\n",
    "            ########### 2nd Big Layer ###########\n",
    "            params = [f3[0], b3[0]]  \n",
    "            conv3_1 = conv(pl2, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "            conv3_1[conv3_1<=0] = 0 #Relu\n",
    "\n",
    "            params = [f3[1], b3[1]]\n",
    "            conv3_2 = conv(conv3_1, params, 1)\n",
    "            conv3_2[conv3_2<=0] = 0 #Relu             \n",
    "            #####################################  8x8x64\n",
    "\n",
    "            ##################################### \n",
    "            ##################################### \n",
    "            #####################################\n",
    "            #Deconvolution/Upsampling\n",
    "            # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "\n",
    "            params = [f_dc[0][0], f_dc[0][1]] # deconv filter, deconv bias\n",
    "            dc1, new_in1 = convTransp(conv3_2, params, 1, 0)   #result:   =  16x6x32 , \n",
    "\n",
    "            #Concat dc1 with conv2_2 so we get 64 channels (16x16x64)\n",
    "            c1 = concat(dc1, conv2_2) # 1st one is the right one size  \n",
    "\n",
    "            ########### 1st Big dc Layer ###########          16x16x64     \n",
    "            params = [f4[0], b4[0]]  \n",
    "            conv4_1 = conv(c1, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "            conv4_1[conv4_1<=0] = 0 #Relu\n",
    "\n",
    "            params = [f4[1], b4[1]]\n",
    "            conv4_2 = conv(conv4_1, params, 1)\n",
    "            conv4_2[conv4_2<=0] = 0 #Relu   \n",
    "            #####################################    16x16x32\n",
    "\n",
    "            #Deconvolution/Upsampling\n",
    "            # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "\n",
    "            params = [f_dc[1][0], f_dc[1][1]] # deconv filter, deconv bias\n",
    "            dc2, new_in2 = convTransp(conv4_2, params, 1, 0)   #result:   =  32x32x16 , \n",
    "\n",
    "            #Concat dc2 with conv1_2 so we get 64 channels (32x32x32)\n",
    "            c2 = concat(dc2, conv1_2) # 1st one is the right one size  \n",
    "\n",
    "            ########### 1st Big dc Layer ###########          32x32x32     \n",
    "            params = [f5[0], b5[0]]  \n",
    "            conv5_1 = conv(c2, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "            conv5_1[conv5_1<=0] = 0 #Relu\n",
    "\n",
    "            params = [f5[1], b5[1]]\n",
    "            conv5_2 = conv(conv5_1, params, 1)\n",
    "            conv5_2[conv5_2<=0] = 0 #Relu   \n",
    "            #####################################    32x32x16\n",
    "\n",
    "\n",
    "            ############################# Last Layer conv(1x1) --> 32x32x1 ##########################\n",
    "            params = [out_f, out_b]\n",
    "            output = conv(conv5_2, params, 1, 0) #output.shape: 32x32x1\n",
    "\n",
    "\n",
    "            output = normalize(output)\n",
    "            ## Sigmoid ##\n",
    "            Y_hat = sigmoid(output)\n",
    "            #print(Y_hat[:,0:3,0:3])\n",
    "            #label crop is needed\n",
    "            #Y_t_b = crop(Y_t[b], Y_hat.shape[1])\n",
    "            #plt.imshow(Y_hat.squeeze(), cmap=plt.get_cmap('gray'), vmin=0, vmax=1);\n",
    "            plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "            cost_,accuracy_ = Dice_Coef(Y_hat, Y_t[b])#Cross_Entropy(Y_hat, Y_t[b])\n",
    "            cost = cost_\n",
    "            accuracy = accuracy_\n",
    "            print(\"Cost: {:.2f}   -   Accuracy: {:.2f}%\".format(cost/batch, (accuracy*100)/batch))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  TRAIN  ######\n",
    "#import torch`\n",
    "import time\n",
    "time.time()\n",
    "\n",
    "\n",
    "def train(X, Y, epochs, learning_rate, dropout, verbose=True, callback=None):\n",
    "    # initiation of neural net parameters\n",
    "    #params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    #f1,f2,f3,f4,f5,fd1,fd2,fd3,fd4\n",
    "    trim= 0.0001 # 0.005,0.00005 with Lr 0.01\n",
    "    filters,bias, f_dc = init_filters(3,16,trim) #Double the channel-filter 4 times  (up to 256 and back again)\n",
    "    ##Final 1x1 filter\n",
    "    trimf = trim\n",
    "    trimb = trim\n",
    "    out_f = np.random.randn(1,16,1,1)*trimf\n",
    "    out_b = np.random.randn(out_f.shape[0],1)*trimb  \n",
    "    out_fb = [out_f, out_b]\n",
    "    #filter shape : (num_filters,input channels, f_h, f_w)\n",
    "    #image shape: (channels, height, width)\n",
    "    accuracy_history=[]\n",
    "    \n",
    "    if(dropout>0):\n",
    "        print(\"Dropout Enabled! -  Value: {}\".format(dropout))\n",
    "    else:\n",
    "        print(\"Dropout Disabled!\")\n",
    "    \n",
    "    v_adam =  [] #filter1,filter2\n",
    "    s_adam =  [] \n",
    "    bv_adam = [] #bias1,bias2\n",
    "    bs_adam = []\n",
    "    fdc_v_adam=[] #filter,bias\n",
    "    fdc_s_adam=[]\n",
    "    #format: [v1,v2,v3,v4,v5,vd1,vd2,vd3,vd4] ,same for the rest, each of these include a tuple for the 2 conv filter\n",
    "    #[s1,s2,s3,s4,s5,sd1,sd2,vs3,sd4]\n",
    "    # upsampling filters : [v1_dc,v2_dc,v3_dc]  \n",
    "\n",
    "            \n",
    "\n",
    "    # performing calculations for subsequent iterations\n",
    "    \n",
    "    \n",
    "    [f1,f2,f3,f4,f5] = filters\n",
    "    [b1,b2,b3,b4,b5]= bias \n",
    "    \n",
    "    f1_dc = f_dc[0][0]\n",
    "    b1_dc = f_dc[0][1]\n",
    "    f2_dc = f_dc[1][0]\n",
    "    b2_dc = f_dc[1][1]\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # step forward\n",
    "        running_loss = 0\n",
    "        accuracy = 0\n",
    "        batch = 1\n",
    "        print(\"Epoch: {%d}\" %(e+1))\n",
    "        for c in range(0, X.shape[0], batch):\n",
    "            print('Batch: {}'.format(int(c/batch +1)))\n",
    "            #### Reset Gradients (Every batch) ####\n",
    "            beta1= 0.9  #0.9\n",
    "            beta2= 0.99 #0.99\n",
    "            lr = learning_rate\n",
    "            batch_size= batch\n",
    "            if(X.shape[0] - c < batch):#means that there is a smaller(<32) part left\n",
    "                batch = X.shape[0] - c\n",
    "            X_t = X[c:(c + batch)]  # shape:(m, ch, h, w) --> shape:(batch, ch, h, w)\n",
    "            Y_t = Y[c:(c + batch)]\n",
    "            cost = 0\n",
    "            accuracy = 0\n",
    "            if(e < 80):\n",
    "                \n",
    "                df =  []\n",
    "                db =  []\n",
    "                dfb=  []\n",
    "                for i in filters:\n",
    "                    v1 = np.zeros(i[0].shape)\n",
    "                    v2 = np.zeros(i[1].shape)\n",
    "                    s1 = np.zeros(i[0].shape)\n",
    "                    s2 = np.zeros(i[1].shape)\n",
    "                    v_a = [v1, v2]\n",
    "                    s_a = [s1, s2]\n",
    "                    v_adam.append(v_a)\n",
    "                    s_adam.append(s_a)\n",
    "\n",
    "                    df1_t = np.zeros(i[0].shape)\n",
    "                    df2_t = np.zeros(i[1].shape)\n",
    "                    f_temp = [df1_t, df2_t]\n",
    "                    df.append(f_temp)\n",
    "\n",
    "                for i in bias:\n",
    "                    bv1 = np.zeros(i[0].shape)\n",
    "                    bv2 = np.zeros(i[1].shape)\n",
    "                    bs1 = np.zeros(i[0].shape)\n",
    "                    bs2 = np.zeros(i[1].shape)    \n",
    "                    bv_a = [bv1, bv2]\n",
    "                    bs_a = [bs1, bs2]\n",
    "                    bv_adam.append(bv_a)\n",
    "                    bs_adam.append(bs_a)\n",
    "\n",
    "\n",
    "                    db1_t = np.zeros(i[0].shape)\n",
    "                    db2_t = np.zeros(i[1].shape)\n",
    "                    b_temp = [db1_t, db2_t]\n",
    "                    db.append(b_temp)\n",
    "\n",
    "                for i in f_dc:\n",
    "                    fdc_v1 = np.zeros(i[0].shape)\n",
    "                    bdc_v2 = np.zeros(i[1].shape)\n",
    "                    fdc_s1 = np.zeros(i[0].shape)\n",
    "                    bdc_s2 = np.zeros(i[1].shape)    \n",
    "                    fdc_v_a = [fdc_v1, bdc_v2]\n",
    "                    fdc_s_a = [fdc_s1, bdc_s2]\n",
    "                    fdc_v_adam.append(fdc_v_a)\n",
    "                    fdc_s_adam.append(fdc_s_a)\n",
    "\n",
    "\n",
    "                    df1_t = np.zeros(i[0].shape)\n",
    "                    db1_t = np.zeros(i[1].shape)\n",
    "                    fb_temp = [df1_t, db1_t]\n",
    "                    dfb.append(fb_temp)\n",
    "\n",
    "\n",
    "                #Final layer 1x1 filter setup\n",
    "\n",
    "                v_out_f = np.zeros(out_f.shape)\n",
    "                s_out_f = np.zeros(out_f.shape)\n",
    "                bv_out_b = np.zeros(out_b.shape)\n",
    "                bs_out_b = np.zeros(out_b.shape)\n",
    "\n",
    "\n",
    "\n",
    "                dout_f = np.zeros(out_f.shape)\n",
    "                dout_b = np.zeros(out_b.shape)\n",
    "\n",
    "            ######################################\n",
    "\n",
    "\n",
    "            #timestamp1 = time.time()\n",
    "\n",
    "\n",
    "            [df1,df2,df3,df4,df5] = df\n",
    "            [db1,db2,db3,db4,db5] = db \n",
    "            [dfb1_dc, dfb2_dc]    = dfb\n",
    "            \n",
    "            \n",
    "            for b in range(batch):\n",
    "                \n",
    "                print('Image: {}/{}'.format((b+1),batch))\n",
    "                #################### TODO: BLOCK IMPLEMENTATION - FUTURE UPDATE ######################\n",
    "                    \n",
    "                    \n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ######################## Forward Propagation Convolution Part  ##########################\n",
    "\n",
    "                \n",
    "                ########### 1st Big Layer ###########    (with zero padding ='same',so with stride =1 we get same dim as the input)\n",
    "                params = [f1[0], b1[0]]  \n",
    "                conv1_1 = conv(X_t[b], params, 1)   #conv1 shape = (num_channels, h, w), padding = 1 (same output dim)\n",
    "                conv1_1[conv1_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f1[1], b1[1]]\n",
    "                conv1_2 = conv(conv1_1, params, 1)\n",
    "                conv1_2[conv1_2<=0] = 0 #Relu\n",
    "                ##################################### conv1_2: 32x32x16\n",
    "                \n",
    "                pl1 = maxpool(conv1_2, 2, 2) #   pl1 : (32-2)/2+1  = 16 \n",
    "                if(dropout>0):\n",
    "                    ## Dropout ##\n",
    "                    d = (np.random.rand(pl1.shape[0],pl1.shape[1],pl1.shape[2])<dropout)\n",
    "                    d = d*1 #Bool --> int(0s and 1s)\n",
    "                    pl1 = d*pl1\n",
    "                    #############\n",
    "                ## ADD DROPOUT HERE(on pl1)\n",
    "                \n",
    "                ########### 2nd Big Layer ###########\n",
    "                params = [f2[0], b2[0]]  \n",
    "                conv2_1 = conv(pl1, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv2_1[conv2_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f2[1], b2[1]]\n",
    "                conv2_2 = conv(conv2_1, params, 1)\n",
    "                conv2_2[conv2_2<=0] = 0 #Relu             \n",
    "                #####################################  16x16x32\n",
    "                \n",
    "                pl2 = maxpool(conv2_2, 2, 2) #   pl1 : (16-2)/2+1  = 8 \n",
    "                if(dropout>0):\n",
    "                    ## Dropout ##\n",
    "                    d = (np.random.rand(pl2.shape[0],pl2.shape[1],pl2.shape[2])<dropout)\n",
    "                    d = d*1 #Bool --> int(0s and 1s)\n",
    "                    pl2 = d*pl2\n",
    "                    #############\n",
    "                ## ADD DROPOUT HERE\n",
    "                \n",
    "                ########### 2nd Big Layer ###########\n",
    "                params = [f3[0], b3[0]]  \n",
    "                conv3_1 = conv(pl2, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv3_1[conv3_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f3[1], b3[1]]\n",
    "                conv3_2 = conv(conv3_1, params, 1)\n",
    "                conv3_2[conv3_2<=0] = 0 #Relu             \n",
    "                #####################################  8x8x64\n",
    "          \n",
    "                ##################################### \n",
    "                ##################################### \n",
    "                #####################################\n",
    "                #Deconvolution/Upsampling\n",
    "                # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "               \n",
    "                params = [f_dc[0][0], f_dc[0][1]] # deconv filter, deconv bias\n",
    "                dc1, new_in1 = convTransp(conv3_2, params, 1, 0)   #result:   =  16x6x32 , \n",
    "                #Concat dc1 with conv2_2 so we get 64 channels (16x16x64)\n",
    "                c1 = concat(dc1, conv2_2) # 1st one is the right one size  \n",
    "                if(dropout>0):\n",
    "                    ## Dropout ##\n",
    "                    d = (np.random.rand(c1.shape[0],c1.shape[1],c1.shape[2])<dropout)\n",
    "                    d = d*1 #Bool --> int(0s and 1s)\n",
    "                    c1 = d*c1\n",
    "                    #############\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                ########### 1st Big dc Layer ###########          16x16x64     \n",
    "                params = [f4[0], b4[0]]  \n",
    "                conv4_1 = conv(c1, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv4_1[conv4_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f4[1], b4[1]]\n",
    "                conv4_2 = conv(conv4_1, params, 1)\n",
    "                conv4_2[conv4_2<=0] = 0 #Relu   \n",
    "                #####################################    16x16x32\n",
    "                \n",
    "                #Deconvolution/Upsampling\n",
    "                # insert zeros : s-1 = 1, padding = k - p -1 = 2-0(what i want)-1=1 ,  s'=1(always) --> (i-1)*s+k-2p = \n",
    "               \n",
    "                params = [f_dc[1][0], f_dc[1][1]] # deconv filter, deconv bias\n",
    "                dc2, new_in2 = convTransp(conv4_2, params, 1, 0)   #result:   =  32x32x16 , \n",
    "                #Concat dc2 with conv1_2 so we get 64 channels (32x32x32)\n",
    "                c2 = concat(dc2, conv1_2) # 1st one is the right one size  \n",
    "                if(dropout>0): \n",
    "                    ## Dropout ##\n",
    "                    d = (np.random.rand(c2.shape[0],c2.shape[1],c2.shape[2])<dropout)\n",
    "                    d = d*1 #Bool --> int(0s and 1s)\n",
    "                    c2 = d*c2\n",
    "                    #############\n",
    "                \n",
    "                ########### 1st Big dc Layer ###########          32x32x32     \n",
    "                params = [f5[0], b5[0]]  \n",
    "                conv5_1 = conv(c2, params, 1)   #conv1 shape = (num_channels, h, w)\n",
    "                conv5_1[conv5_1<=0] = 0 #Relu\n",
    "\n",
    "                params = [f5[1], b5[1]]\n",
    "                conv5_2 = conv(conv5_1, params, 1)\n",
    "                conv5_2[conv5_2<=0] = 0 #Relu   \n",
    "                #####################################    32x32x16\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                ############################# Last Layer conv(1x1) --> 32x32x1 ##########################\n",
    "                params = [out_f, out_b]\n",
    "                output = conv(conv5_2, params, 1, 0) #output.shape: 32x32x1\n",
    "                \n",
    "                \n",
    "                output = normalize(output)\n",
    "                ## Sigmoid ##\n",
    "                Y_hat = sigmoid(output)\n",
    "                #print(Y_hat[:,0:3,0:3])\n",
    "                #label crop is needed\n",
    "                #Y_t_b = crop(Y_t[b], Y_hat.shape[1])\n",
    "                plt.imshow(Y_hat.squeeze(), cmap='Greys_r');\n",
    "                cost_,accuracy_ = Dice_Coef(Y_hat, Y_t[b])#Cross_Entropy(Y_hat, Y_t[b])\n",
    "                cost += cost_\n",
    "                accuracy += accuracy_\n",
    "                accuracy_history.append(accuracy)\n",
    "                if (accuracy>0.655):\n",
    "                    params_values = [filters, bias, f_dc, out_fb]\n",
    "                    return params_values, accuracy_history\n",
    "                #print(accuracy/(b+1))\n",
    "                #########################################################################################\n",
    "                #########################################################################################\n",
    "                ################################# Backward Propagation ##################################\n",
    "                \n",
    "                \n",
    "                #Pixel-wise sub, we we can get the diff(Y includes the 2 classes 0 and 1)\n",
    "                \n",
    "                dA_prev = Y_hat - Y_t[b] #- (np.divide(Y_t[b], Y_hat) - np.divide(1 - Y_t[b], 1 - Y_hat))#\n",
    "                dZ_prev = sigmoid_backward(dA_prev, output)\n",
    "                conv_s =1 \n",
    "                dconv5_2, dout_f_, dout_b_ = convolutionBackward(dZ_prev, conv5_2, out_f, conv_s) #\n",
    "                #pack data\n",
    "                \n",
    "                \n",
    "                dconv5_2[conv5_2<=0] = 0             \n",
    "                dconv5_1, df5_2, db5_2 = convolutionBackward(dconv5_2, conv5_1, f5[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv5_1[conv5_1<=0] = 0\n",
    "                conc_dconv4, df5_1, db5_1 = convolutionBackward(dconv5_1, c2, f5[0], conv_s) #\n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv4, dconv1_2 = crop2half(conc_dconv4)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #conv8_2 is not needed for input,we know how to select the right gradients            \n",
    "                #dconv1_2 = reshape(dconv1_2, conv1_2.shape[1])\n",
    "                dconv4_2, df2_dc, db2_dc = convTranspBackward(dconv4, new_in2, f_dc[1][0], conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                dconv4_2[conv4_2<=0] = 0\n",
    "                dconv4_1, df4_2, db4_2 = convolutionBackward(dconv4_2, conv4_1, f4[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv4_1[conv4_1<=0] = 0\n",
    "                conc_dconv3, df4_1, db4_1 = convolutionBackward(dconv4_1, c1, f4[0], conv_s) #\n",
    "                \n",
    "                \n",
    "                ###### we get the concat gradients ######\n",
    "                #crop the half matrix, we need the second half with the gradients(according to the concat thats the output of the transposed conv)\n",
    "                #### we split the gradients and push them back to their sources  ####\n",
    "                dconv3, dconv2_2 = crop2half(conc_dconv3)  #we will later add gradients of dconv1_2(came from backprop concat) with the extra gradients of its next layer\n",
    "                #conv8_2 is not needed for input,we know how to select the right gradients            \n",
    "                #dconv1_2 = reshape(dconv1_2, conv1_2.shape[1])\n",
    "                dconv3_2, df1_dc, db1_dc = convTranspBackward(dconv3, new_in1, f_dc[0][0], conv_s)\n",
    "                #pack data\n",
    "                \n",
    "                dconv3_2[conv3_2<=0] = 0\n",
    "                dconv3_1, df3_2, db3_2 = convolutionBackward(dconv3_2, conv3_1, f3[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv3_1[conv3_1<=0] = 0\n",
    "                dpl2, df3_1, db3_1 = convolutionBackward(dconv3_1, pl2, f3[0], conv_s) #\n",
    "                \n",
    "                dconv2_2 += maxpoolBackward(dpl2, conv2_2, f=2 , s=2)\n",
    "                \n",
    "                dconv2_2[conv2_2<=0] = 0\n",
    "                dconv2_1, df2_2, db2_2 = convolutionBackward(dconv2_2, conv2_1, f2[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv2_1[conv2_1<=0] = 0\n",
    "                dpl1, df2_1, db2_1 = convolutionBackward(dconv2_1, pl1, f2[0], conv_s) #\n",
    "                \n",
    "                dconv1_2 += maxpoolBackward(dpl1, conv1_2, f=2 , s=2)\n",
    "                \n",
    "                dconv1_2[conv1_2<=0] = 0\n",
    "                dconv1_1, df1_2, db1_2 = convolutionBackward(dconv1_2, conv1_1, f1[1], conv_s) #\n",
    "                #pack data\n",
    "                dconv1_1[conv1_1<=0] = 0\n",
    "                _, df1_1, db1_1 = convolutionBackward(dconv1_1, X_t[b], f1[0], conv_s) #\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                #[df1,df2,df3,df4,df5] = df\n",
    "                #[db1,db2,db3,db4,db5] = db \n",
    "                #dfb1_dc,df     = dfb\n",
    "\n",
    "                \n",
    "                df1[0] += df1_1\n",
    "                df1[1] += df1_2\n",
    "                df2[0] += df2_1\n",
    "                df2[1] += df2_2\n",
    "                df3[0] += df3_1\n",
    "                df3[1] += df3_2\n",
    "                df4[0] += df4_1\n",
    "                df4[1] += df4_2\n",
    "                df5[0] += df5_1\n",
    "                df5[1] += df5_2\n",
    "                \n",
    "                db1[0] += db1_1\n",
    "                db1[1] += db1_2\n",
    "                db2[0] += db2_1\n",
    "                db2[1] += db2_2\n",
    "                db3[0] += db3_1\n",
    "                db3[1] += db3_2\n",
    "                db4[0] += db4_1\n",
    "                db4[1] += db4_2\n",
    "                db5[0] += db5_1\n",
    "                db5[1] += db5_2\n",
    "\n",
    "                dfb1_dc[0] += df1_dc\n",
    "                dfb1_dc[1] += db1_dc\n",
    "                dfb2_dc[0] += df2_dc\n",
    "                dfb2_dc[1] += db2_dc\n",
    "\n",
    "                dout_f += dout_f_\n",
    "                dout_b += dout_b_\n",
    "\n",
    "\n",
    "      \n",
    "            \n",
    "            \n",
    "            ############## Adam Optimization ################\n",
    "            #changing the main structures(which are also updated)\n",
    "            #TODO: Future update - remove uneccessary memory loads/stores, v,s dont need to be saved \n",
    "            for i in range(len(filters)):\n",
    "                v_adam[i][0] = beta1*v_adam[i][0] + (1-beta1)*df[i][0]/batch_size #f1\n",
    "                s_adam[i][0] = beta2*s_adam[i][0] + (1-beta2)*(df[i][0]/batch_size)**2 #f1  \n",
    "                filters[i][0] -= lr*v_adam[i][0]/np.sqrt(s_adam[i][0] + 1e-8)\n",
    "                \n",
    "                v_adam[i][1] = beta1*v_adam[i][1] + (1-beta1)*df[i][1]/batch_size #f2\n",
    "                s_adam[i][1] = beta2*s_adam[i][1] + (1-beta2)*(df[i][1]/batch_size)**2  #f2\n",
    "                filters[i][1] -= lr*v_adam[i][1]/np.sqrt(s_adam[i][1] + 1e-8)\n",
    "                \n",
    "            for i in range(len(bias)):\n",
    "                bv_adam[i][0] = beta1*bv_adam[i][0] + (1-beta1)*db[i][0]/batch_size #b1\n",
    "                bs_adam[i][0] = beta2*bs_adam[i][0] + (1-beta2)*(db[i][0]/batch_size)**2  #b1\n",
    "                bias[i][0] -= lr*bv_adam[i][0]/np.sqrt(bs_adam[i][0] + 1e-8)\n",
    "                \n",
    "                bv_adam[i][1] = beta1*bv_adam[i][1] + (1-beta1)*db[i][1]/batch_size #b2\n",
    "                bs_adam[i][1] = beta2*bs_adam[i][1] + (1-beta2)*(db[i][1]/batch_size)**2  #b2\n",
    "                bias[i][1] -= lr*bv_adam[i][1]/np.sqrt(bs_adam[i][1] + 1e-8)\n",
    "            \n",
    "            for i in range(len(f_dc)):\n",
    "                fdc_v_adam[i][0] = beta1*fdc_v_adam[i][0] + (1-beta1)*dfb[i][0]/batch_size #f1\n",
    "                fdc_s_adam[i][0] = beta2*fdc_s_adam[i][0] + (1-beta2)*(dfb[i][0]/batch_size)**2  #f1\n",
    "                f_dc[i][0] -= lr*fdc_v_adam[i][0]/np.sqrt(fdc_s_adam[i][0] + 1e-8)\n",
    "                \n",
    "                fdc_v_adam[i][1] = beta1*fdc_v_adam[i][1] + (1-beta1)*dfb[i][1]/batch_size #b2\n",
    "                fdc_s_adam[i][1] = beta2*fdc_s_adam[i][1] + (1-beta2)*(dfb[i][1]/batch_size)**2  #b2\n",
    "                f_dc[i][1] -= lr*fdc_v_adam[i][1]/np.sqrt(fdc_s_adam[i][1] + 1e-8)    \n",
    "            \n",
    "            v_out_f = beta1*v_out_f + (1 - beta1)*dout_f/batch_size #f\n",
    "            s_out_f = beta2*s_out_f + (1 - beta2)*(dout_f/batch_size)**2  #f\n",
    "            out_fb[0] -= lr*v_out_f/np.sqrt(s_out_f + 1e-8)\n",
    "            \n",
    "            bv_out_b = beta1*bv_out_b + (1 - beta1)*dout_b/batch_size #b\n",
    "            bs_out_b = beta2*bs_out_b + (1 - beta2)*(dout_b/batch_size)**2  #b\n",
    "            out_fb[1] -= lr*bv_out_b/np.sqrt(bs_out_b + 1e-8)\n",
    "            \n",
    "            '''\n",
    "                        for i in range(len(filters)):\n",
    "                filters[i][0] -= lr*df[i][0]\n",
    "                bias[i][0] -= lr*db[i][0]\n",
    "            \n",
    "            \n",
    "            f_dc[0][0] -= lr*df1_dc\n",
    "            f_dc[0][1] -= lr*db1_dc\n",
    "            \n",
    "            out_fb[0] -= lr*dout_f\n",
    "            out_fb[1] -= lr*dout_b\n",
    "            #Updating Conv Part\n",
    "            \n",
    "\n",
    "            #print(Yh)\n",
    "            #print(t)\n",
    "            #print(\"Batch:{}\".format(c+12))\n",
    "            \n",
    "           \n",
    "            f1 -= (learning_rate) * (df1/batch)\n",
    "            f2 -= (learning_rate) * (df2/batch)\n",
    "            b1 -= (learning_rate) * (db1/batch)\n",
    "            b2 -= (learning_rate) * (db2/batch)\n",
    "            filter_params = [f1, f2, b1, b2]\n",
    "                \n",
    "            # updating FC Part\n",
    "            #params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "            grads_values['dW1'] = dW1#/(batch)\n",
    "            grads_values['dW2'] = dW2#/(batch)\n",
    "            grads_values['db1'] = dB1#/(batch)\n",
    "            grads_values['db2'] = dB2#/(batch)\n",
    "                   \n",
    "            for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "                \n",
    "                params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "                params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "            '''\n",
    "            #print(\"Cost : {:.5f}\".format(cost/batch))\n",
    "            #print(\"Accuracy : {:.5f}%\".format((accuracy*100)/batch))\n",
    "            \n",
    "        #END OF LOOP - EPOCH\n",
    "        #timestamp2 = time.time()\n",
    "        #print (\"This took %.2f seconds\" %(timestamp2 - timestamp1))\n",
    "        if(verbose):\n",
    "            print(\"Epoch: {:5d}   -   cost: {:.2f}   -   Accuracy: {:.2f}%\".format(e+1, cost/batch, (accuracy*100)/batch))\n",
    "        #if(callback is not None):\n",
    "        #    callback(i, params_values)\n",
    "    #pack filters\n",
    "    params_values = [filters, bias, f_dc, out_fb]\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout Disabled!\n",
      "Epoch: {1}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     1   -   cost: 0.75   -   Accuracy: 47.06%\n",
      "Epoch: {2}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     2   -   cost: 0.75   -   Accuracy: 47.04%\n",
      "Epoch: {3}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     3   -   cost: 0.75   -   Accuracy: 47.03%\n",
      "Epoch: {4}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     4   -   cost: 0.75   -   Accuracy: 47.02%\n",
      "Epoch: {5}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     5   -   cost: 0.76   -   Accuracy: 47.00%\n",
      "Epoch: {6}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     6   -   cost: 0.76   -   Accuracy: 46.97%\n",
      "Epoch: {7}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     7   -   cost: 0.76   -   Accuracy: 46.90%\n",
      "Epoch: {8}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     8   -   cost: 0.76   -   Accuracy: 46.58%\n",
      "Epoch: {9}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:     9   -   cost: 0.80   -   Accuracy: 45.07%\n",
      "Epoch: {10}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    10   -   cost: 0.91   -   Accuracy: 40.13%\n",
      "Epoch: {11}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    11   -   cost: 0.96   -   Accuracy: 38.18%\n",
      "Epoch: {12}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    12   -   cost: 0.96   -   Accuracy: 38.27%\n",
      "Epoch: {13}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    13   -   cost: 0.95   -   Accuracy: 38.62%\n",
      "Epoch: {14}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    14   -   cost: 0.93   -   Accuracy: 39.51%\n",
      "Epoch: {15}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    15   -   cost: 0.91   -   Accuracy: 40.39%\n",
      "Epoch: {16}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    16   -   cost: 0.86   -   Accuracy: 42.25%\n",
      "Epoch: {17}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    17   -   cost: 0.81   -   Accuracy: 44.43%\n",
      "Epoch: {18}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    18   -   cost: 0.85   -   Accuracy: 42.64%\n",
      "Epoch: {19}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    19   -   cost: 0.81   -   Accuracy: 44.47%\n",
      "Epoch: {20}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    20   -   cost: 0.85   -   Accuracy: 42.93%\n",
      "Epoch: {21}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    21   -   cost: 0.79   -   Accuracy: 45.23%\n",
      "Epoch: {22}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    22   -   cost: 0.81   -   Accuracy: 44.27%\n",
      "Epoch: {23}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    23   -   cost: 0.76   -   Accuracy: 46.99%\n",
      "Epoch: {24}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    24   -   cost: 0.75   -   Accuracy: 47.03%\n",
      "Epoch: {25}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    25   -   cost: 0.70   -   Accuracy: 49.47%\n",
      "Epoch: {26}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    26   -   cost: 0.68   -   Accuracy: 50.73%\n",
      "Epoch: {27}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    27   -   cost: 0.64   -   Accuracy: 52.60%\n",
      "Epoch: {28}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    28   -   cost: 0.60   -   Accuracy: 54.75%\n",
      "Epoch: {29}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    29   -   cost: 0.57   -   Accuracy: 56.42%\n",
      "Epoch: {30}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    30   -   cost: 0.57   -   Accuracy: 56.47%\n",
      "Epoch: {31}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    31   -   cost: 0.53   -   Accuracy: 58.64%\n",
      "Epoch: {32}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    32   -   cost: 0.51   -   Accuracy: 59.99%\n",
      "Epoch: {33}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    33   -   cost: 0.52   -   Accuracy: 59.40%\n",
      "Epoch: {34}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    34   -   cost: 0.51   -   Accuracy: 60.30%\n",
      "Epoch: {35}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    35   -   cost: 0.59   -   Accuracy: 55.37%\n",
      "Epoch: {36}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    36   -   cost: 0.59   -   Accuracy: 55.50%\n",
      "Epoch: {37}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    37   -   cost: 0.57   -   Accuracy: 56.36%\n",
      "Epoch: {38}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    38   -   cost: 0.87   -   Accuracy: 41.89%\n",
      "Epoch: {39}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    39   -   cost: 0.60   -   Accuracy: 55.02%\n",
      "Epoch: {40}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    40   -   cost: 0.70   -   Accuracy: 49.62%\n",
      "Epoch: {41}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    41   -   cost: 0.64   -   Accuracy: 52.84%\n",
      "Epoch: {42}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    42   -   cost: 0.51   -   Accuracy: 59.89%\n",
      "Epoch: {43}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    43   -   cost: 0.51   -   Accuracy: 59.80%\n",
      "Epoch: {44}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    44   -   cost: 0.51   -   Accuracy: 60.21%\n",
      "Epoch: {45}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    45   -   cost: 0.63   -   Accuracy: 53.37%\n",
      "Epoch: {46}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    46   -   cost: 0.48   -   Accuracy: 62.07%\n",
      "Epoch: {47}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    47   -   cost: 0.53   -   Accuracy: 59.15%\n",
      "Epoch: {48}\n",
      "Batch: 1\n",
      "Image: 1/1\n",
      "Epoch:    48   -   cost: 0.43   -   Accuracy: 64.85%\n",
      "Epoch: {49}\n",
      "Batch: 1\n",
      "Image: 1/1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### Training ######\n",
    "#train_images, train_labels, test_images, test_labels\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "#By tuning LR helps with eliminating big jumps of the accuracy.\n",
    "#Its important to make SMALL adjustment to LR so you can find the right spot\n",
    "params_values, accuracy_history = train(train_images, train_labels, 60, 0.008, 0) #0.05 stable LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(accuracy_history, label=\"Accuracy\")\n",
    "#plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "#plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Forward step . . .\n",
      "Cost: 0.39   -   Accuracy: 67.66%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### Prediction ######\n",
    "Validate(train_images, train_labels, params_values);\n",
    "pass\n",
    "\"\"\"\n",
    "Yt = test_labels.T\n",
    "temp1 = []\n",
    "for i in range(Yt.shape[1]):\n",
    "        for j in range(Yt.shape[0]):\n",
    "            if(Yt[j][i]==1):\n",
    "                temp1.append(j)\n",
    "Yt=np.array(temp1)\n",
    "Y_test_hat, _ = full_forward_propagation(np.transpose(test_images), params_values, NN_ARCHITECTURE)#multiple?!\n",
    "\n",
    "Yht = np.array(Y_test_hat.T)\n",
    "#x_log=np.log( np.exp(Yh) / np.sum(np.exp(Yh), axis = 1) )  #(60000,10) , we need to add along columns so we get sum of 1 on every example-row\n",
    "num = np.exp(Yht)\n",
    "den = np.sum(np.exp(Yht), axis = 1)\n",
    "for i in range(Yht.shape[0]): #60000\n",
    "                #for j in range(Yh.shape[1]): #10\n",
    "                Yht[i][:] = np.log(num[i][:] / den[i])  \n",
    "\n",
    "#cost = get_cost_value(Yht, Yt)\n",
    "\n",
    "#cost_history.append(cost)\n",
    "accuracy = get_accuracy_value(Y_test_hat, test_labels.T)\n",
    "#accuracy_history.append(accuracy)\n",
    "print(\"Accuracy: {:.5f}%\".format( accuracy*100))\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
